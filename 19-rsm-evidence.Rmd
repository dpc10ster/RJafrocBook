# Validity of the RSM {#rsm-evidence}


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(kableExtra)
library(seqinr)
library(RJafroc)
library(ggplot2)
library(gridExtra)
library(binom)
library(here)
```

## Introduction {#rsm-evidence-intro}

This chapter details the evidence for the validity of the radiological search model (RSM). Listed next are the specific reasons. 

1.	Its correspondence to the empirical (i.e., measurement based) Kundel-Nodine model of radiological search. 
2.	In special cases, it reduces to being indistinguishable from the binormal model.
3.	It explains: 
    a. The empirical observation [@RN298] that most ROC datasets are characterized by b-parameter $b < 1$. 
    b. The empirical observation [@RN2635] that the $b$ tends to decrease as contrast increases. 
    c. The empirical observation [@RN2635], that the difference in means of the two pdfs divided by the difference in standard deviations is roughly constant.
4.	It explains data degeneracy, i.e., no interior data points, sometimes observed especially with expert observers. 
5.	It predicts FROC/AFROC and LROC curves that fit real datasets.

As described in TBA Chapter 20, the CBM explains 3(a) and 4 while the bigamma model [@RN100] explains 3(c).

## Correspondence to the Kundel-Nodine model {#rsm-predictions-corresponence-kundel-nodine}
The strongest evidence for the validity of the RSM is its close correspondence to the Kundel-Nodine model of radiological search [@RN1533; @RN1388; @RN438; @RN1663], which in turn is derived from eye-tracking measurements made on radiologists while they perform diagnostic tasks. These show that radiologists identify suspicious regions in a short time and this ability corresponds to the $\lambda', \nu'$ parameters of the RSM. Having found suspicious regions, the next activity uncovered by eye-tracking measurements is the detailed examination of each suspicious region in order to determine if it is a significant finding. This is where the z-sample is calculated, and this process is modeled by two unit normal distributions separated by the $\mu$ parameter of the RSM. 

Other ROC models do not share this close correspondence. The CBM model comes closest â€“ it models the probability that lesions are *found*, which is part of search performance, but the ability to *avoid finding NLs* is not modeled. Like other ROC models, CBM predicts that the point (1,1) is continuously accessible to observer, which implies zero search performance, TBA Fig. 17.6.

## The binormal model is a special case of the RSM {#rsm-predictions-binormal-model}

```{r echo=FALSE, cache=TRUE}
source(here("R/rsm-evidence/PlotBMErrBar.R"))
K1 <- 500;K2 <- 700;desiredNumBins <- 5;
plotArr <- array(list(), dim = 9)
seedArr <- aucArr <- array(dim = 9)
rocDataTable <- array(dim = c(9, 2,desiredNumBins))
LmaxArr <- nuArr <- lambdaArr <- muArr <- array(dim = 9)
AzArr <- pValArr <- bArr <- aArr <- array(dim = 9)

for (Row in 1:9) {
  switch(Row,
         "1" = {seed <- 1;Lmax <- 1;mu <- 2.0;lambda <- 10
         nu <- 1;zeta1 <- -1;set.seed(seed)}, # Row 1
         "2" = {seed <- 1;Lmax <- 1;mu <- 2.5;lambda <- 10
         nu <- 1;zeta1 <- -1;set.seed(seed)}, # Row 2
         "3" = {seed <- 1;Lmax <- 1;mu <- 3.0;lambda <- 10
         nu <- 1;zeta1 <- -1;set.seed(seed)}, # Row 3
         "4" = {seed <- 2;Lmax <- 1;mu <- 2.5;lambda <- 10
         nu <- 1;zeta1 <- -1;set.seed(seed)}, # Row 4
         "5" = {seed <- 2;Lmax <- 2;mu <- 2.0;lambda <- 10
         nu <- 1;zeta1 <- -1;set.seed(seed)},# Row 5
         "6" = {seed <- 2;Lmax <- 2;mu <- 2.5;lambda <- 10
         nu <- 1;zeta1 <- -1;set.seed(seed)},# Row 6
         "7" = {seed <- 2;Lmax <- 2;mu <- 3.0;lambda <- 10
         nu <- 1;zeta1 <- -1;set.seed(seed)},# Row 7
         "8" = {seed <- 2;Lmax <- 2;mu <- 3.0;lambda <-  1
         nu <- 1;zeta1 <- -1;set.seed(seed)},# Row 8
         "9" = {seed <- 2;Lmax <- 2;mu <- 3.0;lambda <- 0.1
         nu <- 1;zeta1 <- -1;set.seed(seed)}# Row 9
  )
  muArr[Row] <- mu
  lambdaArr[Row] <- lambda
  nuArr[Row] <- nu
  LmaxArr[Row] <- Lmax
  seedArr[Row] <- seed
  perCase <- floor(runif(K2, 1, Lmax + 1))
  
  if (Lmax == 2) {
    lesDistr <- c(0.4714286, 0.5285714)
  } else if (Lmax == 1) {
    lesDistr <- 1
  } else stop("Need to run Eng program for other values of Lmax")
  
  frocDataRaw  <- SimulateFrocDataset(mu, lambda, nu, 
                                      zeta1, I = 1, 
                                      J = 1, K1, K2, 
                                      perCase = perCase, 
                                      seed = seed)
  rocDataRaw <- DfFroc2Roc(frocDataRaw)
  
  rocDataBinned <- DfBinDataset(rocDataRaw, 
                                desiredNumBins = desiredNumBins, 
                                opChType = "ROC")
  
  x1 <- table(rocDataBinned$ratings$NL[1:K1])
  if (length(x1) == 1) {
    rocDataTable[Row,1,] <- c(0,x1,0,0,0)
  } else if (length(x1) == 3) {
    rocDataTable[Row,1,] <- c(0, x1,0)
  } else if (length(x1) == 4) {
    rocDataTable[Row,1,] <- c(x1,0)
  } else if (length(x1) == 5) {
    rocDataTable[Row,1,] <- x1
  } else stop("incorrect table length 1")
  
  x2 <- table(rocDataBinned$ratings$LL[1:K2])
  if (length(x2) == 4) {
    rocDataTable[Row,2,] <- c(0,x2)
  } else if (length(x2) == 5) {
    rocDataTable[Row,2,] <- x2
  } else stop("incorrect table length 2")
  
  aucArr[Row] <- UtilAnalyticalAucsRSM(
    mu = mu, lambda = lambda,
    nu = nu, zeta1 = zeta1, lesDistr = lesDistr)$aucROC
  # print(rocDataTable[Row,,])
  # copy the last two rows of output to Eng program; 
  # delete bracket stuff leaving numbers only with spaces 
  # select format 3
  # Run Program
  # compare a, b to following values
  
  switch(Row, 
         # the following values were transferred from the 
         # Eng program output after analyzing rocDataTable 
         # generated by previous lines, using 
         # the appropriate value of Row
         "1" = {a <- 1.002;b <- 0.861; pVal <- 0.223}, # Row 1
         "2" = {a <- 1.497;b <- 0.752; pVal <- 0.460}, # Row 2
         "3" = {a <- 1.928;b <- 0.736; pVal <- 0.198}, # Row 3
         "4" = {a <- 1.221;b <- 0.682; pVal <- 0.120}, # Row 4
         "5" = {a <- 1.250;b <- 0.786; pVal <- 0.903}, # Row 5
         "6" = {a <- 1.554;b <- 0.646; pVal <- 0.592}, # Row 6
         "7" = {a <- 2.057;b <- 0.676; pVal <- 0.009}, # Row 7
         "8" = {a <- 2.391;b <- 0.405; pVal <- 0.988}, # Row 8
         "9" = {a <- 2.015;b <- 0.068; pVal <- NA}  # Row 9
  )
  aArr[Row] <- a
  bArr[Row] <- b
  AzArr[Row] <- pnorm(a/sqrt(1+b^2))
  pValArr[Row] <- pVal
  plotArr[[Row]] <- PlotBMErrBar(a, b, rocDataTable[Row,,]) + 
    ggtitle(paste0("Row = ", Row, ", a = ",  a, ", b = ", b))
}
```

ROC models assume that every case provides a finite decision variable sample. This is what permits the observer to *continuously* move the operating point to (1,1). According to the RSM a decision sample on every case is possible if $\lambda$ is large, since in the limit $\lambda \rightarrow \infty$ every case has at least one latent NL. It turns out, as shown next, that it is not necessary to go to infinite values of $\lambda$ to produce RSM-generated ratings data that are indistinguishable from those generated by the binormal model. Values of $\lambda$ around 1 to 10 are sufficient to demonstrate this fact. A factor that helps showing the indistinguishability is that the binormal model is remarkably resilient to departures from normality, its *robustness property*, demonstrated in [@RN1216]. It literally takes huge datasets, numbering in the thousands of cases, to show departures from strict normality. 

The ROC ratings datasets used in the following examples were generated by the RSM. The parameter values are listed in Table \@ref(tab:rsm-evidence-binormal-table2). RSM generated FROC datasets, in each case 500 non-diseased and 700 diseased cases were used, were converted to (highest rating) ROC datasets, each dataset was binned into 5 bins and then analyzed by an online Java program [@RN1975] which implements Metz's ROCFIT program, TBA Chapter 06, yielding the binormal parameters $a, b$ and the p-value of the chisquare goodness of fit statistic. Shown next are the binormal-model-fitted ROC curves to these datasets as well as the corresponding $a,b$ parameters. The value of Row corresponds to the row number in Table \@ref(tab:rsm-evidence-binormal-table2). 

```{r rsm-evidence-binormal-plots1-do-not-use, fig.show='hold', echo=FALSE, fig.pos='H'}
grid.arrange(plotArr[[1]], plotArr[[2]], plotArr[[3]],plotArr[[4]], nrow = 2, ncol = 2)
```             


```{r rsm-evidence-binormal-plots2, fig.cap="RSM generated ROC points and corresponding binormal model fitted curves.", fig.show='hold', echo=FALSE, fig.pos='H'}
grid.arrange(plotArr[[5]], plotArr[[6]], plotArr[[7]], plotArr[[8]], nrow = 2, ncol = 2)
```             

Fig. \@ref(fig:rsm-evidence-binormal-plots2): ROC *operating points* obtained using RSM ratings-generator and corresponding binormal model *fitted curves*. The $a, b$ parameters are shown in the figure labels. The value of `Row` corresponds to the row number in Table \@ref(tab:rsm-evidence-binormal-table2). The plots show that over a wide range of parameters RSM generated ROC data is fitted reasonably by the binormal model. The p-values of the goodness of fit statistic, see Table \@ref(tab:rsm-evidence-binormal-table2), are all in the range of what is considered an acceptable fit to a model. As far as the binormal model-fitting software is concerned, the counts data arose from two *effectively normal distributions* (i.e., apart from the intrinsic uncertainty due to allowed arbitrary monotone transformations). Even with the large number of cases, sampling variability affects the binormal model fits: e.g., the binormal model curves in plots labeled "2" and "4" differ only in seed values. The hooks near (1,1) in the binormal ROC fitted curves are not easily visible but are nevertheless present as each of the b-parameters in Table \@ref(tab:rsm-evidence-binormal-table2) is less than unity. The error bars are exact 95% binomial confidence intervals on the operating points.   

**Since the binormal model has been used successfully for almost six decades, the ability to the RSM to mimic it is an important justification for the validity of the RSM.**


```{r  rsm-evidence-do-one-row, echo=FALSE}
do_one_row <- function(Row,seed,Lmax,mu,lambda,auc,a,b,Az,pVal){

ret <- c(
  Row,
  seed,
  Lmax,
  mu,
  lambda,
  format(auc, digits = 3),
  a,
  b,
  format(Az, digits = 3),
  pVal)

return(stresc(ret))
}
```


```{r rsm-evidence-all-cells, echo=FALSE}
allCells <- array("", dim = c(8,10))

for (i in 1:8) {
  allCells[i,] <- do_one_row (i,seedArr[i],LmaxArr[i],muArr[i],
                              lambdaArr[i],
                              aucArr[i], aArr[i], 
                              bArr[i],AzArr[i], pValArr[i])  
}
```



```{r rsm-evidence-binormal-table2, echo=FALSE}
cells = array(dim = c(8,10))

for (j in 1:8) cells[j,]  <- allCells[j,]

df <- as.data.frame(cells, stringsAsFactors = FALSE)
colnames(df) <-c("Row", "s", "L", "$\\mu$", "$\\lambda$", "A", "a", "b",  "Az", "pVal")
# escape = FALSE is critical in getting Math right
kbl(df, caption = "Simulating ROC binned ratings data using the RSM and fitting the ratings using the binormal model; s is the seed, L is the maximum number of lesions per case, A is the RSM-predicted ROC-AUC, Az is the binormal model fitted AUC and $\\nu = 1$ for all datasets.", booktabs = TRUE, escape = FALSE) %>% kable_styling(latex_options = c("basic", "scale_down", "HOLD_position"), row_label_position = "c") 
```


Table \@ref(tab:rsm-evidence-binormal-table2): Results of simulating ROC ratings tables using seeds and RSM parameter values specified in columns 2 â€“ 5 and fitting each ratings table using the binormal model. The corresponding binormal model fitted ROC curves are shown in Fig. \@ref(fig:rsm-evidence-binormal-plots2). The number of non-diseased cases was 500, the number of diseased cases was 700, and the reporting threshold $\zeta_1 = -1$. 

Of interest in Table \@ref(tab:rsm-evidence-binormal-table2) is the observation that $b < 1$, and the qualities of the fits are quite good (p > 0.001 is generally considered acceptable, see [@RN300], 3rd edition, page 779). 

One expects $A \equiv AUC_{ROC}^{RSM}$ to exceed the binormal fitted value $A_z$. This has to do with the "proper" property of the RSM-ROC curve, which implies an *ideal observer*, while the binormal model predicts "improper" ROC curves, TBA Chapter 20. For rows 2 and 3, the expected orderings are reversed but the magnitudes of the discrepancies are small. This is because RSM-predicted values are *not* subject to sampling variability as they are derived by numerical integration, whose estimation error is very small compared to sampling error. In contrast, the estimates of $A_z$ are subject to sampling variability, even though large numbers of cases were used. Row-4 repeats Row-2 with a different value of `seed`: this time the expected ordering is observed $A > Az$. 


## Empirical observations
As summarized previously, there are three empirical observations regarding binormal parameters: 

* $b < 1$. 
* $b$ decreases as $\mu$ increases. 
* For fixed experimental conditions $R_{Swets} \equiv \Delta(mean) / \Delta(\sigma)$ is approximately constant. To not confuse with the RSM $\mu$ parameter, the difference in means of the two normal distributions is denoted $\Delta(mean)$, not $\Delta \mu$. 


### Explanation for $b < 1$
The RSM-predicted ROC curves are consistent with empirical observations [@RN298] that observed ROC data, when fitted by the unequal variance binormal model, yield $b < 1$, implying that the diseased case pdf is wider than the non-diseased case pdf. The RSM provides an explanation for this: diseased cases yield two types of z-samples, namely NL z-samples from a zero-centered unit variance normal distribution and LL z-samples from a  $\mu$-centered unit variance normal distribution. The resulting *mixture distribution* is expected, when one attempts to fit it with a normal distribution, to yield standard deviation for diseased cases greater than 1, or, equivalently, $b < 1$. The fit is not expected to be ideal, but it is known that for relatively small numbers of cases, as is true with clinical data sets, it is difficult to detect deviations from strict normality; indeed, the binormal model is quite robust with respect to deviations from strict normality [@RN298]. Several examples of this were evident in the goodness of fit p-values in Table \@ref(tab:rsm-evidence-binormal-table2), which show good binormal fits to RSM generated data even with 1200 cases. 


```{r rsm-evidence-pdfs, echo=FALSE}
K2 <- 700
Lmax <- 1
Lk2 <- floor(runif(K2, 1, Lmax + 1))
lesDistr <- 1
plotArr <- array(list(), dim = c(2,2))
labels <- LETTERS[1:4]; dim(labels) = c(2,2);labels <- t(labels)
muArr <- c(2,3)
lambda <- 1
nuArr <- c(0.15,0.25)
L <- 1  # to show wider pdfs of diseased cases
for (i in 1:length(muArr)) {
  mu <- muArr[i]
  for (j in 1:length(nuArr)) {
    nu <- nuArr[j]
    ret1 <- PlotRsmOperatingCharacteristics(mu,
                                            lambda,
                                            nu,
                                            lesDistr = lesDistr,
                                            legendPosition  = "none")
    plotArr[[i,j]] <- ret1$PDFPlot + 
      ggtitle(paste0(labels[i,j], ", mu = ", muArr[i], ", nu = ", nuArr[j]))
  }
}

```



```{r rsm-evidence-pdf-plots, fig.cap="pdfs along with the parameter values. The dotted curves correspond to non-diseased cases while solid curves correspond to diseased cases.", fig.show='hold', echo=FALSE, fig.pos='H'}
grid.arrange(plotArr[[1,1]], plotArr[[1,2]], plotArr[[2,1]], plotArr[[2,2]], nrow = 2, ncol = 2)
```             


Fig. \@ref(fig:rsm-evidence-pdf-plots): This figure provides an explanation for empirical observation $b < 1$. Displayed are pdfs along with the parameter values. For all plots $\lambda = 1$ and $L_{max} = 1$. The dotted curves correspond to non-diseased cases while the solid curves correspond to diseased cases. The solid curves are broader than the dotted ones. In (A) and (B) the solid curve is noticeably broader. In (D) there is a hint of a secondary peak at zero, which is quite prominent in (C), which corresponds to the largest $\mu$ and the smallest $\nu$. In each case the resulting mixture distribution is expected to lead to a larger estimate of standard deviation of the assumed normal distribution of diseased cases relative to non-diseased cases. 


### Explanation of Swets et al observations 

More than 55 years ago, [@RN2635] noticed in non-medical imaging contexts:
 
* The standard deviation of the non-diseased distribution divided by the standard deviation of the diseased distribution, tended to decrease as contrast increased. In binormal parameter terms, $b$ decreases as $a$ increases.

* The ratio $R_{Swets} \equiv \Delta(mean) / \Delta(\sigma)$ is approximately constant for a fixed set of experimental conditions. In binormal parameter terms, $\frac{a}{1-b}$ is approximately constant ^[The separation is $a$, the wider signal distribution has standard deviation unity while the narrower noise distribution has standard deviation $b$.].

In the RSM $\mu$ is the perceptual signal to noise of found lesions. Swets' first empirical observation implies that the $b$ parameter should decrease with increasing $\mu$. The second observation


Testing this proposition over a wide range of $\mu$ using the preceding methods (i.e., binormal model maximum likelihood fitting) is direct but cumbersome and subject to failure. This is because it depends on the ability to find five counts in each bin and convergence of the binormal model algorithm, which is problematical for larger values of $\mu$ which lead to degenerate datasets. Instead, the following method was used. The search model predicted pdfs were normalized so that they individually integrated to unit areas. This was accomplished by dividing the non-diseased pdf by the x-coordinate of the end-point, and the diseased pdf by the y-coordinate of the end-point. The means and standard deviations of these distributions were calculated by numerical integration. If $f(x)$ is a unit-normalized pdf its mean $<x>$  and variance $\sigma_x^2$ are defined by:

\begin{equation}
\left. 
\begin{aligned}
\left \langle x \right \rangle = & \int_{-\infty}^{\infty} xf(x) dx \\
\sigma_x^2 = & \int_{-\infty}^{\infty} \left (x - \left \langle x \right \rangle  \right )^2f(x) dx \\
\end{aligned}
\right \}
(\#eq:rsm-evidence-x-sigmax2)
\end{equation}


The needed quantities are defined as:

\begin{equation}
\left. 
\begin{aligned}
\Delta(mean) = & \left \langle x \right \rangle_D - \left \langle x \right \rangle_N  \\
\Delta(\sigma)= & \sigma_D - \sigma_N
\end{aligned}
\right \}
(\#eq:rsm-evidence-delta-mean-sigma)
\end{equation}



*The standard deviation is the square root of the variance. Varying experimental conditions were simulated by individually varying two of the three parameters of the RSM (excluding $\zeta_1 = -3$) under the constraint that the RSM predicted AUC remained constant at a specified value. Without this constraint, variation of a single parameter, e.g.,  $\mu$, would cause AUC to vary over the entire range 0.5 to 1, which is uncharacteristic of radiologists interpreting the same case set. Rather, we assume that observers, characterized by different RSM parameters, nevertheless converge to roughly the same RSM-AUCs. In other words, they tend to compensate for deficiencies in some area (e.g., finding too many NLs, large $\lambda$) with increased performance in other areas (e.g., finding more lesions, i.e., larger $\nu$, and/or extracting greater pSNR from found lesions, i.e., larger $\mu$).*

Shown first are three tables corresponding to RSM-AUC = 0.7. 

* In Table \@ref(tab:rsm-evidence-table1A) $\lambda = 2$ is held constant while the other two parameters are varied to keep AUC at 0.7. 
* In Table \@ref(tab:rsm-evidence-table1B) $\nu = 1$ is held constant. 
* In Table \@ref(tab:rsm-evidence-table1C) $\mu = 2$ is held constant.


```{r main-rsm-swets-observations0, echo=FALSE}
source(here("R/rsm-evidence/PlotBMErrBar.R"))
source(here("R/rsm-evidence/effectiveAB.R"))
source(here("R/rsm-evidence/FindParamFixAuc.R"))
source(here("R/rsm-evidence/rsmSupportFns.R"))

logseq <- function( d1, d2, n) {
  logf <- log(d2/d1)/(n-1)
  return (exp(seq(log(d1), log(d2), logf)))
}

# parameters are adjusted to attain this value; 0.7 or 0.8
do_one_value <- function (RsmRocAuc) {
  
  Lmax <- 1;K2 <- 700;Lk2 <- floor(runif(K2, 1, Lmax + 1)) 
  nLesPerCase <- unique(Lk2);lesionDist <- array(dim = c(length(nLesPerCase), 2))
  for (i in nLesPerCase) lesionDist[i, ] <- c(i, sum(Lk2 == i)/K2)
  lesDistr <- 1
  
  # Part A
  dfA <- data.frame()
  lambda <- 2
  muArr <- logseq(2,5,10)
  for (i in 1:length(muArr)) {
    mu <- muArr[i]
    nu <- NA   # NA indicates this is to be varied
    
    retParms <- FindParamFixAuc(mu, lambda, nu, lesionDist, RsmRocAuc)
    if (!is.na(retParms)) nu <- retParms else next
    ret <- effectiveAB(mu, lambda, nu, lesionDist)
    a <- ret$a
    b <- ret$b
    R_Swets  <-  a/(1-b)
    
    ret <- UtilIntrinsic2PhysicalRSM(mu, lambda, nu)
    lambdaP <- ret$lambdaP
    nuP <- ret$nuP
    
    dfA <- rbind(dfA, data.frame(
      AUC = RsmRocAuc,
      mu = mu, 
      lambda = lambdaP,
      nu = nuP,
      a = a,
      b = b, 
      R_Swets = R_Swets
    ))
  }
  
  # Part B
  dfB <- data.frame()
  nu <- 1
  lambdaArr <- logseq(1, 5, 10)
  for (i in 1:length(lambdaArr)) {
    lambda <- lambdaArr[i] 
    mu <- NA  # NA indicates this is to be varied
    
    retParms <- FindParamFixAuc(mu, lambda, nu, lesionDist, RsmRocAuc)
    if (!is.na(retParms)) mu <- retParms else next
    ret <- effectiveAB(mu, lambda, nu, lesionDist)
    a <- ret$a
    b <- ret$b
    R_Swets  <-  a/(1-b)

    ret <- UtilIntrinsic2PhysicalRSM(mu, lambda, nu)
    lambdaP <- ret$lambdaP
    nuP <- ret$nuP
    
    dfB <- rbind(dfB, data.frame(
      AUC = RsmRocAuc,
      mu = mu, 
      lambda = lambdaP,
      nu = nuP, 
      a = a,
      b = b, 
      R_Swets = R_Swets
    ))
  }
  
  # Part C
  dfC <- data.frame()
  mu <- 2
  lambdaArr <- logseq(0.1, 5, 10)
  for (i in 1:length(lambdaArr)) {
    lambda <- lambdaArr[i]
    nu <- NA;  # NA indicates this is to be varied
    
    retParms <- FindParamFixAuc(mu, lambda, nu, lesionDist, RsmRocAuc)
    if (!is.na(retParms)) nu <- retParms else next
    ret <- effectiveAB(mu, lambda, nu, lesionDist)
    a <- ret$a
    b <- ret$b
    R_Swets  <-  a/(1-b)
    
    ret <- UtilIntrinsic2PhysicalRSM(mu, lambda, nu)
    lambdaP <- ret$lambdaP
    nuP <- ret$nuP
    
    dfC <- rbind(dfC, data.frame(
      AUC = RsmRocAuc,
      mu = mu, 
      lambda = lambdaP,
      nu = nuP, 
      a = a,
      b = b, 
      R_Swets = R_Swets
    ))
  }
  return(list(
    dfA = dfA,
    dfB = dfB,
    dfC = dfC
  ))
}

```


```{r main-rsm-swets-observations1, echo=FALSE}
ret <- do_one_value (0.7) 
dfA <- ret$dfA
dfB <- ret$dfB
dfC <- ret$dfC
```


```{r rsm-evidence-table1A, echo=FALSE}
colnames(dfA) <-c("$AUC$", "$\\mu$", "$\\lambda'$", "$\\nu'$", "$a_{eff}$", "$b_{eff}$", "$R_{Swets}$")
# escape = FALSE is critical in getting Math right
kbl(dfA, caption = "Here $\\lambda = 2$ is held constant while the other two parameters are varied to keep AUC at 0.7.", booktabs = TRUE, escape = FALSE) %>% collapse_rows(columns = c(1, 3), valign = "middle") %>% kable_styling(latex_options = c("basic", "scale_down", "HOLD_position"), row_label_position = "c") 
```


```{r rsm-evidence-table1B, echo=FALSE}
colnames(dfB) <-c("$AUC$", "$\\mu$", "$\\lambda'$", "$\\nu'$", "$a_{eff}$", "$b_{eff}$", "$R_{Swets}$")
# escape = FALSE is critical in getting Math right
kbl(dfB, caption = "Here $\\nu = 1$ is held constant while the other two parameters are varied to keep AUC at 0.7.", booktabs = TRUE, escape = FALSE) %>% collapse_rows(columns = c(1, 3), valign = "middle") %>% kable_styling(latex_options = c("basic", "scale_down", "HOLD_position"), row_label_position = "c") 
```



```{r rsm-evidence-table1C, echo=FALSE}
colnames(dfC) <-c("$AUC$", "$\\mu$", "$\\lambda'$", "$\\nu'$", "$a_{eff}$", "$b_{eff}$", "$R_{Swets}$")
# escape = FALSE is critical in getting Math right
kbl(dfC, caption = "Here $\\mu = 2$ is held constant while the other two parameters are varied to keep AUC at 0.7.", booktabs = TRUE, escape = FALSE) %>% collapse_rows(columns = c(1, 2), valign = "middle") %>% kable_styling(latex_options = c("basic", "scale_down", "HOLD_position"), row_label_position = "c") 
```

Shown next are three tables corresponding to RSM-AUC = 0.8.

* In Table \@ref(tab:rsm-evidence-table2A) $\lambda = 2$ is held constant while the other two parameters are varied to keep AUC at 0.8. 
* In Table \@ref(tab:rsm-evidence-table2B) $\nu = 1$ is held constant. 
* In Table \@ref(tab:rsm-evidence-table2C) $\mu = 2$ is held constant.


```{r main-rsm-swets-observations2, echo=FALSE}
ret <- do_one_value (0.8) 
dfA <- ret$dfA
dfB <- ret$dfB
dfC <- ret$dfC
```



```{r rsm-evidence-table2A, echo=FALSE}
colnames(dfA) <-c("$AUC$", "$\\mu$", "$\\lambda'$", "$\\nu'$", "$a_{eff}$", "$b_{eff}$", "$R_{Swets}$")
# escape = FALSE is critical in getting Math right
kbl(dfA, caption = "Here $\\lambda = 2$ is held constant while the other two parameters are varied to keep AUC at 0.8.", booktabs = TRUE, escape = FALSE) %>% collapse_rows(columns = c(1, 3), valign = "middle") %>% kable_styling(latex_options = c("basic", "scale_down", "HOLD_position"), row_label_position = "c") 
```


```{r rsm-evidence-table2B, echo=FALSE}
colnames(dfB) <-c("$AUC$", "$\\mu$", "$\\lambda'$", "$\\nu'$", "$a_{eff}$", "$b_{eff}$", "$R_{Swets}$")
# escape = FALSE is critical in getting Math right
kbl(dfB, caption = "Here $\\nu = 1$ is held constant while the other two parameters are varied to keep AUC at 0.8.", booktabs = TRUE, escape = FALSE) %>% collapse_rows(columns = c(1, 4), valign = "middle") %>% kable_styling(latex_options = c("basic", "scale_down", "HOLD_position"), row_label_position = "c") 
```



```{r rsm-evidence-table2C, echo=FALSE}
colnames(dfC) <-c("$AUC$", "$\\mu$", "$\\lambda'$", "$\\nu'$", "$a_{eff}$", "$b_{eff}$", "$R_{Swets}$")
# escape = FALSE is critical in getting Math right
kbl(dfC, caption = "Here $\\mu = 2$ is held constant while the other two parameters are varied to keep AUC at 0.8.", booktabs = TRUE, escape = FALSE) %>% collapse_rows(columns = c(1, 2), valign = "middle") %>% kable_styling(latex_options = c("basic", "scale_down", "HOLD_position"), row_label_position = "c") 
```



The six tables are organized into 2 x 3 groups. 

* Tables \@ref(tab:rsm-evidence-table1A), \@ref(tab:rsm-evidence-table1B) and \@ref(tab:rsm-evidence-table1C) correspond to RSM-AUC = 0.7, while Tables \@ref(tab:rsm-evidence-table2A), \@ref(tab:rsm-evidence-table2B) and \@ref(tab:rsm-evidence-table2C) correspond to RSM-AUC = 0.8.
* Tables \@ref(tab:rsm-evidence-table1A) and \@ref(tab:rsm-evidence-table2A) corresponds to fixed $\lambda$ and varying $\mu$ and $\nu$.
* Tables \@ref(tab:rsm-evidence-table1B) and \@ref(tab:rsm-evidence-table2B) corresponds to fixed $\nu$ and varying $\mu$ and $\lambda$.
* Tables \@ref(tab:rsm-evidence-table1C) and \@ref(tab:rsm-evidence-table2C) corresponds to fixed $\mu$ and varying $\lambda$ and $\nu$.


In each table the 1st column lists the constrained  AUC, set to either 0.7 or 0.8, the 2nd, 3rd and 4th columns list the RSM parameters, followed by the empirical a and b-parameters and the Swets ratio. 


The number of lesions per diseased case was set to one. The function `FindParamFixAuc()` finds the missing RSM parameter, indicated by initializing it with `NA`. The function `effectiveAB()` calculates the separation $a_{eff}$ and standard deviation ratio $b_{eff}$ (non-diseased to diseased) of the two distributions, after normalizing each to unit area. 

STOP
Examination of Tables \@ref(tab:rsm-evidence-table1A), \@ref(tab:rsm-evidence-table1B) and \@ref(tab:rsm-evidence-table1C) reveals, for both values of AUC, an inverse relation between $\mu$ and $b$ and an inverse relation between $\nu$ and b.  In (C), for fixed  , the only way to improve performance is by increasing  , i.e., by the observer getting better at finding lesions. Furthermore, Table 17.3 reveals an approximately constant value of   especially in Parts A and B. 

The biggest deviation from approximate constancy is observed in Part C: this could be due to unreasonably low values of  , e.g., 0.1, necessary in order to satisfy the AUC constraint. It should be noted that the Swets et al observations are based on only two datasets and they state that   = 4, their observed value, is probably not generally applicable. 

	
The empirical observation that b decreases with increasing lesion detectability is likely more generally true and the results, Table 17.3, support it. It makes physical sense if one assumes that sometimes a sample from the non-diseased distribution exceeds that from the diseased distribution. This implies the existence of two types of z-samples in diseased cases. Another explanation is the existence of heterogeneity in the distribution for diseased cases, e.g., a mix of easy and hard lesions, which too would tend to broaden the diseased distribution and make b < 1 and b decreasing with increasing lesion detectability. Putting all this together, the author makes the following prediction: if the possible location of the lesion is pre-specified (i.e., there is no location uncertainty and therefore latent NLs are eliminated) and all lesions have the same detectability, then one expects b = 1 and no dependence of b on lesion detectability. This is a fairly simple experiment to conduct using simulated Gaussian noise backgrounds and superposed Gaussian disk lesions41-43 with fixed contrast (using simulated clinical backgrounds is not recommended for this study as it could cause differences in lesion detectability depending on variations in the background in the immediate vicinity of the lesion).







 
Table 17.3: This table is organized into 3 parts, A, B and C. The remaining parameter of the RSM, the one held constant, is shown in the second header. Part A: This show that b decreases as   increases: the last two columns in part A list the b-parameter and the    to   ratio, non-diseased to diseased. This confirms that the model has the expected behavior noted by Swets, namely b decreases as   increases and the    to   ratio is approximately constant. A similar organization applies to the other parts of the table. Part B: this shows the dependence of the b-parameter and the   to   ratio on  . Part C: this shows the dependence of the b-parameter and the   to   ratio on  . In all examples, the b-parameter decreases as   increases or   increases. [All parameters in this table represent intrinsic values.]


### Explanation of data degeneracy
An ROC dataset is said to be degenerate if the corresponding ROC plot does not have any interior data points. Data degeneracy is a significant problem faced by the binormal model1,6,44, e.g., ROCFIT or RSCORE-II software. Degenerate datasets cannot be analyzed by binormal model. The RSM provides a natural explanation for such datasets, and as shown in Chapter 19, such datasets are readily fitted by the RSM. The CBM model2-4 also provides an alternative explanation for the data degeneracy and a method for fitting such datasets.

The reason for degenerate datasets is the existence of cases not providing any decision variable samples. Such cases are always binned in the lowest ROC:1 bin. The possibility of data degeneracy can be appreciated by examining Fig. 17.1 (in particular plots D through F, i.e., the higher values of  ). As   increases the accessible portion of the ROC curve shrinks and the curve increasingly approaches the top-left corner of the plot. The effect is particularly pronounced for observers characterized by large values of   and small values of  , i.e., the experts. For them the operating points will be clustered near the initial near-vertical section of the ROC curve. It will be difficult to get such observers to generate appreciable numbers of false positives. Instructions such as "spread your ratings"45 or the use "continuous" ratings46 may not always work; interference with the radiologist's readings style to make the data easier to analyze is undesirable. To the experimenter it will appear that the observer is not cooperating, when in fact they are being perfectly reasonable. A similar issue affected Dr. Swensson's LROC analysis method11 in which originally every case had to be assigned a "most-suspicious" region, even if the radiologist thought the case was perfectly normal: this met with resistance from radiologists. In later versions of his software, Dr. Swensson removed the forced localization requirement and instead did it in software by sampling a random number generator. Radiologists don't like to be told, "even if you believe the case is definitely normal, there must be some region that is least normal, or most suspicious". All of these sematic difficulties go away if one abandons the premise that every case must generate a z-sample.


### Predictions of observed FROC/AFROC/LROC curves
Besides predicting ROC, FROC and AFROC curves, as shown in this chapter, the RSM also predicts LROC curves15. Moreover, these are generally better fits to experimental data since they do not allow the AFROC and LROC curve to go continuously to FPF = 1, as do earlier models, two by the author22,23 and one by the late Dr. Swensson11. As a historical note, the FROCFIT and AFROC software developed22 by the author in 1989 was more successful at fitting microcalcification data than mass data (private communication, Prof. Heang-Ping Chan, ca. 1990). This is consistent with the premise that the microcalcification task is characterized by larger   than the mass task. Radiologists literally use a magnifying glass (a physical on or a software implementation) to search each image for the much smaller specks, and this increases the potential for finding NLs, hence the larger  . Mass detection is more a function of the global gestalt view described in the previous chapter. Larger  yields an FROC curve that traverses more to the right than the corresponding mass curve. The FROCFIT program allows the FROC curve to go far to the right and reach unit ordinate, which is not observed with mass data, but could approximate microcalcification data. 


## Discussion / Summary {#rsm-evidence-discussion-summary}
This chapter has detailed ROC, FROC and AFROC curves predicted by the radiological search model (RSM). All RSM-predicted curves share the constrained end-point property that is qualitatively different from previous ROC models. In the author's experience, it is a property that most researchers in this field have difficulty accepting. There is too much history going back to the early 1940s, of the ROC curve extending from (0,0) to (1,1) that one has to let go of, and this can be difficult. 

The author is not aware of any direct evidence that radiologists can move the operating point continuously in the range (0,0) to (1,1) in search tasks, so the existence of such an ROC is tantamount to an assumption. Algorithmic observers that do not involve the element of search can extend continuously to (1,1). An example of an algorithmic observer not involving search is a diagnostic test that rates the results of a laboratory measurement, e.g., the A1C measure of blood glucose  for presence of a disease. If A1C â‰¥ 6.5% the patient is diagnosed as diabetic. By moving the threshold from infinity to â€“infinity, and assuming a large population of patients, one can trace out the entire ROC curve from the origin to (1,1). This is because every patient yields an A1C value. Now imagine that some finite fraction of the test results are "lost in the mail"; then the ROC curve, calculated over all patients, would have the constrained end-point property, albeit due to an unreasonable cause.

The situation in medical imaging involving search tasks is qualitatively different. Not every case yields a decision variable. There is a reasonable cause for this â€“ to render a decision variable sample the radiologist must find something suspicious to report, and if none is found, there is no decision variable to report. The ROC curve calculated over all patients would exhibit the constrained end-point property, even in the limit of an infinite number of patients. If calculated over only those patients that yielded at least one mark, the ROC curve would extend from (0,0) to (1,1) but then one would be ignoring the cases with no marks, which represent valuable information: unmarked non-diseased cases represent perfect decisions and unmarked diseased cases represent worst-case decisions.

RSM-predicted ROC, FROC and AFROC curves were derived (wAFROC is implemented in the Rjafroc). These were used to demonstrate that the FROC is a poor descriptor of performance. Since almost all work to date, including some by the author47,48, has used FROC curves to measure performance, this is going to be difficulty for some to accept. The examples in Fig. 17.6 (A- F) and Fig. 17.7 (A-B) should convince one that the FROC curve is indeed a poor measure of performance. The only situation where one can safely use the FROC curve is if the two modalities produce curves extending over the same NLF range. This can happen with two variants of a CAD algorithm, but rarely with radiologist observers.

A unique feature is that the RSM provides measures of search and lesion-classification performance. It bears repeating that search performance is the ability to find lesions while avoiding finding non-lesions. Search performance can be determined from the position of the ROC end-point (which in turn is determined by RSM-based fitting of ROC data, Chapter 19). The perpendicular distance between the end-point and the chance diagonal is, apart from a factor of 1.414, a measure of search performance. All ROC models that predict continuous curves extending to (1,1), imply zero search performance. 

Lesion-classification performance is measured by the AUC value corresponding to the   parameter. Lesion-classification performance is the ability to discriminate between LLs and NLs, not between diseased and non-diseased cases: the latter is measured by RSM-AUC. There is a close analogy between the two ways of measuring lesion-classification performance and CAD used to find lesions in screening mammography vs. CAD used in the diagnostic context to determine if a lesion found at screening is actually malignant. The former is termed CADe, for CAD detection, which in the author's opinion, is slightly misleading as at screening lesions are found not detected ("detection" is "discover or identify the presence or existence of something ", correct localization is not necessarily implied; the more precise term is "localize"). In the diagnostic context one has CADx, for CAD diagnostic, i.e., given a specific region of the image, is the region malignant? 

Search and lesion-classification performance can be used as "diagnostic aids" to optimize performance of a reader. For example, is search performance is low, then training using mainly non-diseased cases is called for, so the resident learns the different variants of non-diseased tissues that can appear to be true lesions. If lesion-classification performance is low then training with diseased cases only is called for, so the resident learns the distinguishing features characterizing true lesions from non-diseased tissues that fake true lesions.

Finally, evidence for the RSM is summarized. Its correspondence to the empirical Kundel-Nodine model of visual search that is grounded in eye-tracking measurements. It reduces in the limit of large  , which guarantees that every case will yield a decision variable sample, to the binormal model; the predicted pdfs in this limit are not strictly normal, but deviations from normality would require very large sample size to demonstrate. Examples were given where even with 1200 cases the binormal model provides statistically good fits, as judged by the chi-square goodness of fit statistic, Table 17.2. Since the binormal model has proven quite successful in describing a large body of data, it satisfying that the RSM can mimic it in the limit of large  . The RSM explains most empirical results regarding binormal model fits: the common finding that b < 1; that b decreases with increasing lesion pSNR (large   and / or  ); and the finding that the difference in means divided by the difference in standard deviations is fairly constant for a fixed experimental situation, Table 17.3. The RSM explains data degeneracy, especially for radiologists with high expertise.

The contaminated binormal model2-4 (CBM), Chapter 20, which models the diseased distribution as having two peaks, one at zero and the other at a constrained value, also explains the empirical observation that b-parameter < 1 and data degeneracy. Because it allows the ROC curve to go continuously to (1,1), CBM does not completely account for search performance â€“ it accounts for search when it comes to finding lesions, but not for avoiding finding non-lesions.

The author does not want to leave the impression that RSM is the ultimate model. The current model does not predict satisfaction of search (SOS) effects27-29. Attempts to incorporate SOS effects in the RSM are in the early research stage. As stated earlier, the RSM is a first-order model: a lot of interesting science remains to be uncovered.

### The Wagner review

The two RSM papers12,13 were honored by being included in a list of 25 papers the "Highlights of 2006" in Physics in Medicine and Biology. As stated by the publisher: "I am delighted to present a special collection of articles that highlight the very best research published in Physics in Medicine and Biology in 2006. Articles were selected for their presentation of outstanding new research, receipt of the highest praise from our international referees, and the highest number of downloads from the journal website.

One of the reviewers was the late Dr. Robert ("Bob") Wagner â€“ he had an open-minded approach to imaging science that is lacking these days, and a unique writing style. The author reproduces one of his comments with minor edits, as it pertains to the most interesting and misunderstood prediction of the RSM, namely its constrained end-point property.

I'm thinking here about the straight-line piece of the ROC curve from the max to (1, 1). 
1.	This can be thought of as resulting from two overlapping uniform distributions (thus guessing) far to the left in decision space (rather than delta functions). Please think some more about this point--because it might make better contact with the classical literature. 
2.	BTW -- it just occurs to me (based on the classical early ROC work of Swets & co.) -- that there is a test that can resolve the issue that I struggled with in my earlier remarks. The experimenter can try to force the reader to provide further data that will fill in the space above the max point. If the results are a straight line, then the reader would just be guessing -- as implied by the present model. If the results are concave downward, then further information has been extracted from the data. This could require a great amount of data to sort out--but it's an interesting point (at least to me).


Dr. Wagner made two interesting points. With his passing, the author has been deprived of the penetrating and incisive evaluation of his ongoing work, which the author deeply misses. Here is the author's response (ca. 2006):

The need for delta functions at negative infinity can be seen from the following argument. Let us postulate two constrained width pdfs with the same shapes but different areas, centered at a common value far to the left in decision space, but not at negative infinity. These pdfs would also yield a straight-line portion to the ROC curve. However, they would be inconsistent with the search model assumption that some images yield no decision variable samples and therefore cannot be rated in bin ROC:2 or higher. Therefore, if the distributions are as postulated above then choice of a cutoff in the neighborhood of the overlap would result in some of these images being rated 2 or higher, contradicting the RSM assumption.  The delta function pdfs at negative infinity are seen to be a consequence of the search model. 

One could argue that when the observer sees nothing to report then he starts guessing and indeed this would enable the observer to move along the dashed portion of the curve. This argument implies that the observer knows when the threshold is at negative infinity, at which point the observer turns on the guessing mechanism (the observer who always guesses would move along the chance diagonal). In the author's judgment, this is unreasonable. The existence of two thresholds, one for moving along the non-guessing portion and one for switching to the guessing mode would require abandoning the concept of a single decision rule. To preserve this concept one needs the delta functions at negative infinity.

Regarding Dr. Wagner's second point, it would require a great amount of data to sort out whether forcing the observer to guess would fill in the dashed portion of the curve, but the author doubts it is worth the effort. Given the bad consequences of guessing (incorrect recalls) the author believes that in the clinical situation, the radiologist will never guess. If the radiologist sees nothing to report, nothing will be reported. In addition, the author believes that forcing the observer, to prove some research point, is not a good idea. 












## References {#rsm-evidence-references}
1.	Chakraborty DP. Computer analysis of mammography phantom images (CAMPI): An application to the measurement of microcalcification image quality of directly acquired digital images. Medical Physics. 1997;24(8):1269-1277.
2.	Chakraborty DP, Eckert MP. Quantitative versus subjective evaluation of mammography accreditation phantom images. Medical Physics. 1995;22(2):133-143.
3.	Chakraborty DP, Yoon H-J, Mello-Thoms C. Application of threshold-bias independent analysis to eye-tracking and FROC data. Academic Radiology. 2012;In press.
4.	Chakraborty DP. ROC Curves predicted by a model of visual search. Phys Med Biol. 2006;51:3463â€“3482.
5.	Chakraborty DP. A search model and figure of merit for observer data acquired according to the free-response paradigm. Phys Med Biol. 2006;51:3449â€“3462.

