[
["binaryTask.html", "Chapter 5 Modeling the Binary Task 5.1 Introduction 5.2 Decision variable and decision threshold 5.3 Changing the decision threshold: Example I 5.4 Changing the decision threshold: Example II 5.5 The equal-variance binormal model 5.6 The normal distribution 5.7 Analytic expressions for specificity and sensitivity 5.8 Demonstration of the concepts of sensitivity and specificity 5.9 Binary ratings 5.10 Calculating confidence intervals for sensitivity and specificity 5.11 Summary 5.12 Discussion 5.13 References", " Chapter 5 Modeling the Binary Task 5.1 Introduction Chapter 4 introduced measures of performance associated with the binary decision task. Described in this chapter is a 2-parameter statistical model for the binary task, in other words it shows how one can predict quantities like sensitivity and specificity based on the values of the parameters of a statistical model. It introduces the fundamental concepts of a decision variable and a decision threshold (the latter is one of the parameters of the statistical model) that pervade this book, and shows how the decision threshold can be altered by varying experimental conditions. The receiver-operating characteristic (ROC) plot is introduced which shows how the dependence of sensitivity and specificity on the decision threshold is exploited by a measure of performance that is independent of decision threshold, namely the area AUC under the ROC curve. AUC turns out to be related to the other parameter of the model. The dependence of variability of the operating point on the numbers of cases is explored, introducing the concept of random sampling and how the results become more stable with larger numbers of cases, or larger sample sizes. These are perhaps intuitively obvious concepts but it is important to see them demonstrated, Online Appendix 3.A. Formulae for 95% confidence intervals for estimates of sensitivity and specificity are derived and the calculations are shown explicitly, 5.2 Decision variable and decision threshold The model1 for the binary task involves three assumptions: (i) the existence of a decision variable associated with each case, (ii) the existence of a case-independent decision threshold for reporting individual cases as non-diseased or diseased and (iii) the adequacy of training session(s) in getting the observer to a steady state. In addition, common to all models is that the observer is “blinded” to the truth, while the researcher is not. 5.2.1 Existence of a decision variable Assumption 1: Each case presentation is associated with the occurrence (or realization) of a specific value of a random scalar sensory variable yielding a unidirectional measure of evidence of disease. The two italicized phrases introduce important terms. By sensory variable one means one that is sensed internally by the observer (in the cognitive system, associated with the brain) and as such is not directly measureable in the traditional physical sense. A physical measurement, for example, might consist of measuring a voltage difference across two points with a voltmeter. The term “latent” is often used to describe the sensory variable because it turns out that transforming this variable by an arbitrary monotonic non-decreasing transformation has no effect on the ROC – this will become clearer later. Alternative terms are “psychophysical variable”, “perceived variable”, “perceptual variable” or “confidence level”. The last term is the most common. It is a subjective variable since its value is expected to depend on the observer: the same case shown to different observers could evoke different values of the sensory variable. Since one cannot measure it anyway, it would be a very strong assumption to assume that the two sensations are identical. In this book the term “latent decision variable”, or simply “decision variable” is used, which hopefully gets away from the semantics and focuses instead on what the variable is used for, namely making decisions. The symbol Z will be used for it and specific realized values are termed z-samples. It is a random in the sense that it varies randomly from case to case; unless the cases are similar in some respect, for example, two variants of the same case under different image processing conditions, or images of twins; in these instances the corresponding decision variables are expected to be correlated. In the binary paradigm model to be described, the decision variables corresponding to different cases are assumed mutually independent. The latent decision variable rank-orders cases with respect to evidence for presence of disease. Unlike a traditional rank-ordering scheme, where “1” is the highest rank, the scale is inverted with larger values corresponding to greater evidence of disease. Without loss of generality, one assumes that the decision variable ranges from -∞ to +∞, with large positive values indicative of strong evidence for presence of disease, and large negative values indicative of strong evidence for absence of disease. The zero value indicates no evidence for presence or absence of disease. [The -∞ to +∞ scale is not an assumption. The decision variable scale could just as well range from a to b, where a &lt; b; with appropriate rescaling of the decision variable, there will be no changes in the rank-orderings, and the scale will extend from -∞ to +∞.] Such a decision scale, with increasing values corresponding to increasing evidence of disease, is termed positive-directed. 5.2.2 Existence of a decision threshold Assumption 2: In the binary decision task the radiologist adopts a single and fixed (i.e., case-independent) decision threshold and states: “case is diseased” if the decision variable is greater than or equal to , i.e., , and “case is non-diseased” if the decision variable is smaller than , i.e., . The decision threshold is a fixed value used to separate cases reported as diseased from cases reported as non-diseased. Unlike the random Z-sample, which varies from case to case, the decision threshold is held fixed for the duration of the study. In some of the older literature2 the decision threshold is sometimes referred to as “response bias”. The author hesitates to use the term “bias” which has a negative connotation, whereas, in fact, the choice of decision threshold depends on rational assessment of costs and benefits of different outcomes. The choice of decision threshold depends on the conditions of the study: perceived or known disease prevalence, cost-benefit considerations, instructions regarding dataset characteristics, personal interpreting style, etc. There is a transient “learning curve” during which observer is assumed to find the optimal threshold and henceforth holds it constant for the duration of the study. The learning is expected to stabilize during a sufficiently long training interval. Data should only be collected in the fixed threshold state, i.e., at the end of the training session. If a second study is conducted under different conditions, the observer will determine, after a new training session, the optimal threshold for the new conditions and henceforth hold it constant for the duration of the second study, etc. From assumption #2, it follows that: \\[\\begin{equation} 1-Sp=FPF=P(Z\\ge \\zeta|T=1) \\tag{5.1} \\end{equation}\\] \\[\\begin{equation} Se=TPF=P(Z\\ge \\zeta|T=2) \\tag{5.2} \\end{equation}\\] Explanation: \\(P(Z\\ge \\zeta|T=1)\\) is the probability that the Z-sample for a non-diseased case is greater than or equal to \\(\\zeta\\). According to assumption #2 these cases are incorrectly classified as diseased, i.e., they are FP decisions and the corresponding probability is false positive fraction \\(FPF\\), which is the complement of specificity \\(Sp\\). Likewise, \\(P(Z\\ge \\zeta|T=2)\\) denotes the probability that the Z-sample for a diseased case is greater than or equal to \\(\\zeta\\). These cases are correctly classified as diseased, i.e., these are TP decisions and the corresponding probability is true positive fraction \\(TPF\\), which is sensitivity \\(Se\\). There are several concepts implicit in Eqn. (5.1) and Eqn. (5.2). The Z-samples have an associated probability distribution; this is implicit in the notation \\(P(Z\\ge \\zeta|T=2)\\) and \\(P(Z\\ge \\zeta|T=1)\\). Diseased-cases are not homogenous; in some, disease is easy to detect, perhaps even obvious, in others the signs of disease are subtler, and in some, the disease is almost impossible to detect. Likewise, non-diseased cases are not homogenous. The probability distributions depend on the truth state \\(T\\). The distribution of the Z-samples for non-diseased cases is in general different from that for the diseased cases. Generally, the distribution for \\(T = 2\\) is shifted to the right of that for \\(T = 1\\) (assuming a positive-directed decision variable scale). Later, specific distributional assumptions will be employed to obtain analytic expressions for the right hand sides of Eqn. (5.1) and Eqn. (5.2). The equations imply that via choice of the decision threshold \\(\\zeta\\), \\(Se\\) and \\(Sp\\) are under the control of the observer. The lower the decision threshold the higher the sensitivity and the lower the specificity, and the converses are also true. Ideally both sensitivity and specificity should be large, i.e., unity (since they are probabilities they cannot exceed unity). The tradeoff between sensitivity and specificity says, essentially, that there is no “free lunch”. In general, the price paid for increased sensitivity is decreased specificity and vice-versa. 5.2.3 Adequacy of the training session Assumption 3: The observer has complete knowledge of the distributions of actually non-diseased and actually diseased cases and makes rational decision based on this knowledge. Knowledge of the probabilistic distributions is consistent with not knowing for sure which distribution a specific sample came from, i.e., the “blindedness” assumption common to all observer performance studies. How an observer can be induced to change the decision threshold is the subject of the following two examples. 5.3 Changing the decision threshold: Example I Suppose that in the first study a radiologist interprets a set of cases subject to the instructions that it is rather important to identify actually diseased cases and not to worry about misdiagnosing actually non-diseased cases. One way to do this would be to reward the radiologist with $10 for each TP decision but only $1 for each TN decision. For simplicity, assume there is no penalty imposed for incorrect decisions (FPs and FNs) and the case set contains equal numbers of non-diseased and diseased cases, and the radiologist is informed of these facts. It is also assumed that the radiologist is allowed to reach a steady state and responds rationally to the payoff arrangement. Under these circumstances, the radiologist is expected to set the decision threshold at a small value so that even slight evidence of presence of disease is enough to result in a “case is diseased” decision. The low decision threshold also implies that considerable evidence of lack of disease is needed before a “case is non-diseased” decision is rendered. The radiologist is expected to achieve relatively high sensitivity but specificity will be low. As a concrete example, if there are 100 non-diseased cases and 100 diseased cases, assume the radiologist makes 90 TP decisions; since the threshold for presence of presence of disease is small, this number is close to the maximum possible value, namely 100. Assume further that 10 TN decisions are made; since the implied threshold for evidence of absence of disease is large, this number is close to the minimum possible value, namely 0. Therefore, sensitivity is 90% and specificity is 10%. The radiologist earns 90 x $10 + 10 x $1 = $910 for participating in this study. Next, suppose the study is repeated with the same cases but this time the payoff is $1 for each TP decision and $10 for each TN decision. Suppose, further, that sufficient time has elapsed between the two study sessions that memory effects can be neglected. Now the roles of sensitivity and specificity are reversed. The radiologist’s incentive is to be correct on actually non-diseased cases without worrying too much about missing actually diseased cases. The radiologist is expected to set the decision threshold at a large value so that considerable evidence of disease-presence is required to result in a “case is diseased” decision, but even slight evidence of absence of disease is enough to result in a “case is non-diseased” decision. This radiologist is expected to achieve relatively low sensitivity but specificity will be higher. Assume the radiologist makes 90 TN decisions and 10 TP decisions, earning $910 for the second study. The corresponding sensitivity is 10% and specificity is 90%. The incentives in the first study caused the radiologist to accept low specificity in order to achieve high sensitivity; the incentives in the second study caused the radiologist to accept low sensitivity in order to achieve high specificity. 5.4 Changing the decision threshold: Example II Suppose one asks the same radiologist to interpret a set of cases, but this time the reward for a correct decision is always $1, regardless of the truth state of the case, and as before, there are is no penalty for incorrect decisions. However, the radiologist is told that disease prevalence is only 0.005 and that this is the actual prevalence, i.e., the experimenter is not deceiving the radiologist in this regard. [Even if the experimenter attempts to deceive the radiologist, by claiming for example that there are roughly equal numbers of non-diseased and diseased cases, after interpreting a few tens of cases the radiologist will know that a deception is involved. Deception in such studies is generally not a good idea, as the observer’s performance is not being measured in a “steady state condition” – the observer’s performance will change as the observer “learns” the true disease prevalence.] In other words, only five out of every 1000 cases are actually diseased. This information will cause the radiologist to adopt a high threshold for diagnosing disease-present thereby becoming more reluctant to state: “case is diseased”. By simply diagnosing all cases as non-diseased, without using any case information, the radiologist will be correct on every disease absent case and earn $995, which is very close to the maximum $1000 the radiologist can earn by using case information to the full and being correct on disease-present and disease-absent cases. The example is not as contrived as might appear at first sight. However, in screening mammography, the cost of missing a breast cancer, both in terms of loss of life and a possible malpractice suite, is usually perceived to be higher than the cost of a false positive. This can result in a shift towards higher sensitivity at the expense of lower specificity. If a new study were conducted with a highly enriched set of cases, where the disease prevalence is 0.995 (i.e., only 5 out of every 1000 cases are actually non-diseased), then the radiologist would adopt a low threshold. By simply calling every case “non-diseased”, the radiologist earns $995. These examples show that by manipulating the relative costs of correct vs. incorrect decisions and / or by varying disease prevalence one can influence the radiologist’s decision threshold. These examples apply to laboratory studies. Clinical interpretations are subject to different cost-benefit considerations that are generally not under the researcher’s control: actual (population) disease prevalence, the reputation of the radiologist, malpractice, etc. 5.5 The equal-variance binormal model Here is the model for the Z-samples. Using the notation \\(N(\\mu,\\sigma^2)\\) for the normal (or “Gaussian”) distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), it is assumed: 1. The Z-samples for non-diseased cases are distributed \\(N(0,1)\\). 2. The Z-samples for diseased cases are distributed \\(N(\\mu,1)\\) with \\(\\mu&gt;0\\). 3. A case is diagnosed as diseased if its Z-sample ≥ a constant threshold \\(\\zeta\\), and non-diseased otherwise. The constraint \\(\\mu&gt;0\\) is needed so that the observer’s performance is at least as good as chance. A large negative value for this parameter would imply an observer so predictably bad that the observer is good; one simply reverses the observer’s decision (“diseased” to “non-diseased” and vice versa) to get near-perfect performance . The model described above is termed the equal-variance binormal model. [If the common variance is not unity, one can re-scale the decision axis to achieve unit-variance without changing the predictions of the model.] A more general model termed the unequal-variance binormal model is generally used for modeling human observer data, discussed later, but for the moment, one does not need that complication. The equal-variance binormal model is defined by: \\[\\begin{equation} \\left.\\begin{matrix} Z_{k_tt} \\sim N(\\mu_t,1) \\\\ \\mu_1=0\\\\ \\mu_2=\\mu \\end{matrix}\\right\\} \\tag{5.3} \\end{equation}\\] In Eqn. (5.3) the subscript \\(t\\) denotes the truth, sometimes referred to as the “gold standard”, with \\(t = 1\\) denoting a non-diseased case and \\(t = 2\\) denoting a diseased case. The variable \\(Z_{k_tt}\\) denotes the random Z-sample for case \\(k_tt\\), where \\(k_t\\) is the index for cases with truth state \\(t\\); for example \\(k_11=21\\) denotes the 21st non-diseased case and \\(k_22=3\\) denotes the 3rd diseased case. To explicate \\(k_11=21\\) further, the label \\(k_1\\) indexes the case while the label \\(1\\) indicates the truth of the case. The label \\(k_t\\) ranges from \\(1,2,...,K_t\\) , where \\(K_t\\)$ is the total number of cases with disease state \\(t\\). The author departs from usual convention, see for example paper by Hillis, which labels the cases with a single index \\(k\\), which ranges from 1 to \\(K_1+K_2\\), and one is left guessing as to the truth-state of each case. Also, the proposed notation extends readily to the FROC paradigm where two states of truth have to be distinguished, one at the case level and one at the location level. The first line in Eqn. (5.3) states that \\(Z_{k_tt}\\) is a random sample from the \\(N(\\mu_t,1)\\) distribution, which has unit variance regardless of the value of \\(t\\) (this is the reason for naming it the equal-variance binormal model). The remaining lines in Eqn. (5.3) defines \\(\\mu_1\\) as zero and \\(\\mu_2\\) as \\(\\mu\\). Taken together, these equations state that non-diseased case Z-samples are distributed \\(N(0,1)\\) and diseased case Z-samples are distributed \\(N(\\mu,1)\\). The name binormal arises from the two normal distributions underlying this model. It should not be confused with bivariate, which identifies a single distribution yielding two values per sample, where the two values could be correlated. In the binormal model, the samples from the two distributions are assumed independent of each other. A few facts concerning the normal (or Gaussian) distribution are summarized next. 5.6 The normal distribution In probability theory, a probability density function (pdf), or density of a continuous random variable, is a function giving the relative chance that the random variable takes on a given value. For a continuous distribution, the probability of the random variable being exactly equal to a given value is zero. The probability of the random variable falling in a range of values is given by the integral of this variable’s pdf function over that range. For the normal distribution \\(N(\\mu,\\sigma^2)\\) the pdf is denoted \\(\\phi(z|\\mu,\\sigma)\\). By definition, \\[\\begin{equation} \\phi\\left ( z|\\mu,\\sigma \\right )=P(z&lt;Z&lt;z+dz|Z \\sim N(\\mu,\\sigma^2)) \\tag{5.4} \\end{equation}\\] The right hand side of Eqn. (5.4) is the probability that the random variable \\(Z\\), sampled from \\(N(\\mu,\\sigma^2)\\), is between the fixed limits z and z + dz. For this reason \\(\\phi(z|\\mu,\\sigma)\\) is termed the probability density function. The special case \\(\\phi(z|0,1)\\) is referred to as the unit normal distribution; it has zero mean and unit variance and the corresponding pdf is denoted \\(\\phi(z)\\). The defining equation for the pdf of this distribution is: \\[\\begin{equation} \\phi\\left ( z \\right )=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left ( -\\frac{z^2}{2} \\right ) \\tag{5.5} \\end{equation}\\] The integral of \\(\\phi(t)\\) from \\(-\\infty\\) to \\(z\\), as in Eqn. (5.6), is the probability that a sample from the unit normal distribution is less than \\(z\\). Regarded as a function of \\(z\\), this is termed the cumulative distribution function (CDF) and is denoted, in this book, by the symbol \\(\\Phi\\). The function \\(\\Phi(z)\\), specific to the unit normal distribution, is defined by: \\[\\begin{equation} \\Phi\\left ( z \\right )=\\int_{-\\infty }^{z}\\phi(t)dt \\tag{5.6} \\end{equation}\\] Fig. 5.1 shows plots, as functions of z, of the CDF and the pdf for the unit normal distribution. Since z-samples outside ±3 are unlikely, the plotted range, from -3 to +3 includes most of the distribution. The pdf is the familiar bell-shaped curve, centered at zero; the corresponding R function is dnorm(), i.e., density of the normal distribution. The CDF \\(\\Phi(z)\\) increases monotonically from 0 to unity as z increases from \\(-\\infty\\) to \\(+\\infty\\). It is the sigmoid (S-shaped) shaped curve in Fig. 5.1; the corresponding R function is pnorm(). The sigmoid shaped curve is the CDF, or cumulative distribution function, of the N(0,1) distribution, while the bell-shaped curve is the corresponding pdf, or probability density function. The dashed line corresponds to the reporting threshold \\(\\zeta\\). The area under the pdf to the left of \\(\\zeta\\) equals the value of CDF at the selected \\(\\zeta\\), i.e., 0.841 (pnorm(1) = 0.841). x &lt;- seq(-3,3,0.01) pdfData &lt;- data.frame(z = x, pdfcdf = dnorm(x)) # plot the CDF cdfData &lt;- data.frame(z = x, pdfcdf = pnorm(x)) pdfcdfPlot &lt;- ggplot( mapping = aes(x = z, y = pdfcdf)) + geom_line(data = pdfData) + geom_line(data = cdfData) + geom_vline(xintercept = 1, linetype = 2) + xlab(label = &quot;z&quot;) + ylab(label = &quot;pdf/CDF&quot;) print(pdfcdfPlot) FIGURE 5.1: pdf-CDF plots for unit normal. A related function is the inverse of Eqn. (5.6). Suppose the left hand side of Eqn. (5.6) is denoted \\(p\\), which is a probability in the range 0 to 1. \\[\\begin{equation} p=\\Phi\\left ( z \\right )=\\int_{-\\infty }^{z}\\phi(t)dt \\tag{5.7} \\end{equation}\\] The inverse of \\(\\Phi(z)\\) is that function which when applied to \\(p\\) yields the upper limit \\(z\\) in Eqn. (5.6), i.e., \\[\\begin{equation} \\Phi^{-1}(p) = z \\tag{5.8} \\end{equation}\\] Since \\(p=\\Phi(z)\\) it follows that \\[\\begin{equation} \\Phi(\\Phi^{-1}(z))=z \\tag{5.9} \\end{equation}\\] This nicely satisfies the property of an inverse function. The inverse function is known in statistical terminology as the quantile function, implemented in R as the qnorm() function. Think of pnorm() as a probability and qnorm() as value on the z-axis. To summarize, norm implies the unit normal distribution, p denotes a probability distribution function or CDF, q denotes a quantile function and d denotes a density function; this convention is used with all distributions in R. qnorm(0.025) #&gt; [1] -1.959964 qnorm(1-0.025) #&gt; [1] 1.959964 pnorm(qnorm(0.025)) #&gt; [1] 0.025 qnorm(pnorm(-1.96)) #&gt; [1] -1.96 The first command qnorm(0.025) demonstrates the identity: \\[\\begin{equation} \\Phi^{-1}(0.025)=-1.959964 \\tag{5.10} \\end{equation}\\] The next command qnorm(1-0.025) demonstrates the identity: \\[\\begin{equation} \\Phi^{-1}(1-0.025)=+1.959964 \\tag{5.11} \\end{equation}\\] The last two commands demonstrate that pnorm and qnorm, applied in either order, are inverses of each other. Eqn. (5.10) means that the (rounded) value -1.96 is such that the area under the pdf to the left of this value is 0.025. Similarly, Eqn. (5.11) means that the (rounded) value +1.96 is such that the area under the pdf to the left of this value is 1-0.025 = 0.975. In other words, -1.96 captures, to its left, the 2.5th percentile of the unit-normal distribution, and 1.96 captures, to its left, the 97.5th percentile of the unit-normal distribution, Fig. 5.2. Since between them they capture 95% of the unit-normal pdf, these two values can be used to estimate 95% confidence intervals. mu &lt;- 0;sigma &lt;- 1 zeta &lt;- -qnorm(0.025) step &lt;- 0.1 LL&lt;- -3 UL &lt;- mu + 3*sigma x.values &lt;- seq(zeta,UL,step) cord.x &lt;- c(zeta, x.values,UL) cord.y &lt;- c(0,dnorm(x.values),0) z &lt;- seq(LL, UL, by = step) curveData &lt;- data.frame(z = z, pdfs = dnorm(z)) shadeData &lt;- data.frame(z = cord.x, pdfs = cord.y) shadedTails &lt;- ggplot(mapping = aes(x = z, y = pdfs)) + geom_polygon(data = shadeData, color = &quot;grey&quot;, fill = &quot;grey&quot;) zeta &lt;- qnorm(0.025) x.values &lt;- seq(LL, zeta,step) cord.x &lt;- c(LL, x.values,zeta) cord.y &lt;- c(0,dnorm(x.values),0) shadeData &lt;- data.frame(z = cord.x, pdfs = cord.y) shadedTails &lt;- shadedTails + geom_polygon(data = shadeData, color = &quot;grey&quot;, fill = &quot;grey&quot;) + xlab(label = &quot;z&quot;) shadedTails &lt;- shadedTails + geom_line(data = curveData, color = &quot;black&quot;) print(shadedTails) FIGURE 5.2: Illustrating that 95% of the total area under the unit normal pdf is contained in the range |Z| &lt; 1.96, which can be used to construct a 95% confidence interval for an estimate of a suitably normalized statistic. The area contained in each shaded tail is 2.5%. If one knows that a variable is distributed as a unit-normal random variable, then the observed value minus 1.96 defines the lower limit of its 95% confidence interval, and the observed value plus 1.96 defines the upper limit of its 95% confidence interval. 5.7 Analytic expressions for specificity and sensitivity Specificity corresponding to threshold \\(\\zeta\\) is the probability that a Z-sample from a non-diseased case is smaller than \\(\\zeta\\). By definition, this is the CDF corresponding to the threshold \\(\\zeta\\). In other words: \\[\\begin{equation} Sp\\left ( \\zeta \\right )=P\\left ( Z_{k_11} &lt; \\zeta\\mid Z_{k_11} \\sim N\\left ( 0,1 \\right )\\right ) = \\Phi\\left ( \\zeta \\right ) \\tag{5.12} \\end{equation}\\] The expression for sensitivity can be derived tediously by starting with the fact that \\(Z_{k_22}\\) and then using calculus to obtain the probability that a z-sample for a disease-present case exceeds \\(\\zeta\\). A quicker way is to consider the random variable obtaining by shifting the origin to \\(\\mu\\). A little thought should convince the reader that \\(Z_{k_22}-\\mu\\) must be distributed as \\(N(0,1)\\). Therefore, the desired probability is (the last step follows from the identity in Eqn. (3.7), with z replaced by \\(\\zeta-\\mu\\) : \\[\\begin{equation} Se\\left ( \\zeta \\right )\\\\ =P\\left ( Z_{k_22} \\geq \\zeta\\right ) \\\\ =P\\left (\\left ( Z_{k_22} -\\mu \\right ) \\geq\\left ( \\zeta -\\mu \\right )\\right ) \\\\ =1-P\\left (\\left ( Z_{k_22} -\\mu \\right ) &lt; \\left ( \\zeta -\\mu \\right )\\right ) \\\\ = 1-\\Phi\\left ( \\zeta -\\mu \\right ) \\tag{5.13} \\end{equation}\\] A little thought (based on the definition of the CDF function and the symmetry of the unit-normal pdf function) should convince the reader that: \\[\\begin{equation} 1-\\Phi(\\zeta)=-\\Phi(\\zeta)\\\\ 1-\\Phi(\\zeta-\\mu)=\\Phi(\\mu-\\zeta) \\tag{5.13} \\end{equation}\\] Instead of carrying the “1 minus” around, one can use the more compact notation. Summarizing, the analytical formulae for the specificity and sensitivity for the equal-variance binormal model are: \\[\\begin{equation} Sp\\left ( \\zeta \\right ) = \\Phi(\\zeta)\\\\ Se\\left ( \\zeta \\right ) = \\Phi(\\mu-\\zeta) \\tag{5.14} \\end{equation}\\] In these equations, the threshold \\(\\zeta\\) appears with different signs because specificity is the area under a pdf to the left of a threshold, while sensitivity is the area to the right. As probabilities, both sensitivity and specificity are restricted to the range 0 to 1. The observer’s performance could be characterized by specifying sensitivity and specificity, i.e., a pair of numbers. If both sensitivity and specificity of an imaging system are greater than the corresponding values for another system, then the 1st system is unambiguously better than the 2nd. But what if sensitivity is greater for the 1st but specificity is greater for the 2nd? Now the comparison is ambiguous. It is difficult to unambiguously compare two pairs of performance indices. Clearly, a scalar measure is desirable that combines sensitivity and specificity into a single measure of diagnostic performance. The parameter \\(\\mu\\) satisfies the requirements of a scalar figure of merit (FOM). Eqn. (5.14) can be solved for \\(\\mu\\) as follows. Inverting the equations yields: \\[\\begin{equation} \\zeta =\\Phi^{-1} \\left (Sp\\left ( \\zeta \\right ) \\right )\\\\ \\mu - \\zeta = \\Phi^{-1} \\left (Se\\left ( \\zeta \\right ) \\right ) \\tag{5.15} \\end{equation}\\] Eliminating \\(\\zeta\\) yields: \\[\\begin{equation} \\mu = \\Phi^{-1} \\left (Sp\\left ( \\zeta \\right ) \\right ) + \\Phi^{-1} \\left (Se\\left ( \\zeta \\right ) \\right ) \\tag{5.16} \\end{equation}\\] This is a useful relation, as it converts a pair of numbers that is hard to compare between two modalities, in the sense described above, into a single FOM. Now it is almost trivial to compare two modalities: the one with the higher \\(\\mu\\) wins. In reality, the comparison is not trivial since like sensitivity and specificity, \\(\\mu\\) has to be estimated from a finite dataset and is therefore subject to sampling variability. options(digits=3) mu &lt;- 3;sigma &lt;- 1 zeta &lt;- 1 step &lt;- 0.1 lowerLimit&lt;- -1 # lower limit upperLimit &lt;- mu + 3*sigma # upper limit z &lt;- seq(lowerLimit, upperLimit, by = step) pdfs &lt;- dnorm(z) seqNor &lt;- seq(zeta,upperLimit,step) cord.x &lt;- c(zeta, seqNor,upperLimit) # need two y-coords at each end point of range; # one at zero and one at value of function cord.y &lt;- c(0,dnorm(seqNor),0) curveData &lt;- data.frame(z = z, pdfs = pdfs) shadeData &lt;- data.frame(z = cord.x, pdfs = cord.y) shadedPlots &lt;- ggplot(mapping = aes(x = z, y = pdfs)) + geom_line(data = curveData, color = &quot;blue&quot;) + geom_polygon(data = shadeData, color = &quot;blue&quot;, fill = &quot;blue&quot;) crossing &lt;- uniroot(function(x) dnorm(x) - dnorm(x,mu,sigma), lower = 0, upper = 3)$root crossing &lt;- max(c(zeta, crossing)) seqAbn &lt;- seq(crossing,upperLimit,step) cord.x &lt;- c(seqAbn, rev(seqAbn)) # reason for reverse # we want to explicitly define the polygon # we dont want R to close it cord.y &lt;- c() for (i in seq(1,length(cord.x)/2)) { cord.y &lt;- c(cord.y,dnorm(cord.x[i],mu, sigma)) } for (i in seq(1,length(cord.x)/2)) { cord.y &lt;- c(cord.y,dnorm(cord.x[length(cord.x)/2+i])) } pdfs &lt;- dnorm(z, mu, sigma) curveData &lt;- data.frame(z = z, pdfs = pdfs) shadeData &lt;- data.frame(z = cord.x, pdfs = cord.y) shadedPlots &lt;- shadedPlots + geom_line(data = curveData, color = &quot;red&quot;) + geom_polygon(data = shadeData, color = &quot;red&quot;, fill = &quot;red&quot;) seqAbn &lt;- seq(zeta,upperLimit,step) for (i in seqAbn) { # define xs and ys of two points, separated only along y-axis vlineData &lt;- data.frame(x1 = i, x2 = i, y1 = 0, y2 = dnorm(i, mu, sigma)) # draw vertical line between them shadedPlots &lt;- shadedPlots + geom_segment(aes(x = x1, xend = x2, y = y1, yend = y2), data = vlineData, color = &quot;red&quot;) } shadedPlots &lt;- shadedPlots + xlab(label = &quot;z-sample&quot;) print(shadedPlots) FIGURE 5.3: The equal-variance binormal model for mu = 3 and zeta = 1; the blue curve, centered at zero, corresponds to the pdf of non-diseased cases and the red one, centered at mu = 3, corresponds to the pdf of diseased cases. The left edge of the blue shaded region represents the threshold zeta, currently set at unity. The red shaded area, including the common portion with the vertical red lines, is sensitivity. The blue shaded area including the common portion with the vertical red lines is 1-specificity. Fig. 5.3 shows the equal-variance binormal model for \\(\\mu = 3\\) and \\(\\zeta = 1\\). The blue-shaded area, including the “common” portion with the vertical red lines, is the probability that a z-sample from a non-diseased case exceeds \\(\\zeta = 1\\), which is the complement of specificity, i.e., it is false positive fraction, which is 1 - pnorm(1) = 0.159. The red shaded area, including the “common” portion with the vertical red lines, is the probability that a z-sample from a diseased case exceeds \\(\\zeta = 1\\), which is sensitivity or true positive fraction, which is pnorm(3-1)= 0.977. Demonstrated next are these concepts using R examples. 5.8 Demonstration of the concepts of sensitivity and specificity 5.8.1 Estimating mu from a finite sample The following code simulates 9 non-diseased and 11 diseased cases. The \\(\\mu\\) parameter is 1.5 and \\(\\zeta\\) is \\(\\mu/2\\). Shown are the calculations of sensitivity and specificity and the value of estimated \\(\\mu\\). mu &lt;- 1.5 zeta &lt;- mu/2 seed &lt;- 100 # line 4 K1 &lt;- 9 K2 &lt;- 11 ds &lt;- simulateDataset(K1, K2, mu, zeta, seed) cat(&quot;seed = &quot;, seed, &quot;\\nK1 = &quot;, K1, &quot;\\nK2 = &quot;, K2, &quot;\\nSpecificity = &quot;, ds$Sp, &quot;\\nSensitivity = &quot;, ds$Se, &quot;\\nEst. of mu = &quot;, ds$mu, &quot;\\n&quot;) #&gt; seed = 100 #&gt; K1 = 9 #&gt; K2 = 11 #&gt; Specificity = 0.889 #&gt; Sensitivity = 0.909 #&gt; Est. of mu = 2.56 Since this is a finite sample, the estimate of \\(\\mu\\) is not exactly equal to the true value. In fact, all of the estimates, sensitivity, specificity and \\(\\mu\\) are subject to sampling variability. 5.8.2 Changing the seed variable: case-sampling variability No matter how many times one runs the above code, one always sees the same output shown above. This is because at line 4 one sets the seed of the random number generator to a fixed value, namely 100. This is like having a perfectly reproducible reader repeatedly interpreting the same cases – one always gets the same results. Change the seed to 101. One should see: seed &lt;- 101 # change ds &lt;- simulateDataset(K1, K2, mu, zeta, seed) cat(&quot;seed = &quot;, seed, &quot;\\nK1 = &quot;, K1, &quot;\\nK2 = &quot;, K2, &quot;\\nSpecificity = &quot;, ds$Sp, &quot;\\nSensitivity = &quot;, ds$Se, &quot;\\nEst. of mu = &quot;, ds$mu, &quot;\\n&quot;) #&gt; seed = 101 #&gt; K1 = 9 #&gt; K2 = 11 #&gt; Specificity = 0.778 #&gt; Sensitivity = 0.545 #&gt; Est. of mu = 0.879 Changing seed is equivalent to sampling a completely new set of patients. This is an example of case sampling variability. The effect is quite large (Se fell from 0.909 to 0.545 and estimated mu fell from 2.56 to 0.879!) because the size of the relevant case set, \\(K_2=11\\) for sensitivity, is rather small, leading to large variability. 5.8.3 Increasing the numbers of cases Here we increase \\(K_1\\) and \\(K_2\\), by a factor of 10 each, and return the seed to 100. K1 &lt;- 90 # change K2 &lt;- 110 # change seed &lt;- 100 # change ds &lt;- simulateDataset(K1, K2, mu, zeta, seed) cat(&quot;seed = &quot;, seed, &quot;\\nK1 = &quot;, K1, &quot;\\nK2 = &quot;, K2, &quot;\\nSpecificity = &quot;, ds$Sp, &quot;\\nSensitivity = &quot;, ds$Se, &quot;\\nEst. of mu = &quot;, ds$mu, &quot;\\n&quot;) #&gt; seed = 100 #&gt; K1 = 90 #&gt; K2 = 110 #&gt; Specificity = 0.778 #&gt; Sensitivity = 0.836 #&gt; Est. of mu = 1.74 Next we change seed to 101. seed &lt;- 101 # change ds &lt;- simulateDataset(K1, K2, mu, zeta, seed) cat(&quot;seed = &quot;, seed, &quot;\\nK1 = &quot;, K1, &quot;\\nK2 = &quot;, K2, &quot;\\nSpecificity = &quot;, ds$Sp, &quot;\\nSensitivity = &quot;, ds$Se, &quot;\\nEst. of mu = &quot;, ds$mu, &quot;\\n&quot;) #&gt; seed = 101 #&gt; K1 = 90 #&gt; K2 = 110 #&gt; Specificity = 0.811 #&gt; Sensitivity = 0.755 #&gt; Est. of mu = 1.57 Notice that now the values are less sensitive to seed. TBA Table 3.2 illustrates this trend with ever increasing sample sizes (the reader should confirm the listed values). results &lt;- array(dim = c(9,6)) results[1,] &lt;- c(9, 11, 100, 0.889, 0.909, 2.56) results[2,] &lt;- c(9, 11, 101, 0.778, 0.545, 0.879) results[3,] &lt;- c(90, 110, 100, 0.778, 0.836, 1.74) results[4,] &lt;- c(90, 110, 101, 0.811, 0.755, 1.57) results[5,] &lt;- c(900, 1100, 100, 0.764, 0.761, 1.43) results[6,] &lt;- c(900, 1100, 101, 0.807, 0.759, 1.57) results[7,] &lt;- c(9000, 11000, 100, 0.774, 0.772, 1.5) results[8,] &lt;- c(9000, 11000, 101, 0.771, 0.775, 1.5) results[9,] &lt;- c(Inf, Inf, NA, 0.773, 0.773, 1.5) df &lt;- as.data.frame(results) colnames(df) &lt;- c(&quot;K1&quot;,&quot;K2&quot;,&quot;seed&quot;,&quot;Se&quot;,&quot;Sp&quot;,&quot;mu&quot;) TABLE 5.1: Effect of sample size on case-sampling variability of estimates of sensitivity, specificity and the separation parameter. K1 K2 seed Se Sp mu 9 11 100 0.889 0.909 2.560 9 11 101 0.778 0.545 0.879 90 110 100 0.778 0.836 1.740 90 110 101 0.811 0.755 1.570 900 1100 100 0.764 0.761 1.430 900 1100 101 0.807 0.759 1.570 9000 11000 100 0.774 0.772 1.500 9000 11000 101 0.771 0.775 1.500 Inf Inf NA 0.773 0.773 1.500 5.9 Binary ratings # Line 1 # ... # ... seed &lt;- 100;set.seed(seed) K1 &lt;- 9;K2 &lt;- 11;mu &lt;- 1.5;zeta &lt;- mu/2 z1 &lt;- rnorm(K1) z2 &lt;- rnorm(K2) + mu nTN &lt;- length(z1[z1 &lt; zeta]) nTP &lt;- length(z2[z2 &gt;= zeta]) Sp &lt;- nTN/K1;Se &lt;- nTP/K2 cat(&quot;seed = &quot;, seed, &quot;\\nK1 = &quot;, K1, &quot;\\nK2 = &quot;, K2, &quot;\\nSpecificity = &quot;, Sp, &quot;\\nSensitivity = &quot;, Se, &quot;\\n&quot;) #&gt; seed = 100 #&gt; K1 = 9 #&gt; K2 = 11 #&gt; Specificity = 0.889 #&gt; Sensitivity = 0.909 Line 4 sets the seed of the random number generator to 100: this causes the random number generator to yield the same sequence of “random” numbers every time it is run. This is useful during initial code development and for showing the various steps of the example (if seed &lt;- NULL the random numbers would be different every time, making it harder for me, from a pedagogical point of view, to illustrate the steps). Line 5 initializes variables K1 and K2, which represent the number of non-diseased cases and the number of diseased cases, respectively. In this example 9 non-diseased and 11 diseased cases are simulated. Line 5 also initializes the parameter mu &lt;- 1.5 (mu corresponds to the separation parameter of the simulation model). Finally, this line initializes zeta, which corresponds to the threshold for declaring cases as diseased, to mu/2, i.e., halfway between the means of the two distributions defining the binormal model. Later one can experiment with other values. [Note that multiple statements can be put on a single line as long as semi-colons separate them. The author prefers the “vertical length” of the program to be short, a personal preference that gives the author a better perspective of the code.] Line 6 calls the built-in function rnorm() – for random sample(s) from a normal distribution - with argument K1, which yields K1 (9 in our example) samples from a unit normal distribution N(0,1). Arguments to a function are always comma separated and contained within enclosing parentheses. The samples are assigned to the variable z1 (for z-samples for non-diseased cases). The corresponding samples for the diseased cases, line 7, denoted z2, were obtained using rnorm(K2) + mu. [Alternatively one could have used rnorm(K2, mean = mu), which cause the value mu to override the default value - zero - of the mean of the normal distribution.] Since mu was initialized to 1.5, this line yields 11 samples from a normal distribution with mean zero and unit variance and adds 1.5 to all samples (if one wishes to sample from a distribution with a different variance, for example “3”, one needs to also insert the standard deviation argument, e.g., sd = sqrt(3), in the call to rnorm()). The modifications to the default values can be inserted, separated by commas, in any order, but the names mean and sd must match; try typing rnorm(K1, mean1 = 0) in the console window, one should see an error message. 5.10 Calculating confidence intervals for sensitivity and specificity options(digits=3) seed &lt;- 100;set.seed(seed) alpha &lt;- 0.05;K1 &lt;- 99;K2 &lt;- 111;mu &lt;- 5;zeta &lt;- mu/2 cat(&quot;alpha = &quot;, alpha, &quot;\\nK1 = &quot;, K1, &quot;\\nK2 = &quot;, K2, &quot;\\nmu = &quot;, mu, &quot;\\nzeta = &quot;, zeta, &quot;\\n&quot;) #&gt; alpha = 0.05 #&gt; K1 = 99 #&gt; K2 = 111 #&gt; mu = 5 #&gt; zeta = 2.5 z1 &lt;- rnorm(K1) z2 &lt;- rnorm(K2) + mu nTN &lt;- length(z1[z1 &lt; zeta]) nTP &lt;- length(z2[z2 &gt;= zeta]) Sp &lt;- nTN/K1;Se &lt;- nTP/K2 cat(&quot;Specificity = &quot;, Sp, &quot;\\nSensitivity = &quot;, Se, &quot;\\n&quot;) #&gt; Specificity = 0.99 #&gt; Sensitivity = 0.991 # Approx binomial tests cat(&quot;approx 95% CI on Specificity = &quot;, -abs(qnorm(alpha/2))*sqrt(Sp*(1-Sp)/K1)+Sp, +abs(qnorm(alpha/2))*sqrt(Sp*(1-Sp)/K1)+Sp,&quot;\\n&quot;) #&gt; approx 95% CI on Specificity = 0.97 1.01 # Exact binomial test ret &lt;- binom.test(nTN, K1, p = nTN/K1) cat(&quot;Exact 95% CI on Specificity = &quot;, as.numeric(ret$conf.int),&quot;\\n&quot;) #&gt; Exact 95% CI on Specificity = 0.945 1 # Approx binomial tests cat(&quot;approx 95% CI on Sensitivity = &quot;, -abs(qnorm(alpha/2))*sqrt(Se*(1-Se)/K2)+Se, +abs(qnorm(alpha/2))*sqrt(Se*(1-Se)/K2)+Se,&quot;\\n&quot;) #&gt; approx 95% CI on Sensitivity = 0.973 1.01 # Exact binomial test ret &lt;- binom.test(nTP, K2, p = nTP/K2) cat(&quot;Exact 95% CI on Sensitivity = &quot;, as.numeric(ret$conf.int),&quot;\\n&quot;) #&gt; Exact 95% CI on Sensitivity = 0.951 1 The lines upto cat(\"Specificity = \", Sp, \"Sensitivity = \", Se, \"\\\\n\") are almost identical to those in the previous code chunk. Lines 14-17 calculates the approximate 95% CI for FPF. Note the usage of the absolute value of the qnorm() function; qnorm is the lower quantile function for the unit normal distribution, identical to \\(\\Phi^{-1}\\), and \\(z_{\\alpha/2}\\) is the upper quantile function. Line 19 – 21 calculates and prints the corresponding exact confidence interval, using the function binom.test(); one should look up the documentation on this function for further details (in the Help panel – lower right window - start typing in the function name and RStudio should complete it) and examine the structure of the returned variable ret. The remaining code repeats these calculations for TPF. The approximate confidence intervals can exceed the allowed ranges, but the exact confidence intervals do not. 5.11 Summary 5.12 Discussion 5.13 References "]
]
