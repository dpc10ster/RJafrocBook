[
["index.html", "The RJafroc Book Preface", " The RJafroc Book Dev P. Chakraborty, PhD 2020-04-08 Preface This book, an extended documentation of the RJafroc package, is currently (as of April 2020) in preperation. It is intended to bypass the file size limits of CRAN, which severely limits the extent of the documentation that can be included with the CRAN package. "],
["a-note-on-the-online-distribution-mechanism-of-the-book.html", "A note on the online distribution mechanism of the book", " A note on the online distribution mechanism of the book In the hard-copy version of my book (Chakraborty 2017) the online distribution mechanisms was BitBucket. BitBucket allows code sharing within a closed group of a few users (e.g., myself and a student). Since the purpose of open-source code is to encourage collaborations, this was, in hindsight, an unfortunate choice. Moreover, as my experience with R-packages grew, it became apparent to me that the vast majority of R-packages are shared on GitHub, not BitBucket. For these reasons I have switched to GitHub. Any previous instructions pertaining to BitBucket are obsolete. In order to access GitHub material one needs to create a (free) account. Go to this link and click on Sign Up. REFERENCES "],
["contributing-to-this-book.html", "Contributing to this book", " Contributing to this book I would greatly appreciate any feedback on this document. I welcome corrections and comments. The simplest way to do this is to raise an Issue on the GitHub interface. Click on Issues tab under dpc10ster/RJafrocBook, then click on New issue. Contributions from users then automatically become part of the GitHub documentation/history of the book. "],
["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction This is the book desribing the RJafroc package. The name of the book is The RJafroc Book. Modality and treatment are used interchangeably. Reader is a generic radiologist, or a computer aided detection algorithm, or any algorithmic “reader” TBA "],
["NpvPpv.html", "Chapter 2 Negative and Positive Predictive Values 2.1 Introduction 2.2 Relevant equations 2.3 Example calculation of PPV, NPV and accuracy 2.4 Comments 2.5 PPV and NPV are irrelevant to laboratory tasks", " Chapter 2 Negative and Positive Predictive Values 2.1 Introduction Sensitivity and specificity have desirable characteristics, insofar as they reward the observer for correct decisions on actually diseased and actually non-diseased cases, respectively, so these quantities are expected to be independent of disease prevalence. Stated simply, one is dividing by the relevant denominator, so increased numbers of non-diseased cases are balanced by a corresponding increased number of correct decisions on non-diseased cases, and likewise for diseased cases. However, radiologists interpret cases in a “mixed” situation where cases could be positive or negative for disease and disease prevalence plays a crucial role in their decision-making – this point will be clarified shortly. Therefore, a measure of performance that is desirable from the researcher’s point of view is not necessarily desirable from the radiologist’s point of view. It should be obvious that if most cases are non-diseased, i.e., disease prevalence is close to zero, specificity, being correct on non-diseased cases, is more important to the radiologist. Otherwise, the radiologist would figuratively be crying “wolf” most of the time. The radiologist who makes too many FPs would discover it from subsequent clinical audits or daily case conferences, which are held in most large imaging departments. There is a cost to unnecessary false positives – the cost of additional imaging and / or needle-biopsy to rule out cancer, not to mention the pain and emotional trauma inflicted on the patient. Conversely, if disease prevalence is high, then sensitivity, being correct on diseased cases, is more important to the radiologist. With intermediate disease prevalence a weighted average of sensitivity and specificity, where the weighting involves disease prevalence, is desirable from the radiologist’s point of view. The radiologist is less interested in the normalized probability of a correct decision on non-diseased cases. Rather interest is in the probability that a patient diagnosed as non-diseased is actually non-diseased. The reader should notice how the two probability definitions are “turned around” - more on this below. Likewise, the radiologist is less interested in the normalized probability of correct decisions on diseased cases; rather interest is in the probability that a patient diagnosed as diseased is actually diseased. These are termed negative and positive predictive values, respectively, and denoted NPV and PPV 2.2 Relevant equations These are from Chapter 2 of my book. PPV = Positive Predictive Value NPV = Negative Predictive Value Acc = Accuracy \\(P(D)\\) is the disease prevalence and \\(P(!D)\\) is the complement, i.e., \\(P(!D) = 1 - P(D)\\). \\[\\begin{equation*} NPV =\\frac{P(!D)(1-FPF)}{P(!D)(1-FPF)+P(D)(1-TPF)} \\end{equation*}\\] \\[\\begin{equation*} PPV =\\frac{P(D)(TPF)}{P(D)(TPF)+P(!D)FPF} \\end{equation*}\\] \\[\\begin{equation*} Acc =P(!D)(1-FPF)+P(D)(TPF) \\end{equation*}\\] 2.3 Example calculation of PPV, NPV and accuracy Typical disease prevalence in the US in screening mammography is 0.005. A typical operating point, for an expert mammographer, is FPF = 0.1, TPF = 0.8. What are NPV and PPV? # disease prevalence in # USA screening mammography prevalence &lt;- 0.005 FPF &lt;- 0.1 # typical operating point TPF &lt;- 0.8 # do: specificity &lt;- 1-FPF sensitivity &lt;- TPF NPV &lt;- (1-prevalence)*(specificity)/ ((1-prevalence)*(specificity) + prevalence*(1-sensitivity)) PPV &lt;- prevalence*sensitivity/ (prevalence*sensitivity + (1-prevalence)*(1-specificity)) cat(&quot;NPV = &quot;, NPV, &quot;\\nPPV = &quot;, PPV, &quot;\\n&quot;) #&gt; NPV = 0.9988846 #&gt; PPV = 0.03864734 accuracy &lt;-(1-prevalence)* (specificity)+(prevalence)*(sensitivity) cat(&quot;accuracy = &quot;, accuracy, &quot;\\n&quot;) #&gt; accuracy = 0.8995 Line 3 initializes the variable prevalence, the disease prevalence. In other words, prevalence &lt;- 0.005 causes the value 0.005 to be assigned to the variable prevalence. Do not use prevalence = 0.005 as an assignment statement: it may work some of the time, but can cause problems when one least expects it. In R one does not need to worry about the type of variable - integer, float, double, or declaring variables before using them; this can lead to “sloppy” programming constructs but for the most part R behaves reasonably. Line 4 assigns 0.1 to FPF and line 5 assigns 0.8 to TPF. Lines 6 and 7 initialize the variables specificity and sensitivity, respectively. Line 8 calculates NPV, using Eqn. (2.27) (all equations refer to my book) and line 9 calculates PPV, using Eqn. (2.28). Line 10 prints the values of NPV and PPV, with a helpful message. The cat() function stands for concatenate and print the comma-separated components of the argument. The cat() function starts by printing the string variable “NPV =”, then it encounters a comma, then the variable name NPV, so it prints the value of the variable. Then it encounters another comma, and the string “PPV =”, which it prints. Then it encounters another comma and the variable name PPV, so it prints the value of this variable. Finally, it encounters the last comma, and the string “\\n”, which stand for a newline character, which positions any subsequent output to the next line; without it any subsequent print statements would appear on the same line, which is usually not the intent. Line 11 calculates accuracy, Eqn. (2.17) and the next line prints it. 2.4 Comments If a woman has a negative diagnosis, chances are very small that she has breast cancer: the probability that the radiologist is incorrect in the negative diagnosis is 1 - NPV = 0.0011154. Even is she has a positive diagnosis, the probability that she actually has cancer is still only 0.0386473. That is why following a positive screening diagnosis the woman is recalled for further imaging, and if that reveals cause for reasonable suspicion, then additional imaging is performed, perhaps augmented with a needle-biopsy to confirm actual disease status. If the biopsy turns out positive, only then is the woman referred for cancer therapy. Overall, accuracy is 0.8995. The numbers in this illustration are for expert radiologists. In practice there is wide variability in radiologist performance. 2.5 PPV and NPV are irrelevant to laboratory tasks According to the hierarchy of assessment methods described in Chapter 01, Table 1.1, PPV and NPV are level- 3 measurements, which are calculated from “live” interpretations. In the clinic, the radiologist adjusts the operating point to achieve a balance between sensitivity and specificity. The balance depends critically on the known disease prevalence. Based on geographical location and type of practice, the radiologist over time develops an idea of actual disease prevalence, or it can be found in various databases. For example, a breast-imaging clinic that specializes in imaging high-risk women will have higher disease prevalence than the general population and the radiologist is expected to err more on the side of reduced specificity because of the expected benefit of increased sensitivity. However, in the context of a laboratory study, where one uses enriched case sets, the concepts of NPV and PPV are meaningless. For example, it would be rather difficult to perform a laboratory study with 10,000 randomly sampled women, which would ensure about 50 actually diseased patients, which is large enough to get a reasonably precise estimate of sensitivity (estimating specificity is inherently more precise because most women are actually non-diseased). Rather, in a laboratory study one uses enriched data sets where the numbers of diseased-cases is much larger than in the general population, Eqn. (2.13). The radiologist cannot interpret these cases pretending that the actual prevalence is very low. Negative and positive predictive values, while they can be calculated from laboratory data, have very little, if any, clinical meanings, since they have no effect on radiologist thinking. As noted in Chapter 01 the whole purpose of level-3 measurements is to determine the effect on radiologist thinking. There are no diagnostic decisions riding on laboratory ROC interpretations of retrospectively acquired patient images. However, PPV and NPV do have clinical meanings when calculated from very large population based “live” studies. For example, the 2011 Fenton et al study sampled 684,956 women and used the results of “live” interpretations of their images. In contrast, laboratory ROC studies are typically conducted with 50-100 non-diseased and 50-100 diseased cases. A study using about 300 cases total would be considered a “large” ROC study. "],
["modelingBinaryTask.html", "Chapter 3 Modeling the Binary Task 3.1 Introduction 3.2 The equal-variance binormal model 3.3 Definitions and relevant formulae 3.4 The normal distribution pdf and cdf plots 3.5 Binary ratings 3.6 Calculating confidence intervals for sensitivity and specificity 3.7 References", " Chapter 3 Modeling the Binary Task 3.1 Introduction Chapter 02 introduced measures of performance associated with the binary decision task. Described in this chapter is a 2-parameter statistical model for the binary task, in other words it shows how one can predict quantities like sensitivity and specificity based on the values of the parameters of a statistical model. It introduces the fundamental concepts of a decision variable and a decision threshold (the latter is one of the parameters of the statistical model) that pervade this book, and shows how the decision threshold can be altered by varying experimental conditions. The receiver-operating characteristic (ROC) plot is introduced which shows how the dependence of sensitivity and specificity on the decision threshold is exploited by a measure of performance that is independent of decision threshold, namely the area AUC under the ROC curve. AUC turns out to be related to the other parameter of the model. The dependence of variability of the operating point on the numbers of cases is explored, introducing the concept of random sampling and how the results become more stable with larger numbers of cases, or larger sample sizes. These are perhaps intuitively obvious concepts but it is important to see them demonstrated. Formulae for 95% confidence intervals for estimates of sensitivity and specificity are derived and the calculations are shown explicitly. 3.2 The equal-variance binormal model \\(N(\\mu,\\sigma^2)\\) is the normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). The Z-samples for non-diseased cases are distributed \\(N(0,1)\\). The Z-samples for diseased cases are distributed \\(N(\\mu,1)\\). A case is diagnosed as diseased if its Z-sample ≥ a constant threshold \\(\\zeta\\), and non-diseased otherwise. 3.3 Definitions and relevant formulae pdf = probability density function, denoted \\(\\phi\\). cdf = cumulative distribution function, denoted \\(\\Phi\\). \\[\\begin{equation*} \\phi\\left ( z|\\mu,\\sigma \\right )=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left ( -\\frac{(z-\\mu)^2}{2\\sigma^2} \\right ) \\end{equation*}\\] \\[\\begin{equation*} \\phi\\left ( z \\right )=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left ( -\\frac{z^2}{2} \\right ) \\end{equation*}\\] \\[\\begin{equation*} \\Phi\\left ( z \\right )=\\int_{-\\infty }^{z}\\phi(t)dt \\end{equation*}\\] 3.4 The normal distribution pdf and cdf plots x &lt;- seq(-3,3,0.01) pdfData &lt;- data.frame(z = x, pdfcdf = dnorm(x)) # plot the CDF cdfData &lt;- data.frame(z = x, pdfcdf = pnorm(x)) pdfcdfPlot &lt;- ggplot(mapping = aes(x = z, y = pdfcdf)) + geom_line(data = pdfData) + geom_line(data = cdfData) + geom_vline(xintercept = 1, linetype = 2) + xlab(label = &quot;z&quot;) + ylab(label = &quot;pdf/CDF&quot;) print(pdfcdfPlot) The sigmoid shaped curve is the CDF, or cumulative distribution function, of the N(0,1) distribution, while the bell-shaped curve is the corresponding pdf, or probability density function. The dashed line corresponds to the reporting threshold \\(\\zeta\\). The area under the pdf to the left of \\(\\zeta\\) equals the value of CDF at the selected \\(\\zeta\\), i.e., 0.841 (pnorm(1) = 0.841). 3.5 Binary ratings # Line 1 # ... # ... seed &lt;- 100;set.seed(seed) K1 &lt;- 9;K2 &lt;- 11;mu &lt;- 1.5;zeta &lt;- mu/2 z1 &lt;- rnorm(K1) z2 &lt;- rnorm(K2) + mu nTN &lt;- length(z1[z1 &lt; zeta]) nTP &lt;- length(z2[z2 &gt;= zeta]) Sp &lt;- nTN/K1;Se &lt;- nTP/K2 cat(&quot;seed = &quot;, seed, &quot;, K1 = &quot;, K1, &quot;, K2 = &quot;, K2, &quot;Specificity = &quot;, Sp, &quot;, Sensitivity = &quot;, Se, &quot;\\n&quot;) #&gt; seed = 100 , K1 = 9 , K2 = 11 Specificity = 0.8888889 , Sensitivity = 0.9090909 Line 4 sets the seed of the random number generator to 100: this causes the random number generator to yield the same sequence of “random” numbers every time it is run. This is useful during initial code development and for showing the various steps of the example (if seed &lt;- NULL the random numbers would be different every time, making it harder for me, from a pedagogical point of view, to illustrate the steps). Line 5 initializes variables K1 and K2, which represent the number of non-diseased cases and the number of diseased cases, respectively. In this example 9 non-diseased and 11 diseased cases are simulated. Line 5 also initializes the parameter mu &lt;- 1.5 (mu corresponds to the separation parameter of the simulation model). Finally, this line initializes zeta, which corresponds to the threshold for declaring cases as diseased, to mu/2, i.e., halfway between the means of the two distributions defining the binormal model. Later one can experiment with other values. [Note that multiple statements can be put on a single line as long as semi-colons separate them. The author prefers the “vertical length” of the program to be short, a personal preference that gives the author a better perspective of the code.] Line 6 calls the built-in function rnorm() – for random sample(s) from a normal distribution - with argument K1, which yields K1 (9 in our example) samples from a unit normal distribution N(0,1). Arguments to a function are always comma separated and contained within enclosing parentheses. The samples are assigned to the variable z1 (for z-samples for non-diseased cases). The corresponding samples for the diseased cases, line 7, denoted z2, were obtained using rnorm(K2) + mu. [Alternatively one could have used rnorm(K2, mean = mu), which cause the value mu to override the default value - zero - of the mean of the normal distribution.] Since mu was initialized to 1.5, this line yields 11 samples from a normal distribution with mean zero and unit variance and adds 1.5 to all samples (if one wishes to sample from a distribution with a different variance, for example “3”, one needs to also insert the standard deviation argument, e.g., sd = sqrt(3), in the call to rnorm()). The modifications to the default values can be inserted, separated by commas, in any order, but the names mean and sd must match; try typing rnorm(K1, mean1 = 0) in the console window, one should see an error message. 3.6 Calculating confidence intervals for sensitivity and specificity options(digits=3) seed &lt;- 100;set.seed(seed) alpha &lt;- 0.05;K1 &lt;- 99;K2 &lt;- 111;mu &lt;- 5;zeta &lt;- mu/2 cat(&quot;alpha = &quot;, alpha, &quot;K1 = &quot;, K1, &quot;K2 = &quot;, K2, &quot;mu = &quot;, mu, &quot;zeta = &quot;, zeta, &quot;\\n&quot;) #&gt; alpha = 0.05 K1 = 99 K2 = 111 mu = 5 zeta = 2.5 z1 &lt;- rnorm(K1) z2 &lt;- rnorm(K2) + mu nTN &lt;- length(z1[z1 &lt; zeta]) nTP &lt;- length(z2[z2 &gt;= zeta]) Sp &lt;- nTN/K1;Se &lt;- nTP/K2 cat(&quot;Specificity = &quot;, Sp, &quot;Sensitivity = &quot;, Se, &quot;\\n&quot;) #&gt; Specificity = 0.99 Sensitivity = 0.991 # Approx binomial tests cat(&quot;approx 95% CI on Specificity = &quot;, -abs(qnorm(alpha/2))*sqrt(Sp*(1-Sp)/K1)+Sp, +abs(qnorm(alpha/2))*sqrt(Sp*(1-Sp)/K1)+Sp,&quot;\\n&quot;) #&gt; approx 95% CI on Specificity = 0.97 1.01 # Exact binomial test ret &lt;- binom.test(nTN, K1, p = nTN/K1) cat(&quot;Exact 95% CI on Specificity = &quot;, as.numeric(ret$conf.int),&quot;\\n&quot;) #&gt; Exact 95% CI on Specificity = 0.945 1 # Approx binomial tests cat(&quot;approx 95% CI on Sensitivity = &quot;, -abs(qnorm(alpha/2))*sqrt(Se*(1-Se)/K2)+Se, +abs(qnorm(alpha/2))*sqrt(Se*(1-Se)/K2)+Se,&quot;\\n&quot;) #&gt; approx 95% CI on Sensitivity = 0.973 1.01 # Exact binomial test ret &lt;- binom.test(nTP, K2, p = nTP/K2) cat(&quot;Exact 95% CI on Sensitivity = &quot;, as.numeric(ret$conf.int),&quot;\\n&quot;) #&gt; Exact 95% CI on Sensitivity = 0.951 1 The lines upto cat(\"Specificity = \", Sp, \"Sensitivity = \", Se, \"\\\\n\") are almost identical to those in the previous code chunk. Lines 14-17 calculates the approximate 95% CI for FPF. Note the usage of the absolute value of the qnorm() function; qnorm is the lower quantile function for the unit normal distribution, identical to \\(\\Phi^{-1}\\), and \\(z_{\\alpha/2}\\) is the upper quantile function. Line 19 – 21 calculates and prints the corresponding exact confidence interval, using the function binom.test(); one should look up the documentation on this function for further details (in the Help panel – lower right window - start typing in the function name and RStudio should complete it) and examine the structure of the returned variable ret. The remaining code repeats these calculations for TPF. The approximate confidence intervals can exceed the allowed ranges, but the exact confidence intervals do not. 3.7 References "],
["ratingsParadigm.html", "Chapter 4 Ratings Paradigm 4.1 Introduction 4.2 The ROC counts table 4.3 Operating points from counts table 4.4 Automating all this 4.5 Relation between ratings paradigm and the binary paradigm 4.6 Ratings are not numerical values 4.7 A single “clinical” operating point from ratings data 4.8 Observer performance studies as laboratory simulations of clinical tasks 4.9 Discrete vs. continuous ratings: the Miller study 4.10 References", " Chapter 4 Ratings Paradigm 4.1 Introduction In Chapter 02 the binary task and associated concepts of sensitivity, specificity, true positive fraction, false positive fraction, positive and negative predictive values were introduced. Chapter 03 introduced the concepts of a random scalar decision variable, or z-sample for each case, which is compared, by the observer, to a fixed reporting threshold \\(\\zeta\\), resulting in two types of decisions, “case is non-diseased” or “case is diseased” depending on whether the realized z-sample is less than, or greater than or equal to the reporting threshold. It described a statistical model, for the binary task, characterized by two unit-variance normal distributions separated by \\(\\mu\\). The concept of an underlying receiver operating characteristic (ROC) curve with the reporting threshold defining an operating point on the curve was introduced and the advisability of using the area under the curve as a measure of performance, which is independent of reporting threshold, was stressed. In this chapter the more commonly used ratings method will be described, which yields greater definition to the underlying ROC curve than just one operating point obtained in the binary task, and moreover, is more efficient. In this method, the observer assigns a rating to each case. Described first is a typical ROC counts table and how operating points (i.e., pairs of FPF and TPF values) are calculated from the counts data. A labeling convention for the operating points is introduced. Notation is introduced for the observed integers in the counts table and the rules for calculating operating points are expressed as formulae and implemented in R. The ratings method is contrasted to the binary method, in terms of efficiency and practicality. A theme occurring repeatedly in this book, that the ratings are not numerical values but rather they are ordered labels is illustrated with an example. 4.2 The ROC counts table In a positive-directed rating scale with five discrete levels, the ratings could be the ordered labels “1”: definitely non-diseased, “2”: probably non-diseased, “3”: could be non-diseased or diseased, “4”: probably diseased, “5”: definitely diseased. At the conclusion of the ROC study an ROC counts table is constructed. This is the generalization to rating studies of the 2 x 2 decision vs. truth table introduced in Chapter 02, Table 2.1. This type of data representation is sometimes called a frequency table, but frequency means a rate of number of events per some unit, so the author prefers the clearer term “counts”. The Table (below) is a representative counts table for a 5-rating study that summarizes the collected data. It is the starting point for analysis. It lists the number of counts in each ratings bin, listed separately for non-diseased and diseased cases, respectively. The data is from an actual clinical study. 1 2 3 4 5 non-diseased 30 19 8 2 1 diseased 5 6 5 12 22 In this example, there are \\(K_1 = 60\\) non-diseased cases and \\(K_2 = 50\\) diseased cases. Of the 60 non-diseased cases 30 were assigned the “1” rating, 19 were assigned the “2” rating, eight the “3” rating, two the “4” rating and one received the “5” rating. The distribution of counts is tilted towards the “1” rating end, but there is some spread and one actually non-diseased case appeared definitely diseased to the observer. In contrast, the distribution of the diseased cases is tilted towards the “5” rating end. Of the 50 diseased cases, 22 received the “5” rating, 12 the “4” rating, five the “3” rating, six the “2” rating and five the “1” rating. The spread appears to be more pronounced for the diseased cases, e.g., five of the 50 cases appeared to be definitely non-diseased to the observer. A little thought should convince you that the observed tilting of the counts, towards the “1” end for actually non-diseased cases, and towards the “5” end for actually diseased cases, is reasonable. However, one should be forewarned not to jump to conclusions about the spread of the data being larger for diseased than for non-diseased cases. While it turns out to be true, the ratings are merely ordered labels, and modeling is required, to be described in Chapter 06, that uses only the ordering information implicit in the labels, not the actual values, to reach quantitative conclusions. 4.3 Operating points from counts table RtngGE5 means “rating greater than or equal to 5”, etc. RtngGE5 RtngGE4 RtngGE3 RtngGE2 RtngGE1 FPF 0.017 0.05 0.183 0.5 1 TPF 0.440 0.68 0.780 0.9 1 It is critical to understand the following example. The table illustrates how ROC operating points are calculated from the cell counts. One starts with non-diseased cases that were rated five or more (in this example, since 5 is the highest allowed rating, the “or more” clause is superfluous) and divides by the total number of non-diseased cases, \\(K_1 = 60\\). This yields the abscissa of the lowest non-trivial operating point, namely \\(FPF_{\\ge5}\\) = 1/60 = 0.017. The subscript on FPF is intended to make explicit which ratings are being cumulated. The corresponding ordinate is obtained by dividing the number of diseased cases rated “5” or more and dividing by the total number of diseased cases, \\(K_2 = 50\\), yielding \\(TPF_{\\ge5}\\) = 22/50 = 0.440. Therefore, the coordinates of the lowest operating point are (0.017, 0.44). The abscissa of the next higher operating point is obtained by dividing the number of non-diseased cases that were rated “4” or more and dividing by the total number of non-diseased cases, i.e., \\(TPF_{\\ge4}\\) = 3/60 = 0.05. Similarly the ordinate of this operating point is obtained by dividing the number of diseased cases that were rated “4” or more and dividing by the total number of diseased cases, i.e., \\(FPF_{\\ge4}\\) = 34/50 = 0.680. The procedure, which at each stage cumulates the number of cases equal to or greater (in the sense of increased confidence level for disease presence) than a specified label, is repeated to yield the rest of the operating points listed in Table 4.1. Since they are computed directly from the data, without any assumption, they are called empirical or observed operating points. After done this once it would be nice to have a formula implementing the process, one use of which would be to code the procedure. First, one needs appropriate notation for the bin counts. Let \\(K_{1r}\\) denote the number of non-diseased cases rated r, and \\(K_{2r}\\) denote the number of diseased cases rated r. For convenience, define dummy counts \\(K_{1{(R+1)}}\\) = \\(K_{2{(R+1)}}\\) = 0, where R is the number of ROC bins. This construct allows inclusion of the origin (0,0) in the formulae. The range of r is \\(r = 1,2,...,(R+1)\\). Within each truth-state, the individual bin counts sum to the total number of non-diseased and diseased cases, respectively. The following equations summarize all this: \\[\\begin{equation*} K_1=\\sum_{r=1}^{R+1}K_{1r} \\end{equation*}\\] \\[\\begin{equation*} K_2=\\sum_{r=1}^{R+1}K_{2r} \\end{equation*}\\] \\[\\begin{equation*} K_{1{(R+1)}} = K_{2{(R+1)}} = 0 \\end{equation*}\\] \\[\\begin{equation*} r = 1,2,...,(R+1) \\end{equation*}\\] The operating points are defined by: \\[\\begin{equation*} FPF_r=\\frac {1} {K_1} \\sum_{s=r}^{R+1}K_{1s} \\end{equation*}\\] \\[\\begin{equation*} TPF_r=\\frac {1} {K_2} \\sum_{s=r}^{R+1}K_{2s} \\end{equation*}\\] The labeling of the points follows the following convention: \\(r=1\\) corresponds to the upper right corner (1,1) of the ROC plot, a trivial operating point since it is common to all datasets. Next, \\(r=2\\) is the next lower operating point, etc., and \\(r=R\\) is the lowest non-trivial operating point and finally \\(r=R+1\\) is the origin (0,0) of the ROC plot, which is also a trivial operating point, because it is common to all datasets. In other words, the operating points are numbered starting with the upper right corner, labeled 1, and working down the curve, each time increasing the label by one. Since one is cumulating counts, which can never be negative, the highest non-trivial operating point resulting from cumulating the 2 through 5 ratings has to be to the upper-right of the next adjacent operating point resulting from cumulating the 3 through 5 ratings. This in turn has to be to the upper-right of the operating point resulting from cumulating the 4 through 5 ratings. This in turn has to be to the upper right of the operating point resulting from the 5 ratings. In other words, as one cumulates ratings bins, the operating point must move monotonically up and to the right, or more accurately, the point cannot move down or to the left. If a particular bin has zero counts for non-diseased cases, and non-zero counts for diseased cases, the operating point moves vertically up when this bin is cumulated; if it has zero counts for diseased cases, and non-zero counts for non-diseased cases, the operating point moves horizontally to the right when this bin is cumulated. 4.4 Automating all this It is useful to replace the preceding detailed explanation with a simple algorithm that incorporates all the logic. This is done in the following code: options(digits = 3) FPF &lt;- array(0,dim = R) TPF &lt;- array(0,dim = R) for (r in (R+1):2) { FPF[(R+2)-r] &lt;- sum(Ktr[1, r:(R+1)])/sum(Ktr[1,]) TPF[(R+2)-r] &lt;- sum(Ktr[2, r:(R+1)])/sum(Ktr[2,]) } cat(&quot;FPF =&quot;, &quot;\\n&quot;) #&gt; FPF = cat(FPF, &quot;\\n&quot;) #&gt; 0.0167 0.05 0.183 0.5 cat(&quot;TPF = &quot;, &quot;\\n&quot;) #&gt; TPF = cat(TPF, &quot;\\n&quot;) #&gt; 0.44 0.68 0.78 0.9 mu &lt;- qnorm(.5)+qnorm(.9);sigma &lt;- 1 Az &lt;- pnorm(mu/sqrt(2)) cat(&quot;uppermost point based estimate of mu = &quot;, mu, &quot;\\n&quot;) #&gt; uppermost point based estimate of mu = 1.28 cat(&quot;corresponding estimate of Az = &quot;, Az, &quot;\\n&quot;) #&gt; corresponding estimate of Az = 0.818 cat(&quot;showing observed operating points and equal variance model fitted ROC curve&quot;, &quot;\\n&quot;) #&gt; showing observed operating points and equal variance model fitted ROC curve plotROC (mu, sigma, FPF, TPF) Notice that the values of the arrays FPF and TPF are identical to those listed in Table 4.1. It was shown in Chapter 03 that in the equal variance binormal model, an operating point determines the parameters \\(\\mu\\) = 1.282, Eqn. (3.21), or equivalently \\(A_{z;\\sigma = 1}\\) = 0.818, Eqn. (3.30). The last three lines of the preceding code chunk illustrate the application of these formulae using the coordinates (0.5, 0.9) of the uppermost non-trivial operating point, followed by a plot of the ROC curve and the operating points. It should come as no surprise that the uppermost operating point is exactly on the predicted curve: after all, this point was used to calculate \\(\\mu\\) = 1.282. The corresponding value of \\(\\zeta\\) can be calculated from Eqn. (3.17), namely: \\[\\begin{equation*} \\Phi^{-1}\\left ( Sp \\right )=\\zeta \\end{equation*}\\] \\[\\begin{equation*} \\zeta=\\mu - \\Phi^{-1}\\left ( Se \\right ) \\end{equation*}\\] These are coded below: qnorm(1-0.5) #&gt; [1] 0 mu-qnorm(0.9) #&gt; [1] 0 Either way, one gets the same result: \\(\\zeta\\) = 0. It should be clear that this makes sense: FPF = 0.5 is consistent with half of the (symmetrical) unit-normal non-diseased distribution being above \\(\\zeta\\) = 0. The transformed value \\(\\zeta\\) (zero in this example) is a genuine numerical value. To reiterate, ratings cannot be treated as genuine numerical values, but thresholds, estimated from an appropriate model, can be treated as genuine numerical values. Exercise: calculate \\(\\zeta\\) for each of the remaining operating points. Notice that \\(\\zeta\\) increases as one moves down the curve. mu &lt;- 2.17;sigma &lt;- 1.65 Az &lt;- pnorm(mu/sqrt(1+sigma^2)) plotROC (mu, sigma, FPF, TPF) cat(&quot;binormal unequal variance model estimate of Az = &quot;, Az, &quot;\\n&quot;) #&gt; binormal unequal variance model estimate of Az = 0.87 cat(&quot;showing observed operating points and unequal variance model fitted ROC curve&quot;, &quot;\\n&quot;) #&gt; showing observed operating points and unequal variance model fitted ROC curve The ROC curve in Fig. 4.1 (A), as determined by the uppermost operating point, passes exactly through this point but misses the others. If a different operating point were used to estimate \\(\\mu\\) = and \\(A_{z;\\sigma = 1}\\), the estimated values would have been different and the new curve would pass exactly through the new selected point. No single-point based choice of \\(\\mu\\) would yield a satisfactory visual fit to all the observed operating points. [The reader should confirm these statements with appropriate modifications to the code.] * This is the reason one needs a modified model, with an extra parameter, namely the unequal variance binormal model, to fit radiologist data (the extra parameter is the ratio of the standard deviations of the two distributions). Fig. 4.1 (B) shows the predicted ROC curve by the unequal variance binormal model, to be introduced in Chapter 06. The corresponding parameter values are \\(\\mu\\) = 2.17and \\(\\sigma\\) = 1.65. Notice the improved visual quality of the fit. Each observed point is “not engraved in stone”, rather both FPF and TPF are subject to sampling variability. Estimation of confidence intervals for FPF and TPF was addressed in §3.10. [A detail: the estimated confidence interval in the preceding chapter was for a single operating point; since the multiple operating points are correlated – some of the counts used to calculate them are common to two or more operating points – the method tends to overestimate the confidence interval. A modeling approach is possible to estimate confidence intervals that accounts for data correlation and this yields tighter confidence intervals.] 4.5 Relation between ratings paradigm and the binary paradigm In Chapter 02 it was shown that the binary task requires a single fixed threshold parameter \\(\\zeta\\) and a decision rule, namely, to give the case a diseased rating of 2 if \\(Z \\ge \\zeta\\) and a rating of 1 otherwise. The R-rating task can be viewed as \\((R-1)\\) simultaneously conducted binary tasks each with its own fixed threshold \\(\\zeta_r, r = 1, 2, ..., R-1\\). It is efficient compared to \\((R-1)\\) sequentially conducted binary tasks; however, the onus is on the observer to maintain fixed-multiple thresholds through the duration of the study. The rating method is a more efficient way of collecting the data compared to running the study repeatedly with appropriate instructions to cause the observer to adopt different fixed thresholds specific to each replication. In the clinical context such repeated studies would be impractical because it would introduce memory effects, wherein the diagnosis of a case would depend on how many times the case had been seen, along with other cases, in previous sessions. A second reason is that it is difficult for a radiologist to change the operating threshold in response to instructions. To the author’s knowledge, repeated use of the binary paradigm has not been used in any clinical ROC study. How does one model the binning? For convenience one defines dummy thresholds \\(\\zeta_0 = - \\infty\\) and \\(\\zeta_R = + \\infty\\), in which case the thresholds satisfy the ordering requirement \\(\\zeta_{r-1} \\le \\zeta_r\\) , r = 1, 2, …, R. The rating or binning rule is: \\[\\begin{equation*} if \\left (\\zeta_{r-1} \\le z \\le \\zeta_r \\right )\\Rightarrow \\text rating = r \\end{equation*}\\] 4.6 Ratings are not numerical values The ratings are to be thought of as ordered labels, not as numeric values. Arithmetic operations that are allowed on numeric values, such as averaging, are not allowed on ratings. One could have relabeled the ratings in Table 4.2 as A, B, C, D and E, where A &lt; B etc. As long as the counts in the body of the table are unaltered, such relabeling would have no effect on the observed operating points and the fitted curve. Of course one cannot average the labels A, B, etc. of different cases. The issue with numeric labels is not fundamentally different. At the root is that the difference in thresholds corresponding to the different operating points are not in relation to the difference between their numeric values. There is a way to estimate the underlying thresholds, if one assumes a specific model, for example the unequal-variance binormal model to be described in Chapter 06. The thresholds so obtained are genuine numeric values and can be averaged. [Not to hold the reader in suspense, the four thresholds corresponding to the data in Table 4.1 are 0.007676989, 0.8962713, 1.515645 and 2.396711; see §6.4.1; these values would be unchanged if, for example, the labels were doubled, with allowed values 2, 4, 6, 8 and 10, or any of an infinite number of rearrangements that preserves their ordering.] The temptation to regard confidence levels / ratings as numeric values can be particularly strong when one uses a large number of bins to collect the data. One could use of quasi-continuous ratings scale, implemented for example, by having a slider-bar user interface for selecting the rating. The slider bar typically extends from 0 to 100, and the rating could be recorded as a floating-point number, e.g., 63.45. Here too one cannot assume that the difference between a zero-rated case and a 10 rated case is a tenth of the difference between a zero-rated case and a 100 rated case. So averaging the ratings is not allowed. Additionally, one cannot assume that different observers use the labels in the same way. One observer’s 4-rating is not equivalent to another observers 4-rating. Working directly with the ratings is a bad idea: valid analytical methods use the rankings of the ratings, not their actual values. The reason for the emphasis is that there are serious misconceptions about ratings. The author is aware of a publication stating, to the effect, that a modality resulted in an increase in average confidence level for diseased cases. Another publication used a specific numerical value of a rating to calculate the operating point for each observer – this assumes all observers use the rating scale in the same way. 4.7 A single “clinical” operating point from ratings data The reason for the quotes in the title to this section is that a single operating point on a laboratory ROC plot, no matter how obtained, has little relevance to how radiologists operate in the clinic. However, some consider it useful to quote an operating point from an ROC study. For a 5-rating ROC study, Table 4.2, it is not possible to unambiguously calculate the operating point of the observer in the binary task of discriminating between non-diseased and diseased cases. One possibility would be to use the three and above ratings to define the operating point, but one might have chosen two and above. A second possibility is to instruct the radiologist that a four or higher rating, for example, implies the case would be reported “clinically” as diseased. However, the radiologist can only pretend so far that this study, which has no clinical consequences, is somehow a “clinical” study. If a single laboratory study based operating point is desired2, the best strategy , in the author’s opinion, is to obtain the rating via two questions. This method is also illustrated in a book on detection theory, Ref. 3, Table 3.1. The first question is “is the case diseased?” The binary (Yes/No) response to this question allows unambiguous calculation of the operating point, as in Chapter 02. The second question is: “what is your confidence in your previous decision?” and allow three responses, namely Low, Medium and High. The dual-question approach is equivalent to a 6-point rating scale, Fig. 4.2. The ordering of the ratings can be understood as follows. The four, five and six ratings are as expected. If the radiologist states the patient is diseased and the confidence level is high that is clearly the highest end of the scale, i.e., six, and the lower confidence levels, five and four, follow, as shown. If, on the other hand, the radiologist states the patient is non-diseased, and the confidence level is high, then that must be the lowest end of the scale, i.e., “1”. The lower confidence levels in a negative decision must be higher than “1”, namely “2” and “3”, as shown. As expected, the low confidence ratings, namely “3” (non-diseased, low confidence) and “4” (diseased, low confidence) are adjacent to each other. With this method of data-collection, there is no confusion as to what rating defines the single desired operating point as this is determined by the binary response to the first question. The 6-point rating scale is also sufficiently fine to not smooth out the ability of the radiologist to maintain distinct different levels. In the author’s experience, using this scale one expects rating noise of about ±½ a rating bin, i.e., the same difficult case, shown on different occasions to the same radiologist (with sufficient time lapse or other intervening cases to minimize memory effects) is expected to elicit a “3” or “4”, with roughly equal probability. 4.8 Observer performance studies as laboratory simulations of clinical tasks Observer performance paradigms (ROC, FROC, LROC and ROI) should be regarded as experiments conducted in a laboratory (i.e., controlled) setting that are intended to be representative of the actual clinical task. They should not to be confused with performance in a real “live” clinical setting: there is a known “laboratory effect”22-24. For example, in one study radiologists performed better during live clinical interpretations than they did later, on the same cases, in a laboratory ROC study22. This is expected because there is more at stake during live interpretations: e.g., the patient’s health and the radiologist’s reputation, than during laboratory ROC studies. The claimed “laboratory effect” has caused some controversy. A paper25 titled “Screening mammography: test set data can reasonably describe actual clinical reporting” argues against the laboratory effect. Real clinical interpretations happen every day in radiology departments all over the world. In the laboratory, the radiologist is asked to interpret the images “as if in a clinical setting” and render a “diagnosis”. The laboratory decisions have no clinical consequences, e.g., the radiologist will not be sued for mistakes and their ROC study decisions have no impact on the clinical management of the patients. Usually laboratory ROC studies are conducted on retrospectively acquired images. Patients, whose images were used in an ROC study, have already been imaged in the clinic and decisions have already been made on how to manage them. There is no guarantee that results of the laboratory study are directly applicable to clinical practice. Indeed there is an assumption that the laboratory study correlates with clinical performance. Strict equality is not required, simply that the performance in the laboratory is related monotonically to actual clinical performance. Monotonicity assures preservation of performance orderings, e.g., a radiologist has greater performance than another does or one modality is superior to another, regardless of how they are measured, in the laboratory or in the clinic. The correlation is taken to be an axiomatic truth by researchers, when in fact it is an assumption. To the extent that the participating radiologist brings his/her full clinical expertise to bear on each laboratory image interpretation, this assumption is likely to be valid. This section provoked a strong negative response from a collaborator. To paraphrase him, \"… my friend, I think it is a pity in this book chapter you argue that these studies are simulations. I mean, the reason people perform these studies is because they believe in the results\". The author also believes in observer performance studies. Otherwise, he would not be writing this book. Distrust of the word “simulation” seems to be peculiar to this field. Simulations are widely used in “hard” sciences, e.g., they are used in astrophysics to determine conditions dating to 10-31 seconds after the big bang. Simulations are not to be taken lightly. Conducting clinical studies is very difficult as there are many factors not under the researcher’s control. Observer performance studies of the type described in this book are the closest that one can come to the “real thing” and the author is a firm believer in them. These studies include key elements of the actual clinical task: the entire imaging system, radiologists (assuming the radiologist take these studies seriously in the sense of bringing their full clinical expertise to bear on each image interpretation) and real clinical images and as such are expected to correlate with real “live” interpretations. Proving this correlation is going to be difficult as there are many factors that complicated real interpretations. It is not clear to the author that proving or disproving this correlation is ever going to be a settled issue. 4.9 Discrete vs. continuous ratings: the Miller study There is controversy about the merits of discrete vs. continuous ratings26-28. Since the late Prof. Charles E. Metz and the late Dr. Robert F. Wagner have both backed the latter (i.e., continuous or quasi-continuous ratings) new ROC study designs sometimes tend to follow their advice. The author’s recommendation is to follow the 6-point rating scale as outlined in Fig. 4.2. This section provides the background for the recommendation. A widely cited (22,909 citations at the time of writing) 1954 paper by Miller29 titled “The Magical Number Seven, Plus or Minus Two: Some Limits on Our Capacity for Processing Information” is relevant. It is a readable paper, freely downloadable in several languages (www.musanim.com/miller1956/). In the author’s judgment, this paper has not received the attention it should have in the ROC community, and for this reason portions from it are reproduced below. [George Armitage Miller, February 3, 1920 – July 22, 2012, was one of the founders of the field of cognitive psychology.] Miller’s first objective was to comment on absolute judgments of unidimensional stimuli. Since all (univariate, i.e., single decision per case) ROC models assume a unidimensional decision variable, Miller’s work is highly relevant. He comments on two papers by Pollack30,31. Pollack asked listeners to identify tones by assigning numerals to them, analogous to a rating task described above. The tones differed in frequency, covering the range 100 to 8000 Hz in equal logarithmic steps. A tone was sounded and the listener responded by giving a numeral (i.e., a rating, with higher values corresponding to higher frequencies). After the listener had made his response, he was told the correct identification of the tone. When only two or three tones were used, the listeners never confused them. With four different tones, confusions were quite rare, but with five or more tones, confusions were frequent. With fourteen different tones, the listeners made many mistakes. Since it is so succinct, the entire content of the first (1952) paper by Pollack is reproduced below: “In contrast to the extremely acute sensitivity of a human listener to discriminate small differences in the frequency or intensity between two sounds is his relative inability to identify (and name) sounds presented individually. When the frequency of a single tone is varied in equal‐logarithmic steps in the range between 100 cps and 8000 cps (and when the level of the tone is randomly adjusted to reduce loudness cues), the amount of information transferred is about 2.3 bits per stimulus presentation. This is equivalent to perfect identification among only 5 tones. The information transferred, under the conditions of measurement employed, is reasonably invariant under wide variations in stimulus conditions.” By “information” is meant (essentially) the number of levels, measured in bits (binary digits), thereby making it independent of the unit of measurement: 1 bit corresponds to a binary rating scale, 2 bits to a four-point rating scale and 2.3 bits to 22.3 = 4.9, i.e., about 5 ratings bins. Based on Pollack’s’ original unpublished data, Miller put an upper limit of 2.5 bits (corresponding to about 6 ratings bins) on the amount of information that is transmitted by listeners who make absolute judgments of auditory pitch. A second paper31 by Pollack was related to: (1) the frequency range of tones; (2) the utilization of objective reference tones presented with the unknown tone; and (3) the “dimensionality”—the number of independently varying stimulus aspects. Little additional gain in information transmission was associated with the first factor; a moderate gain was associated with the second; and a relatively substantial gain was associated with the third (we return to the dimensionality issue below). As an interesting side-note, Miller states: “Most people are surprised that the number is as small as six. Of course, there is evidence that a musically sophisticated person with absolute pitch can identify accurately any one of 50 or 60 different pitches. Fortunately, I do not have time to discuss these remarkable exceptions. I say it is fortunate because I do not know how to explain their superior performance. So I shall stick to the more pedestrian fact that most of us can identify about one out of only five or six pitches before we begin to get confused. It is interesting to consider that psychologists have been using seven-point rating scales for a long time, on the intuitive basis that trying to rate into finer categories does not really add much to the usefulness of the ratings. Pollack’s results indicate that, at least for pitches, this intuition is fairly sound. Next you can ask how reproducible this result is. Does it depend on the spacing of the tones or the various conditions of judgment? Pollack varied these conditions in a number of ways. The range of frequencies can be changed by a factor of about 20 without changing the amount of information transmitted more than a small percentage. Different groupings of the pitches decreased the transmission, but the loss was small. For example, if you can discriminate five high-pitched tones in one series and five low-pitched tones in another series, it is reasonable to expect that you could combine all ten into a single series and still tell them all apart without error. When you try it, however, it does not work. The channel capacity for pitch seems to be about six and that is the best you can do.” In contrast to the careful experiments conducted in the psychophysical context to elucidate this issue, the author was unable to find a single study of the number of discrete rating levels that an observer can support. Even lacking such a study, a recommendation has been made to acquire data on a quasi-continuous scale27. There is no question that for multidimensional data, as observed in the second study by Pollack31, the observer can support more than 7 ratings bins. To quote Miller: “You may have noticed that I have been careful to say that this magical number seven applies to one- dimensional judgments. Everyday experience teaches us that we can identify accurately any one of several hundred faces, any one of several thousand words, any one of several thousand objects, etc. The story certainly would not be complete if we stopped at this point. We must have some understanding of why the one-dimensional variables we judge in the laboratory give results so far out of line with what we do constantly in our behavior outside the laboratory. A possible explanation lies in the number of independently variable attributes of the stimuli that are being judged. Objects, faces, words, and the like differ from one another in many ways, whereas the simple stimuli we have considered thus far differ from one another in only one respect.” In the medical imaging context, a trivial way to increase the number of ratings would be to color-code the images: red, green and blue; now one can assign a red image rated 3, a green image rated 2, etc., which would be meaningless unless the color encoded relevant diagnostic information. Another ability, quoted in the publication27 advocating continuous ratings is the ability to recognize faces, again a multidimensional categorization task, as noted by Miller. Also quoted as an argument for continuous ratings is the ability of computer aided detection schemes that calculate many features for each perceived lesion and combine them into a single probability of malignancy, which is on a highly precise floating point 0 to 1 scale. Radiologists are not computers. Other arguments for greater number of bins: it cannot hurt and one should acquire the rating data at greater precision than the noise, especially if the radiologist is able to maintain the finer distinctions. The author worries that radiologists who are willing to go along with greater precision are over-anxious to co-operate with the experimentalist. In the author’s experience, expert radiologists will not modify their reading style and one should be suspicious when overzealous radiologists accede to an investigators request to interpret images in a style that does not closely resemble the clinic. Radiologists, especially experts, do not like more than about four ratings. The author has worked with a famous chest radiologist (the late Dr. Robert Fraser) who refused to use more than four ratings. Another reason given for using continuous ratings is it reduces instances of data degeneracy. Data is sometimes said to be degenerate if the curve-fitting algorithm, the binormal model and the proper binormal model, cannot fit it. This occurs, for example, if there are no interior points on the ROC plot. Modifying radiologist behavior to accommodate the limitations of analytical methods seems to be inherently dubious. One could simply randomly add or subtract half an integer from the observed ratings, thereby making the rating scale more granular and reduce instances of degeneracy (this is actually done in some ROC software to overcome degeneracy issues). Another possibility is to use the empirical (trapezoidal) area under the ROC curve. This quantity can always be calculated; there are no degeneracy problems with it. Actually, fitting methods now exist that are robust to data degeneracy, such as discussed in Chapter 18 and Chapter 20, so this reason for acquiring continuous data no longer applies. The rating task involves a unidimensional scale and the author sees no way of getting around the basic channel-limitation noted by Miller and for this reason the author recommends a 6 point scale, as in Fig. 4.2. On the other side of the controversy it has been argued that given a large number of allowed ratings levels the observer essentially bins the data into a much smaller number of bins (e.g., 0, 20, 40, 60, 80, 100) and adds a zero-mean noise term to appear to be “spreading out the ratings” 35. This allows easier curve-fitting with the binormal model. However, if the intent is to get the observer to spread the ratings, so that th binormal model does not fail to fit, a better approach is to use alternate models that are very robust with respect to degneracy of the data. More on thsi later (CBM and RSM). 4.10 References "],
["BinormalModel.html", "Chapter 5 Binormal model 5.1 Introduction 5.2 The binormal model 5.3 Least-squares estimation 5.4 Maximum likelihood estimation (MLE) 5.5 Discussion 5.6 References", " Chapter 5 Binormal model 5.1 Introduction In this chapter the univariate binormal model is described, in which one is dealing with one ROC rating per case, as in a single observer interpreting cases, one at a time, in a single modality. By convention the qualifier “univariate” is often omitted, i.e., it is implicit. In a later chapter a bivariate binormal model will be described where each case yields two ratings, as in a single observer interpreting cases in two modalities, or equivalently, two observers interpreting cases in a single modality. The equal variance binormal model was described in Chapter “Binary Paradigm”. The ratings method of acquiring ROC data and calculation of operating points was illustrated in Chapter “Ratings Paradigm”. It was shown that for a clinical ROC dataset the unequal-variance binormal model fitted the data better than the equal-variance binormal model. This chapter deals with details of the unequal-variance binormal model, establishes necessary notation for describing the model, and derives expressions for sensitivity, specificity and the area under the predicted ROC curve (due to its complexity it appears in an Appendix). The main aim of this chapter is to take the mystery out of statistical curve fitting. Accordingly, this is one chapter where the Appendices are longer than the main text, but as usual, they are essential reading as they reinforce the main text. It is not too much to expect the reader to load each file beginning with “main”, click on Source and see what happens. [The reader is reminded that any file that starts with “main” contains directly executable code.] 5.2 The binormal model The unequal-variance binormal model (henceforth abbreviated to binormal model; when the author means equal variances, it will be made explicit) is defined by (capital letters indicate random variables and their lower-case counterparts are actual realized values): \\[\\begin{equation*} Z_{k_tt}\\sim N\\left ( \\mu_t,\\sigma_{t}^{2} \\right );t=1,2 \\end{equation*}\\] where \\[\\begin{equation*} \\mu_1=0;\\mu_2=\\mu;\\sigma_{1}^{2}=1;\\sigma_{2}^{2}=\\sigma^{2} \\end{equation*}\\] Eqn. (6.2.1) states that the Z-samples for non-diseased cases are distributed as a \\(N(0,1)\\) distribution, i.e., the unit normal distribution, while the Z-samples for diseased cases are distributed as a \\(N(\\mu,\\sigma^2)\\) distribution, i.e., a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). This is a 2-parameter model of the z-samples, not counting additional threshold parameters needed for data binning. 5.2.1 Binning the data In an R-rating ROC study the observed ratings r take on integer values, 1 through R, it being understood that higher ratings correspond to greater confidence for disease. Defining dummy cutoffs \\(\\zeta_0 = -\\infty\\) and \\(\\zeta_R = +\\infty\\), the binning rule for a case with realized z-sample z is (Chapter “Ratings Paradigm”, Eqn. 4.13): \\[\\begin{equation*} if \\left (\\zeta_{r-1} \\le z \\le \\zeta_r \\right )\\Rightarrow \\text rating = r \\end{equation*}\\] mu &lt;- 1.5;sigma &lt;- 1.5 z1 &lt;- seq(-3, 4, by = 0.01) z2 &lt;- seq(-3, 6, by = 0.01) Pdf1 &lt;- dnorm(z1) Pdf2 &lt;- dnorm(z2, mu, sd = sigma) df &lt;- data.frame(z = c(z1, z2), pdfs = c(Pdf1, Pdf2), truth = c(rep(&#39;non-diseased&#39;, length(Pdf1)), rep(&#39;diseased&#39;, length(Pdf2))), stringsAsFactors = FALSE) cut_point &lt;- data.frame(z = c(-2.0, -0.5, 1, 2.5)) rocPdfs &lt;- ggplot(df, aes(x = z, y = pdfs, color = truth)) + geom_line(size = 2) + scale_colour_manual(values=c(&quot;darkgrey&quot;,&quot;black&quot;)) + theme(legend.title = element_blank(), legend.position = c(0.85, 0.95), legend.text = element_text(size=25, face = &quot;bold&quot;), axis.title.x = element_text(hjust = 0.8, size = 30,face=&quot;bold&quot;), axis.title.y = element_text(size = 25,face=&quot;bold&quot;)) + geom_vline(data = cut_point, aes(xintercept = z), linetype = &quot;dotted&quot;, size = 1.5) + annotation_custom(grob = textGrob(bquote(italic(&quot;O&quot;)), gp = gpar(fontsize = 32)), xmin = -3.2, xmax = -3.2, # adjust the position of &quot;O&quot; ymin = -0.0, ymax = -0.01) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) for (i in 1 : length(cut_point$z)){ rocPdfs &lt;- rocPdfs + annotation_custom(grob = textGrob(bquote(zeta[.(i)]),gp = gpar(fontsize = 20)), xmin = cut_point$z[i], xmax = cut_point$z[i], ymin = -0.01, ymax = -0.01) } gt &lt;- ggplot_gtable(ggplot_build(rocPdfs)) gt$layout$clip[gt$layout$name == &quot;panel&quot;] &lt;- &quot;off&quot; grid.draw(gt) In the unequal-variance binormal model, the variance \\(\\sigma^2\\) of the Z-samples for diseased cases, is allowed to be different from unity. Most ROC datasets are consistent with \\(\\sigma &gt; 1\\). The above figure, generated with \\(\\mu = 1.5, \\sigma = 1.5\\), illustrates how realized z-samples are converted to ratings, i.e., it illustrates application of the binning rule. For example, a case with Z-sample equal to -2.5 would be rated “1”, and one with Z-sample equal to -1 would be rated “2”, cases with Z-samples greater than 2.5 would be rated “5”, etc. The binormal model is not as restrictive as might appear at first sight. Any monotone increasing transformation \\(Y=f(Z)\\) applied to the observed z-samples, and the associated thresholds, will yield the same observed data, e.g., Table 6.1. This is because such a transformation leaves the ordering of the ratings unaltered and hence results in the same operating points. While the distributions for will not be binormal (i.e., two independent normal distributions), one can safely “pretend” that one is still dealing with an underlying binormal model. An alternative way of stating this is that any pair of distributions is allowed as long as they are reducible to a binormal model form by a monotonic increasing transformation of Y: e.g., \\(Z=f^{-1}\\). [If \\(f\\) is a monotone increasing function of its argument, so is \\(f^{-1}\\)}.] For this reason, the term “pair of latent underlying normal distributions” is sometimes used to describe the binormal model. The robustness of the binormal model has been investigated5,6. The referenced paper by Dorfman et al has an excellent discussion of the robustness of the binormal model. 5.2.2 Expressions for sensitivity and specificity Let \\(Z_t\\) denote the random Z-sample for truth state \\(t\\) (\\(t\\) = 1 for non-diseased and \\(t\\) = 2 for diseased cases). Since the distribution of Z-samples from disease-free cases is \\(N(0,1)\\), the expression for specificity, Chapter “Modeling Binary Paradigm”, Eqn. 3.13, applies. It is reproduced below: \\[\\begin{equation*} Sp\\left ( \\zeta \\right )=P\\left ( Z_1 &lt; \\zeta \\right )=\\Phi\\left ( \\zeta \\right ) \\end{equation*}\\] To obtain an expression for sensitivity, consider that for truth state \\(t = 2\\), the random variable \\(\\frac{Z_2-\\mu}{\\sigma}\\) is distributed as \\(N(0,1)\\): \\[\\begin{equation*} \\frac{Z_2-\\mu}{\\sigma}\\sim N\\left ( 0,1 \\right ) \\end{equation*}\\] Sensitivity is \\(P\\left ( Z_2 &gt; \\zeta \\right )\\), which implies, because \\(\\sigma\\) is positive (subtract from both sides of the “greater than” symbol and divide by \\(\\sigma\\)): \\[\\begin{equation*} Se\\left ( \\zeta | \\mu, \\sigma \\right )= P\\left ( Z_2 &gt; \\zeta \\right )=P\\left ( \\frac{Z_2-\\mu}{\\sigma} &gt; \\frac{\\zeta-\\mu}{\\sigma} \\right ) \\end{equation*}\\] The right-hand-side can be rewritten as follows: \\[\\begin{equation*} Se\\left ( \\zeta | \\mu, \\sigma \\right )= 1 - P\\left ( \\frac{Z_2-\\mu}{\\sigma} \\leq \\frac{\\zeta-\\mu}{\\sigma} \\right )\\\\ =1-\\Phi\\left ( \\frac{\\zeta-\\mu}{\\sigma}\\right )=\\Phi\\left ( \\frac{\\mu-\\zeta}{\\sigma}\\right ) \\end{equation*}\\] Summarizing, the formulae for the specificity and sensitivity for the binormal model are: \\[\\begin{equation*} Sp\\left ( \\zeta \\right ) = \\Phi\\left ( \\zeta \\right )\\\\ Se\\left ( \\zeta | \\mu, \\sigma \\right ) = \\Phi\\left ( \\frac{\\mu-\\zeta}{\\sigma}\\right ) \\end{equation*}\\] The coordinates of the operating point defined by \\(\\zeta\\) are given by: \\[\\begin{equation*} FPF\\left ( \\zeta \\right ) = 1 - Sp\\left ( \\zeta \\right ) = 1 - \\Phi\\left ( \\zeta \\right ) = \\Phi\\left ( -\\zeta \\right ) \\\\ TPF\\left ( \\zeta | \\mu, \\sigma \\right ) = \\Phi\\left ( \\frac{\\mu-\\zeta}{\\sigma} \\right ) \\end{equation*}\\] These expressions allow calculation of the operating point for any \\(\\zeta\\). An equation for a curve is usually expressed as \\(y=f(x)\\). An expression of this form for the ROC curve, i.e., the y coordinate (TPF) expressed as a function of the x coordinate (FPF), follows upon inversion of the expression for FPF, Eqn. (6.2.11): \\[\\begin{equation*} \\zeta = -\\Phi^{-1}\\left ( FPF \\right ) \\end{equation*}\\] Substituting Eqn. (6.2.13) in the expression for TPF in 2nd Eqn. (6.2.11): \\[\\begin{equation*} TPF = \\Phi\\left ( \\frac{\\mu + \\Phi^{-1}\\left (FPF \\right )}{\\sigma} \\right ) \\end{equation*}\\] 5.2.3 Binormal model in “conventional” notation The following notation is widely used in the literature: \\[\\begin{equation*} a=\\frac{\\mu}{\\sigma};b=\\frac{1}{\\sigma} \\end{equation*}\\] The reason for the \\((a,b)\\) instead of the \\((\\mu,\\sigma)\\) notation is that Dorfman and Alf assumed, in their seminal paper3, that the diseased (often termed signal) distribution had unit variance, and the non-diseased (often termed noise) distribution had standard deviation \\(b\\) (\\(b &gt; 0\\)) or variance \\(b^2\\), and that the separation of the two distributions was a, Fig. 6.2(A). In this example, \\(a = 1.11\\) and \\(b = 0.556\\). Dorfman and Alf’s fundamental contribution, namely estimating these parameters from ratings data, to be described below, has led to the widespread usage of the \\((a,b)\\) parameters, estimated by their software (RSCORE), and its modern variants (e.g., ROCFIT and ROCKIT). 5.2.4 Properties of the binormal model ROC curve Using the a, b notation, Eqn. (6.2.14) for the ROC curve reduces to: \\[\\begin{equation*} TPF = \\Phi\\left ( a+ b \\Phi^{-1}\\left (FPF \\right ) \\right ) \\end{equation*}\\] Since \\(\\Phi^{-1}(FPF)\\) is an increasing function of its argument FPF, and \\(b &gt; 0\\), the argument of the \\(\\Phi\\) function is an increasing function of FPF. Since \\(\\Phi\\) is a monotonically increasing function of its argument, TPF is a monotonically increasing function of FPF. This is true regardless of the sign of \\(a\\). If FPF = 0, then \\(\\Phi^{-1}(0) = -\\infty\\) and TPF = 0. If FPF = 1, then \\(\\Phi^{-1}(1) = +\\infty\\) and TPF = 1. [The fact that TPF is a monotonic increasing function of FPF is consistent with the following argument: to increase FPF, \\(\\zeta\\) must decrease, which will increase the area under the diseased distribution to the right of \\(\\zeta\\), i.e., increase TPF.] Regardless of the value of \\(a\\), as long as \\(b \\ge 0\\), the ROC curve starts at (0,0) and ends at (1,1), increasing monotonically from the origin to (1,1). From Eqn. (6.2.11) and Eqn. (6.2.12), the expressions for FPF and TPF in terms of model parameters \\((a,b)\\) are: \\[\\begin{equation*} FPF\\left ( \\zeta \\right ) = \\Phi\\left ( -\\zeta \\right )\\\\ TPF = \\Phi\\left ( a - b \\zeta \\right ) \\end{equation*}\\] 5.2.5 pdfs of the binormal model 5.2.6 A fitted ROC curve 5.3 Least-squares estimation 5.4 Maximum likelihood estimation (MLE) 5.4.1 Validating the fitting model 5.4.2 Estimating the covariance matrix 5.4.3 Estimating the variance of Az 5.5 Discussion I have tried to take out some of the mystery about how the binormal model is used to estimate parameters. A good understanding of this chapter should enable the reader to better understand alternative ROC models, discussed in a later chapter. It has been stated without explanation that the b-parameter of the binormal model is generally observed to be less than one, consistent with the diseased distribution being wider than the non-diseased one. The ROC literature is largely silent on the reason for this finding. One reason, namely location uncertainty, is presented in Chapter “Predictions of the RSM”, where RSM stands for Radiological Search Model. Basically, if the location of the lesion is unknown, then z-samples from diseased cases can be of two types, samples from the correct lesion location, or samples from other non-lesion locations. The resulting mixture distribution will then appear to have larger variance than the corresponding samples from non-diseased cases. This type of mixing need not be restricted to location uncertainty. Even is location is known, if the lesions are non-homogenous (e.g., they contain a range of contrasts) then a similar mixture-distribution induced broadening is expected. The fact that the b-parameter is less than unity implies that the predicted ROC curve is improper, meaning its slope is not monotone decreasing as the operating point moves up the curve. The result is that a portion of the curve, near (1,1) that crosses the chance-diagonal and hooks upward approaching (1,1) with infinite slope. Ways of fitting proper ROC curves are described in Chapter “Other proper ROC models”. Usually the hook is not readily visible, which has been used as an excuse to ignore the problem. For example, in Fig. 6.4, one would have to “zoom-in” on the upper right corner to see it, but the reader should make no mistake about it, the hook is there as . A recent example is Fig. 1 in the publication resulting from the Digital Mammographic Imaging Screening Trial (DMIST) clinical trial14,15, involving 49,528 asymptomatic women from 33 clinical sites and involving 153 radiologists, where each of the film modality ROC plots crosses the chance diagonal and hooks upwards to (1,1), which as is known, results anytime . The unphysical nature of the hook (predicting worse than chance-level performance for supposedly expert readers) is not the only reason for seeking alternate ROC models. The binormal model is susceptible to degeneracy problems. If the dataset does not provide any interior operating points (i.e., all observed points lie on the axes defined by FPF = 0 or TPF = 1) then the model fits these points with b = 0. The resulting straight-line segment fits do not make physical sense. These problems are addressed by the contaminated binormal model16 to be discussed in Chapter “Other proper ROC models”. The first paper in the series has particularly readable accounts of data degeneracy. To this day the binormal model is widely used to fit ROC datasets. In spite of its limitations, the binormal model has been very useful in bringing a level of quantification to this field that did not exist prior to the work3 by Dorfman and Alf. 5.6 References "],
["rocdataformat.html", "Chapter 6 ROC DATA FORMAT 6.1 Introduction 6.2 Note to existing users 6.3 The Excel data format 6.4 Illustrative toy file 6.5 The Truth worksheet 6.6 The structure of an ROC dataset 6.7 The false positive (FP) ratings 6.8 The true positive (TP) ratings 6.9 Correspondence between NL member of dataset and the FP worksheet 6.10 Correspondence between LL member of dataset and the TP worksheet 6.11 Correspondence using the which function 6.12 References", " Chapter 6 ROC DATA FORMAT 6.1 Introduction The purpose of this chapter is to explain the data format of the input Excel file and to introduce the capabilities of the function DfReadDataFile(). Background on observer performance methods are in my book (Chakraborty 2017). I will start with Receiver Operating Characteristic (ROC) data (Metz 1978), as this is by far the simplest paradigm. In the ROC paradigm the observer assigns a rating to each image. A rating is an ordered numeric label, and, in our convention, higher values represent greater certainty or confidence level for presence of disease. With human observers, a 5 (or 6) point rating scale is typically used, with 1 representing highest confidence for absence of disease and 5 (or 6) representing highest confidence for presence of disease. Intermediate values represent intermediate confidence levels for presence or absence of disease. Note that location information associated with the disease, if applicable, is not collected. There is no restriction to 5 or 6 ratings. With algorithmic observers, e.g., computer aided detection (CAD) algorithms, the rating could be a floating point number and have infinite precision. All that is required is that higher values correspond to greater confidence in presence of disease. 6.2 Note to existing users The Excel file format has recently undergone changes resulting in 4 extra list members in the final created dataset object (i.e., 12 members instead of 8). Code should run on the old format Excel files as the 4 extra list members are simply ignored. Reasons for the change will become clearer in these chapters. Basically they are needed for generalization to other data collection paradigms instead of crossed, for example to the split-plot data acquisition paradigm, and for better data entry error control. 6.3 The Excel data format The Excel file has three worksheets. These are named Truth, NL (or FP), LL (or TP). 6.4 Illustrative toy file Toy files are artificial small datasets intended to illustrate essential features of the data format. The examples shown in this chapter corresponds to Excel file inst/extdata/toyFiles/ROC/rocCr.xlsx in the project directory. To view these files one needs to clone the source files from GitHub. 6.5 The Truth worksheet The Truth worksheet contains 6 columns: CaseID, LesionID, Weight, ReaderID, ModalityID and Paradigm. For ROC data the first five columns contain as many rows as there are cases (images) in the dataset. CaseID: unique integers, one per case, representing the cases in the dataset. LesionID: integers 0 or 1, with each 0 representing a non-diseased case and each 1 representing a diseased case. In the current toy dataset, the non-diseased cases are labeled 1, 2 and 3, while the diseased cases are labeled 70, 71, 72, 73 and 74. The values do not have to be consecutive integers; they need not be ordered; the only requirement is that they be unique. Weight: Not used for ROC data, a floating point value, typically filled in with 0 or 1. ReaderID: a comma-separated listing of reader labels, each represented by a unique string, that have interpreted the case. In the example shown below each cell has the value 0, 1, 2, 3, 4 meaning that each of the readers, represented by the strings “0”, “1”, “2”, “3” and “4”, have interpreted all cases (hence the “crossed” design). With reader names that could be confused with integers, each cell in this column has to be text formatted as otherwise Excel will not accept it. [Try entering 0, 1, 2, 3, 4 in a numeric formatted Excel cell.] The reader names could just as well have been Rdr0, Rdr1, Rdr2, Rdr3, Rdr4. The only requirement is that they be unique strings. Look in in the inst/extdata/toyFiles/ROC directory for files rocCrStrRdrsTrts.xlsx and rocCrStrRdrsNonUnique.xlsx for examples of data files using longer strings for readers. The second file generates an error because the reader names are not unique. ModalityID: a comma-separated listing of modalities (one or more modalities), each represented by a unique string, that are applied to each case. In the example each cell has the value \"0\", \"1\". With treatment names that could be confused with integers, each cell has to be text formatted as otherwise Excel will not accept it. The treatment names could just as well have been Trt0, Trt1. Again, the only requirement is that they be unique strings. Paradigm: this column contains two cells, ROC and crossed. It informs the software that this is an ROC dataset, and the design is crossed, meaning each reader has interpreted each case in each modality (in statistical terminology: modality and reader factors are “crossed”). There are 5 diseased cases in the dataset (the number of 1’s in the LesionID column of the Truth worksheet). There are 3 non-diseased cases in the dataset (the number of 0’s in the LesionID column). There are 5 readers in the dataset (each cell in the ReaderID column contains the string 0, 1, 2, 3, 4). There are 2 modalities in the dataset (each cell in the ModalityID column contains the string 0, 1). FIGURE 6.1: Truth worksheet for file rocCr.xlsx 6.6 The structure of an ROC dataset In the following code chunk the first statement retrieves the name of the data file, located in a hidden directory that one need not be concerned with. The second statement reads the file using the function DfReadDataFile() and saves it to object x. The third statement shows the structure of the dataset object x. rocCr &lt;- system.file(&quot;extdata&quot;, &quot;toyFiles/ROC/rocCr.xlsx&quot;, package = &quot;RJafroc&quot;, mustWork = TRUE) x &lt;- DfReadDataFile(rocCr, newExcelFileFormat = TRUE) str(x) #&gt; List of 12 #&gt; $ NL : num [1:2, 1:5, 1:8, 1] 1 3 2 3 2 2 1 2 3 2 ... #&gt; $ LL : num [1:2, 1:5, 1:5, 1] 5 5 5 5 5 5 5 5 5 5 ... #&gt; $ lesionVector : int [1:5] 1 1 1 1 1 #&gt; $ lesionID : num [1:5, 1] 1 1 1 1 1 #&gt; $ lesionWeight : num [1:5, 1] 1 1 1 1 1 #&gt; $ dataType : chr &quot;ROC&quot; #&gt; $ modalityID : Named chr [1:2] &quot;0&quot; &quot;1&quot; #&gt; ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;0&quot; &quot;1&quot; #&gt; $ readerID : Named chr [1:5] &quot;0&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; ... #&gt; ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;0&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; ... #&gt; $ design : chr &quot;CROSSED&quot; #&gt; $ normalCases : int [1:3] 1 2 3 #&gt; $ abnormalCases: int [1:5] 70 71 72 73 74 #&gt; $ truthTableStr: num [1:2, 1:5, 1:8, 1:2] 1 1 1 1 1 1 1 1 1 1 ... In the above code chunk flag newExcelFileFormat is set to TRUE as otherwise columns D - F in the Truth worksheet are ignored and the dataset is assumed to be crossed, with dataType automatically determined from the contents of the FP and TP worksheets. Flag newExcelFileFormat = FALSE is for compatibility with older JAFROC format Excel files, which did not have these columns in the Truth worksheet. Its usage is deprecated. The dataset object x is a list variable with 12 members. The x$NL member, with dimension [2, 5, 8, 1], contains the ratings of normal cases. The extra values in the third dimension, filled with NAs, are needed for compatibility with FROC datasets, as unlike ROC, false positives are possible on diseased cases. The x$LL, with dimension [2, 5, 5, 1], contains the ratings of abnormal cases. The x$lesionVector member is a vector with 5 ones representing the 5 diseased cases in the dataset. The x$lesionID member is an array with 5 ones. The x$lesionWeight member is an array with 5 ones. The lesionVector, lesionID and lesionWeight members are not used for ROC datasets. They are there for compatibility with FROC datasets. The dataType member indicates that this is an ROC dataset. The x$modalityID member is a vector with two elements \"0\" and \"1\", naming the two modalities. The x$readerID member is a vector with five elements \"0\", \"1\", \"2\", \"3\" and \"4\", naming the five readers. The x$design member is CROSSED; specifies the dataset design, which is “CROSSED”. The x$normalCases member lists the integer names of the normal cases, 1, 2, 3. The x$abnormalCases member lists the integer names of the abnormal cases, 70, 71, 72, 73, 74. The x$truthTableStr member quantifies the structure of the dataset, as explained in prior chapters. 6.7 The false positive (FP) ratings These are found in the FP or NL worksheet, see below. FIGURE 6.2: FP worksheet for file rocCr.xlsx It consists of 4 columns, each of length 30 (= # of modalities times number of readers times number of non-diseased cases). ReaderID: the reader labels: 0, 1, 2, 3 and 4. Each reader label occurs 6 times (= # of modalities times number of non-diseased cases). ModalityID: the modality or treatment labels: 0 and 1. Each label occurs 15 times (= # of readers times number of non-diseased cases). CaseID: the case labels for non-diseased cases: 1, 2 and 3. Each label occurs 10 times (= # of modalities times # of readers). The label of a diseased case cannot occur in the FP worksheet. If it does the software generates an error. FP_Rating: the floating point ratings of non-diseased cases. Each row of this worksheet contains a rating corresponding to the values of ReaderID, ModalityID and CaseID for that row. 6.8 The true positive (TP) ratings These are found in the TP or LL worksheet, see below. FIGURE 6.3: TP worksheet for file rocCr.xlsx It consists of 5 columns, each of length 50 (= # of modalities times number of readers times number of diseased cases). ReaderID: the reader labels: 0, 1, 2, 3 and 4. Each reader label occurs 10 times (= # of modalities times number of diseased cases). ModalityID: the modality or treatment labels: 0 and 1. Each label occurs 25 times (= # of readers times number of diseased cases). LesionID: For an ROC dataset this column contains fifty 1’s (each diseased case has one lesion). CaseID: the case labels for non-diseased cases: 70, 71, 72, 73 and 74. Each label occurs 10 times (= # of modalities times # of readers). The label of a non-diseased case cannot occur in the TP worksheet. TP_Rating: the floating point ratings of diseased cases. Each row of this worksheet contains a rating corresponding to the values of ReaderID, ModalityID, LesionID and CaseID for that row. 6.9 Correspondence between NL member of dataset and the FP worksheet The list member x$NL is an array with dim = c(2,5,8,1). The first dimension (2) comes from the number of modalities. The second dimension (5) comes from the number of readers. The third dimension (8) comes from the total number of cases. The fourth dimension is alway 1 for an ROC dataset. The value of x$NL[1,5,2,1], i.e., 5, corresponds to row 15 of the FP table, i.e., to ModalityID = 0, ReaderID = 4 and CaseID = 2. The value of x$NL[2,3,2,1], i.e., 4, corresponds to row 24 of the FP table, i.e., to ModalityID 1, ReaderID 2 and CaseID 2. All values for case index &gt; 3 are -Inf. For example the value of x$NL[2,3,4,1] is -Inf. This is because there are only 3 non-diseased cases. The extra length is needed for compatibility with FROC datasets. 6.10 Correspondence between LL member of dataset and the TP worksheet The list member x$LL is an array with dim = c(2,5,5,1). The first dimension (2) comes from the number of modalities. The second dimension (5) comes from the number of readers. The third dimension (5) comes from the number of diseased cases. The fourth dimension is alway 1 for an ROC dataset. The value of x$LL[1,1,5,1], i.e., 4, corresponds to row 6 of the TP table, i.e., to ModalityID = 0, ReaderID = 0 and CaseID = 74. The value of x$LL[1,2,2,1], i.e., 3, corresponds to row 8 of the TP table, i.e., to ModalityID = 0, ReaderID = 1 and CaseID = 71. There are no -Inf values in x$LL: any(x$LL == -Inf) = FALSE. 6.11 Correspondence using the which function Converting from names to subscripts (indicating position in an array) can be confusing. The following example uses the which function to help out. The first line says that the abnormalCase named 70 corresponds to subscript 1 in the LL array case dimension. The second line prints the NL rating for modalityID = 0, readerID = 1 and normalCases = 1. The third line prints the LL rating for modalityID = 0, readerID = 1 and abnormalCases = 70. The last line shows what happens if one enters an invalid value for name; the result is a numeric(0). Note that in each of these examples, the last dimension is 1 because we are dealing with an ROC dataset. The reader is encouraged to examine the correspondence between the NL and LL ratings and the Excel file using this method. which(x$abnormalCases == 70) #&gt; [1] 1 x$NL[which(x$modalityID == &quot;0&quot;),which(x$readerID == &quot;1&quot;),which(x$normalCases == 1),1] #&gt; [1] 2 x$LL[which(x$modalityID == &quot;0&quot;),which(x$readerID == &quot;1&quot;),which(x$abnormalCases == 70),1] #&gt; [1] 5 x$LL[which(x$modalityID == &quot;a&quot;),which(x$readerID == &quot;1&quot;),which(x$abnormalCases == 70),1] #&gt; numeric(0) 6.12 References REFERENCES "],
["frocdataformat.html", "Chapter 7 FROC data format 7.1 Purpose 7.2 Introduction 7.3 The Excel data format 7.4 The Truth worksheet 7.5 The structure of an FROC dataset 7.6 The false positive (FP) ratings 7.7 The true positive (TP) ratings 7.8 On the distribution of numbers of lesions in abnormal cases 7.9 Definition of lesWghtDistr array 7.10 Summary 7.11 References", " Chapter 7 FROC data format 7.1 Purpose Explain the data format of the input Excel file for FROC datasets. Explain the format of the FROC dataset. Explain the lesion distribution array returned by UtilLesionDistr(). Explain the lesion weights array returned by UtilLesionWeightsDistr(). Details on the FROC paradigm are in my book. 7.2 Introduction See my book Chakraborty (2017) for full details. In the Free-response Receiver Operating Characteristic (FROC) paradigm (Chakraborty 1989) the observer searches each case for signs of localized disease and marks and rates localized regions that are sufficiently suspicious for disease presence. FROC data consists of mark-rating pairs, where each mark is a localized-region that was considered sufficiently suspicious for presence of a localized lesion and the rating is the corresponding confidence level. By adopting a proximity criterion, each mark is classified by the investigator as a lesion localization (LL) - if it is close to a real lesion - or a non-lesion localization (NL) otherwise. The observer assigns a rating to each region. The rating, as in the ROC paradigm, can be an integer or quasi-continuous (e.g., 0 – 100), or a floating point value, as long as higher numbers represent greater confidence in presence of a lesion at the indicated region. 7.3 The Excel data format The Excel file has three worsheets. These are named Truth, NL or FP and LL or TP. 7.4 The Truth worksheet The Truth worksheet contains 6 columns: CaseID, LesionID, Weight, ReaderID, ModalityID and Paradigm. Since a diseased case may have more than one lesion, the first five columns contain at least as many rows as there are cases (images) in the dataset. CaseID: unique integers, one per case, representing the cases in the dataset. LesionID: integers 0, 1, 2, etc., with each 0 representing a non-diseased case, 1 representing the first lesion on a diseased case, 2 representing the second lesion on a diseased case, if present, and so on. The non-diseased cases are labeled 1, 2 and 3, while the diseased cases are labeled 70, 71, 72, 73 and 74. There are 3 non-diseased cases in the dataset (the number of 0’s in the LesionID column). There are 5 diseased cases in the dataset (the number of 1’s in the LesionID column of the Truth worksheet). There are 3 readers in the dataset (each cell in the ReaderID column contains 0, 1, 2). There are 2 modalities in the dataset (each cell in the ModalityID column contains 0, 1). Weight: floating point; 0, for each non-diseased case, or values for each diseased case that add up to unity. Diseased case 70 has two lesions, with LesionIDs 1 and 2, and weights 0.3 and 0.7. Diseased case 71 has one lesion, with LesionID = 1, and Weight = 1. Diseased case 72 has three lesions, with LesionIDs 1, 2 and 3 and weights 1/3 each. Diseased case 73 has two lesions, with LesionIDs 1, and 2 and weights 0.1 and 0.9. Diseased case 74 has one lesion, with LesionID = 1 and Weight = 1. ReaderID: a comma-separated listing of readers, each represented by a unique integer, that have interpreted the case. In the example shown below each cell has the value 0, 1, 2. Each cell has to be text formatted. Otherwise Excel will not accept it. ModalityID: a comma-separated listing of modalities (or treatments), each represented by a unique integer, that apply to each case. In the example each cell has the value 0, 1. Each cell has to be text formatted. Paradigm: In the example shown below, the contents are FROC and crossed. It informs the software that this is an FROC dataset and the design is “crossed”, as in TBA chapter xx. FIGURE 7.1: Truth worksheet for file inst/extdata/toyFiles/FROC/frocCr.xlsx 7.5 The structure of an FROC dataset The example shown above corresponds to Excel file inst/extdata/toyFiles/FROC/frocCr.xlsx in the project directory. frocCr &lt;- system.file(&quot;extdata&quot;, &quot;toyFiles/FROC/frocCr.xlsx&quot;, package = &quot;RJafroc&quot;, mustWork = TRUE) x &lt;- DfReadDataFile(frocCr, newExcelFileFormat = TRUE) str(x) #&gt; List of 12 #&gt; $ NL : num [1:2, 1:3, 1:8, 1:2] 1.02 2.89 2.21 3.01 2.14 ... #&gt; $ LL : num [1:2, 1:3, 1:5, 1:3] 5.28 5.2 5.14 4.77 4.66 4.87 3.01 3.27 3.31 3.19 ... #&gt; $ lesionVector : int [1:5] 2 1 3 2 1 #&gt; $ lesionID : num [1:5, 1:3] 1 1 1 1 1 ... #&gt; $ lesionWeight : num [1:5, 1:3] 0.3 1 0.333 0.1 1 ... #&gt; $ dataType : chr &quot;FROC&quot; #&gt; $ modalityID : Named chr [1:2] &quot;0&quot; &quot;1&quot; #&gt; ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;0&quot; &quot;1&quot; #&gt; $ readerID : Named chr [1:3] &quot;0&quot; &quot;1&quot; &quot;2&quot; #&gt; ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;0&quot; &quot;1&quot; &quot;2&quot; #&gt; $ design : chr &quot;CROSSED&quot; #&gt; $ normalCases : int [1:3] 1 2 3 #&gt; $ abnormalCases: int [1:5] 70 71 72 73 74 #&gt; $ truthTableStr: num [1:2, 1:3, 1:8, 1:4] 1 1 1 1 1 1 1 1 1 1 ... This follows the general description in TBA chapter xx. The differences are described below. The x$dataType member indicates that this is an FROC dataset. The x$lesionVector member is a vector whose contents reflect the number of lesions in each diseased case, i.e., 2, 1, 3, 2, 1 in the current example. The x$lesionID member indicates the labeling of the lesions in each diseased case. x$lesionID #&gt; [,1] [,2] [,3] #&gt; [1,] 1 2 -Inf #&gt; [2,] 1 -Inf -Inf #&gt; [3,] 1 2 3 #&gt; [4,] 1 2 -Inf #&gt; [5,] 1 -Inf -Inf This shows that the lesions on the first diseased case are labeled 1 and 2. The -Inf is a filler used to denote a missing value. The second diseased case has one lesion labeled 1. The third diseased case has three lesions labeled 1, 2 and 3, etc. The lesionWeight member is the clinical importance of each lesion. Lacking specific clinical reasons, the lesions should be equally weighted; this is not true for this toy dataset. x$lesionWeight #&gt; [,1] [,2] [,3] #&gt; [1,] 0.3000000 0.7000000 -Inf #&gt; [2,] 1.0000000 -Inf -Inf #&gt; [3,] 0.3333333 0.3333333 0.3333333 #&gt; [4,] 0.1000000 0.9000000 -Inf #&gt; [5,] 1.0000000 -Inf -Inf The first diseased case has two lesions, the first has weight 0.3 and the second has weight 0.7. The second diseased case has one lesion with weight 1.The third diseased case has three equally weighted lesions, each with weight 1/3. Etc. 7.6 The false positive (FP) ratings These are found in the FP or NL worksheet, see below. FIGURE 7.2: Fig. 2: FP/NL worksheet for file inst/extdata/toyFiles/FROC/frocCr.xlsx It consists of 4 columns, of equal length. The common length is unpredictable. It could be zero if the dataset has no NL marks (a distinct possibility if the lesions are very easy to find and the modality and/or observer has high performance). All one knows is that the common length is an integer greater than or equal to zero. In the example dataset, the common length is 22. ReaderID: the reader labels: these must be 0, 1, or 2, as declared in the Truth worksheet. ModalityID: the modality labels: must be 0 or 1, as declared in the Truth worksheet. CaseID: the labels of cases with NL marks. In the FROC paradigm, NL events can occur on non-diseased and diseased cases. FP_Rating: the floating point ratings of NL marks. Each row of this worksheet yields a rating corresponding to the values of ReaderID, ModalityID and CaseID for that row. For ModalityID 0, ReaderID 0 and CaseID 1 (the first non-diseased case declared in the Truth worksheet), there is a single NL mark that was rated 1.02, corresponding to row 2 of the FP worksheet. Diseased cases with NL marks are also declared in the FP worksheet. Some examples are seen at rows 15, 16 and 21-23 of the FP worksheet. Rows 21 and 22 show that caseID = 71 got two NL marks, rated 2.24, 4.01. That this is the only case with two marks determines the length of the fourth dimension of the x$NL list member, 2 in the current example. Absent this case, the length would have been one. In general, the case with the most NL marks determines the length of the fourth dimension of the x$NL list member. The reader should convince oneself that the ratings in x$NL reflect the contents of the FP worksheet. 7.7 The true positive (TP) ratings These are found in the TP or LL worksheet, see below. FIGURE 7.3: Fig. 3: TP/LL worksheet for file inst/extdata/toyFiles/FROC/frocCr.xlsx This worksheet can only have diseased cases. The presence of a non-diseased case in this worksheet will generate an error. The common vertical length, 31 in this example, is a-priori unpredictable. Given the structure of the Truth worsheet for this dataset, the maximum length would be 9 times 2 times 3, assuming every lesion is marked for each modality, reader and diseased case. The 9 comes from the total number of non-zero entries in the LesionID column of the Truth worksheet. The fact that the length is smaller than the maximum length means that there are combinations of modality, reader and diseased cases on which some lesions were not marked. As an example, the first lesion in CaseID equal to 70 was marked (and rated 5.28) in ModalityID 0 and ReaderID 0. The length of the fourth dimension of the x$LL list member, 3 in the present example, is determined by the diseased case with the most lesions in the Truth worksheet. The reader should convince oneself that the ratings in x$LL reflect the contents of the TP worksheet. 7.8 On the distribution of numbers of lesions in abnormal cases Consider a much larger dataset, dataset11, with structure as shown below: x &lt;- dataset11 str(x) #&gt; List of 12 #&gt; $ NL : num [1:4, 1:5, 1:158, 1:4] -Inf -Inf -Inf -Inf -Inf ... #&gt; $ LL : num [1:4, 1:5, 1:115, 1:20] -Inf -Inf -Inf -Inf -Inf ... #&gt; $ lesionVector : int [1:115] 6 4 7 1 3 3 3 8 11 2 ... #&gt; $ lesionID : num [1:115, 1:20] 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ lesionWeight : num [1:115, 1:20] 0.167 0.25 0.143 1 0.333 ... #&gt; $ dataType : chr &quot;FROC&quot; #&gt; $ modalityID : Named chr [1:4] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; #&gt; ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; #&gt; $ readerID : Named chr [1:5] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... #&gt; ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... #&gt; $ design : chr &quot;CROSSED&quot; #&gt; $ normalCases : int [1:43] 6 9 14 27 62 66 70 71 83 91 ... #&gt; $ abnormalCases: int [1:115] 1 2 3 5 7 8 10 11 13 17 ... #&gt; $ truthTableStr: num [1:4, 1:5, 1:158, 1:21] 1 1 1 1 1 1 1 1 1 1 ... Focus for now in the 115 abnormal cases. The numbers of lesions in these cases is contained in x$lesionVector. x$lesionVector #&gt; [1] 6 4 7 1 3 3 3 8 11 2 4 6 2 16 5 2 8 3 4 7 11 1 4 3 4 #&gt; [26] 4 7 3 2 5 2 2 7 6 6 4 10 20 12 6 4 7 12 5 1 1 5 1 2 8 #&gt; [51] 3 1 2 2 3 2 8 16 10 1 2 2 6 3 2 2 4 6 10 11 1 2 6 2 4 #&gt; [76] 5 2 9 6 6 8 3 8 7 1 1 6 3 2 1 9 8 8 2 2 12 1 1 1 1 #&gt; [101] 1 3 1 2 2 1 1 1 1 3 1 1 1 2 1 For example, the first abnormal case contains 6 lesions, the second contains 4 lesions, the third contains 7 lesions, etc. and the last abnormal case contains 1 lesion. To get an idea of the distribution of the numbers of lesions per abnormal cases, one could interrogate this vector as shown below using the which() function: for (el in 1:max(x$lesionVector)) cat( &quot;abnormal cases with&quot;, el, &quot;lesions = &quot;, length(which(x$lesionVector == el)), &quot;\\n&quot;) #&gt; abnormal cases with 1 lesions = 25 #&gt; abnormal cases with 2 lesions = 23 #&gt; abnormal cases with 3 lesions = 13 #&gt; abnormal cases with 4 lesions = 10 #&gt; abnormal cases with 5 lesions = 5 #&gt; abnormal cases with 6 lesions = 11 #&gt; abnormal cases with 7 lesions = 6 #&gt; abnormal cases with 8 lesions = 8 #&gt; abnormal cases with 9 lesions = 2 #&gt; abnormal cases with 10 lesions = 3 #&gt; abnormal cases with 11 lesions = 3 #&gt; abnormal cases with 12 lesions = 3 #&gt; abnormal cases with 13 lesions = 0 #&gt; abnormal cases with 14 lesions = 0 #&gt; abnormal cases with 15 lesions = 0 #&gt; abnormal cases with 16 lesions = 2 #&gt; abnormal cases with 17 lesions = 0 #&gt; abnormal cases with 18 lesions = 0 #&gt; abnormal cases with 19 lesions = 0 #&gt; abnormal cases with 20 lesions = 1 This tells us that 25 cases contain 1 lesion Likewise, 23 cases contain 2 lesions Etc. 7.8.1 Definition of lesDistr array Let us ask what is the fraction of (abnormal) cases with 1 lesion, 2 lesions etc. for (el in 1:max(x$lesionVector)) cat(&quot;fraction of abnormal cases with&quot;, el, &quot;lesions = &quot;, length(which(x$lesionVector == el))/length(x$LL[1,1,,1]), &quot;\\n&quot;) #&gt; fraction of abnormal cases with 1 lesions = 0.2173913 #&gt; fraction of abnormal cases with 2 lesions = 0.2 #&gt; fraction of abnormal cases with 3 lesions = 0.1130435 #&gt; fraction of abnormal cases with 4 lesions = 0.08695652 #&gt; fraction of abnormal cases with 5 lesions = 0.04347826 #&gt; fraction of abnormal cases with 6 lesions = 0.09565217 #&gt; fraction of abnormal cases with 7 lesions = 0.05217391 #&gt; fraction of abnormal cases with 8 lesions = 0.06956522 #&gt; fraction of abnormal cases with 9 lesions = 0.0173913 #&gt; fraction of abnormal cases with 10 lesions = 0.02608696 #&gt; fraction of abnormal cases with 11 lesions = 0.02608696 #&gt; fraction of abnormal cases with 12 lesions = 0.02608696 #&gt; fraction of abnormal cases with 13 lesions = 0 #&gt; fraction of abnormal cases with 14 lesions = 0 #&gt; fraction of abnormal cases with 15 lesions = 0 #&gt; fraction of abnormal cases with 16 lesions = 0.0173913 #&gt; fraction of abnormal cases with 17 lesions = 0 #&gt; fraction of abnormal cases with 18 lesions = 0 #&gt; fraction of abnormal cases with 19 lesions = 0 #&gt; fraction of abnormal cases with 20 lesions = 0.008695652 This tells us that fraction 0.217 of (abnormal) cases contain 1 lesion And fraction 0.2 of (abnormal) cases contain 2 lesions Etc. This information is contained the the lesDistr array It is coded in the Utility function UtilLesionDistr() lesDistr &lt;- UtilLesionDistr(x) lesDistr #&gt; [,1] [,2] #&gt; [1,] 1 0.217391304 #&gt; [2,] 2 0.200000000 #&gt; [3,] 3 0.113043478 #&gt; [4,] 4 0.086956522 #&gt; [5,] 5 0.043478261 #&gt; [6,] 6 0.095652174 #&gt; [7,] 7 0.052173913 #&gt; [8,] 8 0.069565217 #&gt; [9,] 9 0.017391304 #&gt; [10,] 10 0.026086957 #&gt; [11,] 11 0.026086957 #&gt; [12,] 12 0.026086957 #&gt; [13,] 16 0.017391304 #&gt; [14,] 20 0.008695652 The UtilLesionDistr() function returns an array with two columns and number of rows equal to the number of distinct values of lesions per case. The first column contains the number of distinct values of lesions per case, 14 in the current example. The second column contains the fraction of diseased cases with the number of lesions indicated in the first column. The second column must sum to unity sum(UtilLesionDistr(x)[,2]) #&gt; [1] 1 The lesion distribution array will come in handy when it comes to predicting the operating characteristics from using the Radiological Search Model (RSM), as detailed in Chapter 17 of my book. 7.9 Definition of lesWghtDistr array This is returned by UtilLesionWeightsDistr(). This contains the same number of rows as lesDistr. The number of columns is one plus the number of rows as lesDistr. The first column contains the number of distinct values of lesions per case, 14 in the current example. The second column contains the weights of cases with number of lesions per case corresponding to row 1. The third column contains the weights of cases with number of lesions per case corresponding to row 2. Etc. Missing values are filled with -Inf. lesWghtDistr &lt;- UtilLesionWeightsDistr(x) cat(&quot;dim(lesDistr) =&quot;, dim(lesDistr),&quot;\\n&quot;) #&gt; dim(lesDistr) = 14 2 cat(&quot;dim(lesWghtDistr) =&quot;, dim(lesWghtDistr),&quot;\\n&quot;) #&gt; dim(lesWghtDistr) = 14 21 cat(&quot;lesWghtDistr = \\n\\n&quot;) #&gt; lesWghtDistr = lesWghtDistr #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] #&gt; [1,] 1 1.00000000 -Inf -Inf -Inf -Inf -Inf #&gt; [2,] 2 0.50000000 0.50000000 -Inf -Inf -Inf -Inf #&gt; [3,] 3 0.33333333 0.33333333 0.33333333 -Inf -Inf -Inf #&gt; [4,] 4 0.25000000 0.25000000 0.25000000 0.25000000 -Inf -Inf #&gt; [5,] 5 0.20000000 0.20000000 0.20000000 0.20000000 0.20000000 -Inf #&gt; [6,] 6 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 #&gt; [7,] 7 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 #&gt; [8,] 8 0.12500000 0.12500000 0.12500000 0.12500000 0.12500000 0.12500000 #&gt; [9,] 9 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 #&gt; [10,] 10 0.10000000 0.10000000 0.10000000 0.10000000 0.10000000 0.10000000 #&gt; [11,] 11 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909 #&gt; [12,] 12 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333 #&gt; [13,] 16 0.06250000 0.06250000 0.06250000 0.06250000 0.06250000 0.06250000 #&gt; [14,] 20 0.05000000 0.05000000 0.05000000 0.05000000 0.05000000 0.05000000 #&gt; [,8] [,9] [,10] [,11] [,12] [,13] [,14] #&gt; [1,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [2,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [3,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [4,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [5,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [6,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [7,] 0.14285714 -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [8,] 0.12500000 0.12500000 -Inf -Inf -Inf -Inf -Inf #&gt; [9,] 0.11111111 0.11111111 0.11111111 -Inf -Inf -Inf -Inf #&gt; [10,] 0.10000000 0.10000000 0.10000000 0.10000000 -Inf -Inf -Inf #&gt; [11,] 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909 -Inf -Inf #&gt; [12,] 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333 -Inf #&gt; [13,] 0.06250000 0.06250000 0.06250000 0.06250000 0.06250000 0.06250000 0.0625 #&gt; [14,] 0.05000000 0.05000000 0.05000000 0.05000000 0.05000000 0.05000000 0.0500 #&gt; [,15] [,16] [,17] [,18] [,19] [,20] [,21] #&gt; [1,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [2,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [3,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [4,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [5,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [6,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [7,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [8,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [9,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [10,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [11,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [12,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [13,] 0.0625 0.0625 0.0625 -Inf -Inf -Inf -Inf #&gt; [14,] 0.0500 0.0500 0.0500 0.05 0.05 0.05 0.05 Row 3 corresponds to 3 lesions per case and the weights are 1/3, 1/3 and 1/3. Row 13 corresponds to 16 lesions per case and the weights are 0.06250000, 0.06250000, …, repeated 13 times. Note that the number of rows is less than the maximum number of lesions per case (20). This is because some configurations of lesions per case (e.g., cases with 13 lesions per case) do not occur in this dataset. 7.10 Summary The FROC dataset has far less regularity in structure as compared to an ROC dataset. The length of the first dimension of either x$NL or x$LL list members is the total number of modalities, 2 in the current example. The length of the second dimension of either x$NL or x$LL list members is the total number of readers, 3 in the current example. The length of the third dimension of x$NL is the total number of cases, 8 in the current example. The first three positions account for NL marks on non-diseased cases and the remaining 5 positions account for NL marks on diseased cases. The length of the third dimension of x$LL is the total number of diseased cases, 5 in the current example. The length of the fourth dimension of x$NL is determined by the case (diseased or non-diseased) with the most NL marks, 2 in the current example. The length of the fourth dimension of x$LL is determined by the diseased case with the most lesions, 3 in the current example. 7.11 References REFERENCES "],
["rocSpdataformat.html", "Chapter 8 ROC split plot data format 8.1 Introduction 8.2 The Excel data format 8.3 The Truth worksheet 8.4 The structure of the ROC split plot dataset 8.5 The truthTableStr member 8.6 The false positive (FP) ratings 8.7 The true positive (TP) ratings 8.8 Summary 8.9 References", " Chapter 8 ROC split plot data format 8.1 Introduction The purpose of this chapter is to explain the data format of the input Excel file for an ROC split-plot dataset. In a split-plot dataset each reader interprets a different sub-set of cases in all modalities, i.e., the cases interpreted by different readers have no overlap. Each sub-set of cases can have different numbers of non-diseased and diseased cases. The example below assumes the same numbers of non-diseased and diseased cases. The data format has been extended to NewFormat to allow such datasets. 8.2 The Excel data format As before,the Excel file has three worsheets named Truth, NL or FP and LL or TP. The Excel file corresponding to the example that follows is inst/extdata/toyFiles/ROC/rocSp.xlsx. 8.3 The Truth worksheet The Truth worksheet contains 6 columns: CaseID, LesionID, Weight, ReaderID, ModalityID and Paradigm. The first five columns contain as many rows as there are cases in the dataset. CaseID: unique integers, one per case, representing the cases in the dataset. LesionID: integers 0, representing non-diseased cases and 1 representing the diseased cases. The ReaderID column is a listing of readers each represented by a unique string. Note that, unlike the crossed design, the ReaderID column has single values. Each cell has to be text formatted. The non-diseased cases interpreted by reader with ReaderID value 1 are labeled 6, 7, 8, 9 and 10, each with LesionID value 0. The diseased cases interpreted by this reader are labeled 16, 17, 18, 19 and 20, each with LesionID value 1. The second reader, with ReaderID value 4, interprets five non-diseased cases labeled 21, 22, 23, 24 and 25, each with LesionID value 0, and five diseased cases labeled 36, 37, 38, 39 and 40, each with LesionID value 1. The third reader, with ReaderID value 5, interprets five non-diseased cases labeled 46, 47, 48, 49 and 50, each with LesionID value 0 and five diseased cases labeled 51, 52, 53, 54 and 55, each with LesionID value 1. Weight: floating point value 0 - this is not used for ROC data. ModalityID: a comma-separated listing of modalities, each represented by a unique string. In the example shown below each cell has the value 1, 2. Each cell has to be text formatted. Paradigm: In the example shown here, the contents are ROC and split-plot. FIGURE 8.1: Fig. 1: Truth worksheet for file inst/extdata/toyFiles/ROC/rocSp.xlsx 8.4 The structure of the ROC split plot dataset The example shown in Fig. 1 corresponds to Excel file inst/extdata/toyFiles/ROC/rocSp.xlsx in the project directory. rocSp &lt;- system.file(&quot;extdata&quot;, &quot;toyFiles/ROC/rocSp.xlsx&quot;, package = &quot;RJafroc&quot;, mustWork = TRUE) x &lt;- DfReadDataFile(rocSp, newExcelFileFormat = TRUE) str(x) #&gt; List of 12 #&gt; $ NL : num [1:2, 1:3, 1:30, 1] 1 1 -Inf -Inf -Inf ... #&gt; $ LL : num [1:2, 1:3, 1:15, 1] 5 2.3 -Inf -Inf -Inf ... #&gt; $ lesionVector : int [1:15] 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ lesionID : num [1:15, 1] 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ lesionWeight : num [1:15, 1] 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ dataType : chr &quot;ROC&quot; #&gt; $ modalityID : Named chr [1:2] &quot;1&quot; &quot;2&quot; #&gt; ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;1&quot; &quot;2&quot; #&gt; $ readerID : Named chr [1:3] &quot;1&quot; &quot;4&quot; &quot;5&quot; #&gt; ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;1&quot; &quot;4&quot; &quot;5&quot; #&gt; $ design : chr &quot;SPLIT-PLOT&quot; #&gt; $ normalCases : int [1:15] 6 7 8 9 10 21 22 23 24 25 ... #&gt; $ abnormalCases: int [1:15] 16 17 18 19 20 36 37 38 39 40 ... #&gt; $ truthTableStr: num [1:2, 1:3, 1:30, 1:2] 1 1 NA NA NA NA 1 1 NA NA ... DfReadDataFile() flag newExcelFileFormat must be set to TRUE for split plot data. The dataset object x is a list variable with 12 members. There are 15 diseased cases in the dataset (the number of 1’s in the LesionID column of the Truth worksheet) and 15 non-diseased cases (the number of 0’s in the LesionID column). x$NL, with dimension [2, 3, 30, 1], contains the ratings of normal cases. The extra values in the third dimension, filled with NAs, are needed for compatibility with FROC datasets. x$LL, with dimension [2, 3, 15, 1], contains the ratings of abnormal cases. The x$lesionVector member is a vector with 15 ones representing the 15 diseased cases in the dataset. The x$lesionID member is an array with 15 ones (this member is needed for compatibility with FROC datasets). The x$lesionWeight member is an array with 15 ones (this member is needed for compatibility with FROC datasets). The dataType member is ROC which specifies the data collection method (“ROC”, “FROC”, “LROC” or “ROI”). The x$modalityID member is a vector with two elements \"1\" and \"2\", naming the two modalities. The x$readerID member is a vector with three elements \"1\", \"4\" and \"5\", naming the three modalities. The x$design member is SPLIT-PLOT; specifies the dataset design, which can be either “CROSSED” or “SPLIT-PLOT”. The x$normalCases member lists the names of the normal cases, 6, 7, 8, 9, 10, 21, 22, 23, 24, 25, 46, 47, 48, 49, 50. The x$abnormalCases member lists the names of the abnormal cases, 16, 17, 18, 19, 20, 36, 37, 38, 39, 40, 51, 52, 53, 54, 55. The x$truthTableStr member quantifies the structure of the dataset, as explained next. It is used in the DfReadDataFile() function to check for data entry errors. 8.5 The truthTableStr member This is a 2 x 3 x 30 x 2 array, i.e., I x J x K x (maximum number of lesions per case plus 1). The plus 1 is needed to accommodate normal cases with lesionID = 0. [Zero is not a valid array subscript in R.] Each entry in this array is either 1, meaning the corresponding interpretation exists, or NA, meaning the corresponding interpretation does not exist. For example, x$truthTableStr[1,1,1,1] is 1. This means that an interpretation exists for the first treatment (modalityID = 1), first reader (readerID = 1) and first (normal) case (caseID = 6 and lesionID = 0). This example corresponds to row 2 in the TRUTH worksheet. The following shows that the first reader interprets the first five normal cases in both modalities. x$truthTableStr[,1,1:15,1] #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #&gt; [1,] 1 1 1 1 1 NA NA NA NA NA NA NA NA NA #&gt; [2,] 1 1 1 1 1 NA NA NA NA NA NA NA NA NA #&gt; [,15] #&gt; [1,] NA #&gt; [2,] NA In the following all elements are NA because normal cases correspond to lesionID = 1. x$truthTableStr[,1,1:15,2] #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #&gt; [1,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA #&gt; [2,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA #&gt; [,15] #&gt; [1,] NA #&gt; [2,] NA The following shows that the second reader interprets the next group of five normal cases, indexed 6 through 10, in both modalities. x$truthTableStr[,2,1:15,1] #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #&gt; [1,] NA NA NA NA NA 1 1 1 1 1 NA NA NA NA #&gt; [2,] NA NA NA NA NA 1 1 1 1 1 NA NA NA NA #&gt; [,15] #&gt; [1,] NA #&gt; [2,] NA The following shows that the third reader interprets the next group of five normal cases, indexed 11 through 15, in both modalities. x$truthTableStr[,3,1:15,1] #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #&gt; [1,] NA NA NA NA NA NA NA NA NA NA 1 1 1 1 #&gt; [2,] NA NA NA NA NA NA NA NA NA NA 1 1 1 1 #&gt; [,15] #&gt; [1,] 1 #&gt; [2,] 1 The following shows that the first reader interprets the first group of five abnormal cases, indexed 16 through 20, in both modalities. x$truthTableStr[,1,16:30,2] #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #&gt; [1,] 1 1 1 1 1 NA NA NA NA NA NA NA NA NA #&gt; [2,] 1 1 1 1 1 NA NA NA NA NA NA NA NA NA #&gt; [,15] #&gt; [1,] NA #&gt; [2,] NA In the following all elements are NA because abnormal cases correspond to lesionID = 2. x$truthTableStr[,1,16:30,1] #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #&gt; [1,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA #&gt; [2,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA #&gt; [,15] #&gt; [1,] NA #&gt; [2,] NA FIGURE 8.2: Fig. 2 FP/TP worksheets; LEFT=FP, (b) RIGHT=TP 8.6 The false positive (FP) ratings These are found in the FP or NL worksheet, see Fig. 2, left panel. This worksheet has the ratings of non-diseased cases. The common vertical length is 30 in this example (2 modalities times 3 readers times 5 non-diseased cases per reader). ReaderID: the reader labels: these must be from 1, 4 or 5, as declared in the Truth worksheet. ModalityID: the modality labels: 1 or 2, as declared in the Truth worksheet. CaseID: the labels of non-diseased cases. Each CaseID - ReaderID combination must be consistent with that declared in the Truth worsheet. NL_Rating: the floating point ratings of non-diseased cases. Each row of this worksheet yields a rating corresponding to the values of ReaderID, ModalityID and CaseID for that row. x$NL[,1,1:15,1] #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #&gt; [1,] 1 2 0.1 1 1 -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [2,] 1 2 0.3 1 1 -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [,15] #&gt; [1,] -Inf #&gt; [2,] -Inf x$NL[,2,1:15,1] #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #&gt; [1,] -Inf -Inf -Inf -Inf -Inf 0.2 0.2 1 3 3 -Inf -Inf -Inf -Inf #&gt; [2,] -Inf -Inf -Inf -Inf -Inf 2.0 1.0 1 1 2 -Inf -Inf -Inf -Inf #&gt; [,15] #&gt; [1,] -Inf #&gt; [2,] -Inf x$NL[,3,1:15,1] #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #&gt; [1,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf 0.234 5 2 2 #&gt; [2,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf 3.000 2 2 2 #&gt; [,15] #&gt; [1,] 2.00 #&gt; [2,] 0.33 The first line of the above code shows the ratings, in both modalities, of the first five non-diseased cases with CaseIDs 6,7,8,9,10 (indexed 1, 2, 3, 4, 5 and appearing in the first five columns) interpreted by the first reader (ReaderID 1). The second line shows the ratings, in both modalities, of the next five non-diseased cases with CaseIDs 21,22,23,24,25 (indexed 6, 7, 8, 9, 10and appearing in the next five columns) interpreted by the second reader (ReaderID 4). The third line shows the ratings, in both modalities, of the final five non-diseased cases with CaseIDs 46,47,48,49,50 (indexed 11, 12, 13, 14, 15and appearing in the final five columns) interpreted by the third reader (ReaderID 5). Values such as x$NL[,,16:30,1], which are there for compatibility with FROC data, are all filled with -Inf. 8.7 The true positive (TP) ratings These are found in the TP or LL worksheet, see Fig. 2, right panel. This worksheet has the ratings of diseased cases. The common vertical length is 30 in this example (2 modalities times 3 readers times 5 diseased cases per reader). ReaderID: the reader labels: these must be from 1, 4 or 5, as declared in the Truth worksheet. ModalityID: the modality labels: 1 or 2, as declared in the Truth worksheet. CaseID: the labels of diseased cases. Each CaseID - ReaderID combination must be consistent with that declared in the Truth worsheet. LL_Rating: the floating point ratings of diseased cases. Each row of this worksheet yields a rating corresponding to the values of ReaderID, ModalityID and CaseID for that row. x$LL[,1,1:15,1] #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #&gt; [1,] 5.0 5.5 4.9 4 3.7 -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [2,] 2.3 4.1 5.7 5 6.0 -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [,15] #&gt; [1,] -Inf #&gt; [2,] -Inf x$LL[,2,1:15,1] #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #&gt; [1,] -Inf -Inf -Inf -Inf -Inf 2.70 2.90 5.10 4.90 4.990 -Inf -Inf -Inf -Inf #&gt; [2,] -Inf -Inf -Inf -Inf -Inf 5.22 4.77 5.33 4.99 1.999 -Inf -Inf -Inf -Inf #&gt; [,15] #&gt; [1,] -Inf #&gt; [2,] -Inf x$LL[,3,1:15,1] #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #&gt; [1,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf 5.4 2.7 5.8 4.7 #&gt; [2,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf 5.4 2.7 5.8 4.7 #&gt; [,15] #&gt; [1,] 5 #&gt; [2,] 5 The first line of code shows the ratings, in both modalities, of the first five diseased cases with CaseIDs 16,17,18,19,20 (indexed 1, 2, 3, 4, 5and appearing in the first five columns) interpreted by the first reader (ReaderID 1). The second line shows the ratings, in both modalities, of the next five diseased cases with CaseIDs 36,37,38,39,40 (indexed 6, 7, 8, 9, 10and appearing in the next five columns) interpreted by the second reader (ReaderID 4). The third line shows the ratings, in both modalities, of the final five non-diseased cases with CaseIDs 51,52,53,54,55 (indexed 11, 12, 13, 14, 15and appearing in the final five columns) interpreted by the third reader (ReaderID 5). 8.8 Summary The FROC dataset has far less regularity in structure as compared to an ROC dataset. The length of the first dimension of either x$NL or x$LL list members is the total number of modalities, 2 in the current example. The length of the second dimension of either x$NL or x$LL list members is the total number of readers, 3 in the current example. The length of the third dimension of x$NL is the total number of cases, 8 in the current example. The first three positions account for NL marks on non-diseased cases and the remaining 5 positions account for NL marks on diseased cases. The length of the third dimension of x$LL is the total number of diseased cases, 5 in the current example. The length of the fourth dimension of x$NL is determined by the case (diseased or non-diseased) with the most NL marks, 2 in the current example. The length of the fourth dimension of x$LL is determined by the diseased case with the most lesions, 3 in the current example. 8.9 References "],
["frocSpdataformat.html", "Chapter 9 FROC ROC DATA FORMAT SPLIT PLOT 9.1 Introduction 9.2 The Excel data format 9.3 The Truth worksheet 9.4 The structure of the FROC split plot dataset 9.5 The false positive (FP) ratings 9.6 The true positive (TP) ratings 9.7 Summary 9.8 References", " Chapter 9 FROC ROC DATA FORMAT SPLIT PLOT 9.1 Introduction The purpose of this chapter is to explain the data format of the input Excel file for an FROC split-plot dataset. In a split-plot dataset each reader interprets a sub-set of cases in all modalities. The cases interpreted by different readers have no overlap. It is assumed, for now, that each sub-set of cases has the same numbers of non-diseased and diseased cases. 9.2 The Excel data format The Excel file has three worsheets named Truth, NL or FP and LL or TP. 9.3 The Truth worksheet The Truth worksheet contains 6 columns: CaseID, LesionID, Weight, ReaderID, ModalityID and Paradigm. The first five columns contain as many rows as there are non-diseased cases (9) plus total number of lesions (27) in the dataset (each row with a non-zero LesionID corresponds to a lesion). CaseID: unique integers, one per case, representing the cases in the dataset. LesionID: integers 0, 1, 2, etc., with each 0 representing a non-diseased case, 1 representing the first lesion on a diseased case, 2 representing the second lesion on a diseased case, if present, and so on. The three non-diseased cases interpreted by reader with ReaderID value 0 are labeled 1, 2, 3, while the diseased cases interpreted by this reader are labeled 70, 71, 72, 73 and 74, with LesionID values ranging from 1 to 3. The second reader, with ReaderID value 1, interprets three non-diseased cases labeled 4, 5 and 6, each with LesionID value 0, and five diseased cases labeled 80, 81, 82, 83 and 84, with LesionID values ranging from 1 to 3. The third reader, with ReaderID value 2, interprets three non-diseased cases labeled 7, 8 and 9, each with LesionID value 0 and five diseased cases labeled 90, 91, 92, 93 and 94, with LesionID values ranging from 1 to 3. Weight: floating point value adding upto unity for diseased cases as required for FROC data. ModalityID: a comma-separated listing of modalities, each represented by a unique integer. In the example shown below each cell has the value 0, 1. Each cell has to be text formatted. Paradigm: In the example shown below, the contents are FROC and split-plot. FIGURE 9.1: Two views of Truth worksheet for file frocSp.xlsx 9.4 The structure of the FROC split plot dataset The example shown in Fig. 1 corresponds to Excel file inst/extdata/toyFiles/FROC/frocSp.xlsx in the project directory. frocSp &lt;- system.file(&quot;extdata&quot;, &quot;toyFiles/FROC/frocSp.xlsx&quot;, package = &quot;RJafroc&quot;, mustWork = TRUE) x &lt;- DfReadDataFile(frocSp, newExcelFileFormat = TRUE) str(x) #&gt; List of 12 #&gt; $ NL : num [1:2, 1:3, 1:24, 1:3] 1.02 2.89 -Inf -Inf -Inf ... #&gt; $ LL : num [1:2, 1:3, 1:15, 1:3] 5.28 5.2 -Inf -Inf -Inf ... #&gt; $ lesionVector : int [1:15] 2 1 3 2 1 2 1 3 2 1 ... #&gt; $ lesionID : num [1:15, 1:3] 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ lesionWeight : num [1:15, 1:3] 0.3 1 0.333 0.1 1 ... #&gt; $ dataType : chr &quot;FROC&quot; #&gt; $ modalityID : Named chr [1:2] &quot;0&quot; &quot;1&quot; #&gt; ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;0&quot; &quot;1&quot; #&gt; $ readerID : Named chr [1:3] &quot;0&quot; &quot;1&quot; &quot;2&quot; #&gt; ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;0&quot; &quot;1&quot; &quot;2&quot; #&gt; $ design : chr &quot;SPLIT-PLOT&quot; #&gt; $ normalCases : int [1:9] 1 2 3 4 5 6 7 8 9 #&gt; $ abnormalCases: int [1:15] 70 71 72 73 74 80 81 82 83 84 ... #&gt; $ truthTableStr: num [1:2, 1:3, 1:24, 1:4] 1 1 NA NA NA NA 1 1 NA NA ... Flag newExcelFileFormat must be set to TRUE for split plot data. The dataset object x is a list variable with 12 members. Note that the dataType member is FROC and the design member is SPLIT-PLOT. There are 15 diseased cases in the dataset (the number of 1’s in the LesionID column of the Truth worksheet) and 9 non-diseased cases (the number of 0’s in the LesionID column). The x$lesionVector member is a vector with 15 ones representing the 15 diseased cases in the dataset. The x$lesionID member is a 15 x 3 array labeling the lesions in the dataset. The x$lesionWeight member is a 15 x 3 array. x$lesionVector #&gt; [1] 2 1 3 2 1 2 1 3 2 1 2 1 3 2 1 x$lesionID #&gt; [,1] [,2] [,3] #&gt; [1,] 1 2 -Inf #&gt; [2,] 1 -Inf -Inf #&gt; [3,] 1 2 3 #&gt; [4,] 1 2 -Inf #&gt; [5,] 1 -Inf -Inf #&gt; [6,] 1 2 -Inf #&gt; [7,] 1 -Inf -Inf #&gt; [8,] 1 2 3 #&gt; [9,] 1 2 -Inf #&gt; [10,] 1 -Inf -Inf #&gt; [11,] 1 2 -Inf #&gt; [12,] 1 -Inf -Inf #&gt; [13,] 1 2 3 #&gt; [14,] 1 2 -Inf #&gt; [15,] 1 -Inf -Inf x$lesionWeight #&gt; [,1] [,2] [,3] #&gt; [1,] 0.3000000 0.7000000 -Inf #&gt; [2,] 1.0000000 -Inf -Inf #&gt; [3,] 0.3333333 0.3333333 0.3333333 #&gt; [4,] 0.1000000 0.9000000 -Inf #&gt; [5,] 1.0000000 -Inf -Inf #&gt; [6,] 0.3000000 0.7000000 -Inf #&gt; [7,] 1.0000000 -Inf -Inf #&gt; [8,] 0.3333333 0.3333333 0.3333333 #&gt; [9,] 0.1000000 0.9000000 -Inf #&gt; [10,] 1.0000000 -Inf -Inf #&gt; [11,] 0.3000000 0.7000000 -Inf #&gt; [12,] 1.0000000 -Inf -Inf #&gt; [13,] 0.3333333 0.3333333 0.3333333 #&gt; [14,] 0.1000000 0.9000000 -Inf #&gt; [15,] 1.0000000 -Inf -Inf The x$truthTableStr member is a 2 x 3 x 24 x 4 array, i.e., I x J x K x (maximum number of lesions per case plus 1). The plus 1 is needed to accommodate normal cases with lesionID = 0. Each entry in this array is either 1, meaning the corresponding interpretation exists, or NA, meaning the corresponding interpretation does not exist. For example, x$truthTableStr[1,1,1,1] is 1. This means that an interpretation exists for the first treatment (modalityID = 0), first reader (readerID = 0) and first (normal) case caseID = 1 and lesionID = 0. This example corresponds to row 2 in the TRUTH worksheet. x$truthTableStr[1,1,4,1] is NA, which means an interpretation does not exist for the first treatment, first reader and fourth (normal) case. However, x$truthTableStr[1,2,4,1] is 1, which means an interpretation does exist for the first treatment, second reader and fourth (normal) case. This example corresponds to row 5 in the TRUTH worksheet. Likewise, x$truthTableStr[1,1,10,3] is 1, which means an interpretation does exist for the first treatment, first reader, tenth (abnormal) case and lesionID = 2. This example corresponds to row 12 in the TRUTH worksheet. As an aside, in the FROC paradigm an interpretation need not yield a mark-rating pair. An interpretation means the reader was “exposed to” and had the opportunity to mark the corresponding treatment-reader-case-lesion combination. The reader should confirm that the contents of x$truthTableStr summarizes the structure of the data in the TRUTH worksheet. 9.5 The false positive (FP) ratings These are found in the FP or NL worksheet, see Fig. 2. FIGURE 9.2: NL/FP worksheet, left, and LL/TP worksheet, right, for file frocSp.xlsx This worksheet has the ratings of non-diseased cases. The common vertical length is 30 in this example (2 modalities times 3 readers times 5 non-diseased cases per reader). ReaderID: the reader labels: these must be from 0, 1 or 2, as declared in the Truth worksheet. ModalityID: the modality labels: 0 or 1, as declared in the Truth worksheet. CaseID: the labels of non-diseased cases. Each CaseID, ModalityID, ReaderID combination must be consistent with that declared in the Truth worsheet. FP_Rating: the floating point ratings of non-diseased cases. Each row of this worksheet yields a rating corresponding to the values of ReaderID, ModalityID and CaseID for that row. Each CaseID, ModalityID, ReaderID combination must be consistent with that declared in the Truth worsheet. x$NL[,1,1:9,1] #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] #&gt; [1,] 1.02 2.22 1.90 -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [2,] 2.89 0.84 1.85 -Inf -Inf -Inf -Inf -Inf -Inf x$NL[,2,1:9,1] #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] #&gt; [1,] -Inf -Inf -Inf 2.21 3.10 2.21 -Inf -Inf -Inf #&gt; [2,] -Inf -Inf -Inf 3.22 3.01 1.96 -Inf -Inf -Inf x$NL[,3,1:9,1] #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] #&gt; [1,] -Inf -Inf -Inf -Inf -Inf -Inf 2.14 1.98 1.95 #&gt; [2,] -Inf -Inf -Inf -Inf -Inf -Inf 2.24 4.01 1.65 The first line of the above code shows the ratings, in both modalities, of the first three non-diseased cases with CaseIDs 1,3,3 (indexed 1, 2, 3 and appearing in the first three columns) interpreted by the first reader (ReaderID 0). The second line shows the ratings, in both modalities, of the next three non-diseased cases with CaseIDs 4,5,6 (indexed 4, 5, 6and appearing in the next three columns) interpreted by the second reader (ReaderID 1). The third line shows the ratings, in both modalities, of the final three non-diseased cases with CaseIDs 7,8,9 (indexed 7, 8, 9and appearing in the final three columns) interpreted by the third reader (ReaderID 2). Values such as x$NL[,,16:30,1], which are there for compatibility with FROC data, are all filled with -Inf. 9.6 The true positive (TP) ratings These are found in the TP or LL worksheet, see below. This worksheet has the ratings of diseased cases. The common vertical length is 30 in this example (2 modalities times 3 readers times 5 diseased cases per reader). ReaderID: the reader labels: these must be from 0, 1 or 2, as declared in the Truth worksheet. ModalityID: the modality labels: 0 or 1, as declared in the Truth worksheet. CaseID: the labels of diseased cases. Each CaseID, ModalityID, ReaderID combination must be consistent with that declared in the Truth worsheet. TP_Rating: the floating point ratings of diseased cases. Each row of this worksheet yields a rating corresponding to the values of ReaderID, ModalityID and CaseID for that row. Each CaseID, ModalityID, ReaderID combination must be consistent with that declared in the Truth worsheet. x$LL[,1,1:15,1] #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #&gt; [1,] 5.28 3.01 5.98 5.00 4.26 -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [2,] 5.20 3.27 4.61 5.18 4.72 -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf #&gt; [,15] #&gt; [1,] -Inf #&gt; [2,] -Inf x$LL[,2,1:15,1] #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #&gt; [1,] -Inf -Inf -Inf -Inf -Inf 5.14 3.31 4.92 4.95 5.30 -Inf -Inf -Inf -Inf #&gt; [2,] -Inf -Inf -Inf -Inf -Inf 4.77 3.19 5.20 5.39 5.01 -Inf -Inf -Inf -Inf #&gt; [,15] #&gt; [1,] -Inf #&gt; [2,] -Inf x$LL[,3,1:15,1] #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #&gt; [1,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf 4.66 4.03 5.22 4.94 #&gt; [2,] -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf 4.87 1.94 -Inf -Inf #&gt; [,15] #&gt; [1,] 5.27 #&gt; [2,] 4.78 The first line of code shows the ratings, in both modalities, of the first five diseased cases with CaseIDs 70,71,72,73,74 (indexed 1, 2, 3, 4, 5 and appearing in the first five columns) interpreted by the first reader (ReaderID 0). The second line shows the ratings, in both modalities, of the next five diseased cases with CaseIDs 80,81,82,83,84 (indexed 6, 7, 8, 9, 10 and appearing in the next five columns) interpreted by the second reader (ReaderID 1). The third line shows the ratings, in both modalities, of the final five non-diseased cases with CaseIDs 90,91,92,93,94 (indexed 11, 12, 13, 14, 15 and appearing in the final five columns) interpreted by the third reader (ReaderID 2). 9.7 Summary TBA 9.8 References "],
["QuickStartDBM1.html", "Chapter 10 QUICK START DBM1 10.1 Introduction 10.2 An ROC dataset 10.3 Creating a dataset from a JAFROC format file 10.4 Analyzing the ROC dataset 10.5 Explanation of the output 10.6 ORH significance testing 10.7 References", " Chapter 10 QUICK START DBM1 10.1 Introduction This chapter is intended for those seeking a quick transition from Windows JAFROC to RJafroc. Described first is the structure of an RJafroc dataset followed by how to read a JAFROC format Excel file to create an RJafroc dataset. 10.2 An ROC dataset Dataset dataset03 corresponding to the Franken ROC data (Franken et al. 1992) is predefined. The following code shows the structure of this dataset. str(dataset03) #&gt; List of 12 #&gt; $ NL : num [1:2, 1:4, 1:100, 1] 3 3 4 3 3 3 4 1 1 3 ... #&gt; $ LL : num [1:2, 1:4, 1:67, 1] 5 5 4 4 5 4 4 5 2 2 ... #&gt; $ lesionVector : num [1:67] 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ lesionID : num [1:67, 1] 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ lesionWeight : num [1:67, 1] 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ dataType : chr &quot;ROC&quot; #&gt; $ modalityID : Named chr [1:2] &quot;TREAT1&quot; &quot;TREAT2&quot; #&gt; ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;TREAT1&quot; &quot;TREAT2&quot; #&gt; $ readerID : Named chr [1:4] &quot;READER_1&quot; &quot;READER_2&quot; &quot;READER_3&quot; &quot;READER_4&quot; #&gt; ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;READER_1&quot; &quot;READER_2&quot; &quot;READER_3&quot; &quot;READER_4&quot; #&gt; $ design : chr &quot;CROSSED&quot; #&gt; $ normalCases : int [1:33] 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ abnormalCases: int [1:67] 34 35 36 37 38 39 40 41 42 43 ... #&gt; $ truthTableStr: num [1:2, 1:4, 1:100, 1:2] 1 1 1 1 1 1 1 1 1 1 ... It is a list with 8 members. The false positive ratings are contained in {NL}, an array with dimensions [1:2,1:4,1:100,1]. The first index corresponds to treatments, and since the dataset has 2 treatments, the corresponding dimension is 2. The second index corresponds to readers, and since the dataset has 4 readers, the corresponding dimension is 4. The third index corresponds to the total number of cases. Since the dataset has 100 cases, the corresponding dimension is 100. But, as you can see from the code below, the entries in this array for cases 34 through 100 are -Inf: i.e., all(dataset03$NL[1,1,34:100,1] == -Inf) = TRUE. This is because in the ROC paradigm false positive are not possible on diseased cases. So the actual FP ratings are contained in the first 33 elements of the array. How did I know that there are 33 non-diseased cases? This can be understood in several ways. LL is an array with dimensions [1:2,1:4,1:67,1]. This implies 67 diseased cases, and by subtraction from 100, there must be 33 non-diseased cases. The list member lesionVector is a vector with length 67, implying 33 non-diseased cases. The list members lesionID and lesionWeight are arrays with dimensions [1:67,1] containing ones. Again, these imply 67 diseased cases. The fields lesionVector, lesionID and lesionWeight, while not needed for ROC data, are needed for the FROC paradigm. The dataType list member is the character string \"ROC\", characterizing the ROC dataset. dataset03$dataType #&gt; [1] &quot;ROC&quot; The modalityID list member is a character string with two entries, \"TREAT1\" and \"TREAT2\", corresponding to the two modalities. dataset03$modalityID #&gt; TREAT1 TREAT2 #&gt; &quot;TREAT1&quot; &quot;TREAT2&quot; The readerID list member is a character string with four entries, \"READER_1\", \"READER_2\", \"READER_3\" and \"READER_4\" corresponding to the four readers. dataset03$readerID #&gt; READER_1 READER_2 READER_3 READER_4 #&gt; &quot;READER_1&quot; &quot;READER_2&quot; &quot;READER_3&quot; &quot;READER_4&quot; Here are the actual ratings for cases 1:34. dataset03$NL[1,1,1:33,1] #&gt; [1] 3 1 2 2 2 2 2 4 1 1 4 2 1 2 4 2 1 2 1 2 4 2 3 2 2 2 4 3 2 2 2 5 3 This says that for treatment 1 and reader 1, (non-diseased) case 1 was rated 3, case 2 was rated 1, cases 3-7 were rated 2, case 8 was rated 4, etc. As another example, for treatment 2 and reader 3, the FP ratings are: dataset03$NL[2,3,1:33,1] #&gt; [1] 3 1 2 2 2 2 4 4 2 3 2 2 1 3 2 4 2 3 2 2 2 2 2 4 2 2 1 2 2 2 2 4 2 10.3 Creating a dataset from a JAFROC format file There is a file RocData.xlsx that is part of the package installation. Since it is a system file one must get its name as follows. fileName &lt;- &quot;RocData.xlsx&quot; sysFileName &lt;- system.file(paste0(&quot;extdata/&quot;,fileName), package = &quot;RJafroc&quot;, mustWork = TRUE) Next, one uses DfReadDataFile() as follows, assuming it is a JAFROC format file. ds &lt;- DfReadDataFile(sysFileName, newExcelFileFormat = FALSE) str(ds) #&gt; List of 12 #&gt; $ NL : num [1:2, 1:5, 1:114, 1] 1 3 2 3 2 2 1 2 3 2 ... #&gt; $ LL : num [1:2, 1:5, 1:45, 1] 5 5 5 5 5 5 5 5 5 5 ... #&gt; $ lesionVector : int [1:45] 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ lesionID : num [1:45, 1] 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ lesionWeight : num [1:45, 1] 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ dataType : chr &quot;ROC&quot; #&gt; $ modalityID : Named chr [1:2] &quot;0&quot; &quot;1&quot; #&gt; ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;0&quot; &quot;1&quot; #&gt; $ readerID : Named chr [1:5] &quot;0&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; ... #&gt; ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;0&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; ... #&gt; $ design : chr &quot;CROSSED&quot; #&gt; $ normalCases : int [1:69] 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ abnormalCases: int [1:45] 70 71 72 73 74 75 76 77 78 79 ... #&gt; $ truthTableStr: num [1:2, 1:5, 1:114, 1:2] 1 1 1 1 1 1 1 1 1 1 ... Analysis is illustrated for dataset03, but one could have used the newly created dataset ds. 10.4 Analyzing the ROC dataset This illustrates the StSignificanceTesting() function. The significance testing method is specified as \"DBMH\" and the figure of merit FOM is specified as “Wilcoxon”. ret &lt;- StSignificanceTesting(dataset03, FOM = &quot;Wilcoxon&quot;, method = &quot;DBMH&quot;) print(ret) #&gt; $fomArray #&gt; RdrREADER_1 RdrREADER_2 RdrREADER_3 RdrREADER_4 #&gt; TrtTREAT1 0.8534600 0.8649932 0.8573044 0.8152420 #&gt; TrtTREAT2 0.8496156 0.8435097 0.8401176 0.8143374 #&gt; #&gt; $anovaY #&gt; Source SS DF MS #&gt; 1 Row1_T 0.02356541 1 0.023565410 #&gt; 2 Row2_R 0.20521800 3 0.068406000 #&gt; 3 Row3_C 52.52839868 99 0.530589886 #&gt; 4 Row4_TR 0.01506079 3 0.005020264 #&gt; 5 Row5_TC 6.41004881 99 0.064747968 #&gt; 6 Row6_RC 39.24295381 297 0.132131158 #&gt; 7 Row7_TRC 22.66007764 297 0.076296558 #&gt; 8 Row8_Total 121.08532315 799 NA #&gt; #&gt; $anovaYi #&gt; Source DF TrtTREAT1 TrtTREAT2 #&gt; 1 R 3 0.04926635 0.02415991 #&gt; 2 C 99 0.29396753 0.30137032 #&gt; 3 RC 297 0.10504787 0.10337984 #&gt; #&gt; $varComp #&gt; varR varC varTR varTC varRC varErr #&gt; 1 3.775568e-05 0.05125091 -0.0007127629 -0.002887147 0.0279173 0.07629656 #&gt; #&gt; $FTestStatsRRRC #&gt; fRRRC ndfRRRC ddfRRRC pRRRC #&gt; 1 4.694058 1 3 0.1188379 #&gt; #&gt; $ciDiffTrtRRRC #&gt; TrtDiff Estimate StdErr DF t PrGTt CILower #&gt; 1 TrtTREAT1-TrtTREAT2 0.01085482 0.005010122 3 2.166577 0.1188379 -0.005089627 #&gt; CIUpper #&gt; 1 0.02679926 #&gt; #&gt; $ciAvgRdrEachTrtRRRC #&gt; Treatment Area StdErr DF CILower CIUpper #&gt; 1 TrtTREAT1 0.8477499 0.02440215 70.12179 0.7990828 0.8964170 #&gt; 2 TrtTREAT2 0.8368951 0.02356642 253.64403 0.7904843 0.8833058 #&gt; #&gt; $FTestStatsFRRC #&gt; fFRRC ndfFRRC ddfFRRC pFRRC #&gt; 1 0.363956 1 99 0.547697 #&gt; #&gt; $ciDiffTrtFRRC #&gt; Treatment Estimate StdErr DF t PrGTt CILower #&gt; 1 TrtTREAT1-TrtTREAT2 0.01085482 0.01799277 99 0.6032876 0.547697 -0.02484675 #&gt; CIUpper #&gt; 1 0.04655638 #&gt; #&gt; $ciAvgRdrEachTrtFRRC #&gt; Treatment Area StdErr DF CILower CIUpper #&gt; 1 TrtTREAT1 0.8477499 0.02710939 99 0.7939590 0.9015408 #&gt; 2 TrtTREAT2 0.8368951 0.02744860 99 0.7824311 0.8913591 #&gt; #&gt; $msAnovaEachRdrFRRC #&gt; Source DF RdrREADER_1 RdrREADER_2 RdrREADER_3 RdrREADER_4 #&gt; 1 T 1 0.0007389761 0.02307702 0.01476929 4.091217e-05 #&gt; 2 C 99 0.2038747746 0.22344191 0.21424677 2.854199e-01 #&gt; 3 TC 99 0.0915587344 0.08027926 0.06122898 6.057067e-02 #&gt; #&gt; $ciDiffTrtEachRdrFRRC #&gt; Reader Treatment Estimate StdErr DF t #&gt; 1 RdrREADER_1 TrtTREAT1-TrtTREAT2 0.0038444143 0.04279223 99 0.08983908 #&gt; 2 RdrREADER_2 TrtTREAT1-TrtTREAT2 0.0214834916 0.04006975 99 0.53615233 #&gt; 3 RdrREADER_3 TrtTREAT1-TrtTREAT2 0.0171867933 0.03499399 99 0.49113552 #&gt; 4 RdrREADER_4 TrtTREAT1-TrtTREAT2 0.0009045681 0.03480536 99 0.02598933 #&gt; PrGTt CILower CIUpper #&gt; 1 0.9285966 -0.08106465 0.08875348 #&gt; 2 0.5930559 -0.05802359 0.10099057 #&gt; 3 0.6244176 -0.05224888 0.08662247 #&gt; 4 0.9793182 -0.06815683 0.06996596 #&gt; #&gt; $FTestStatsRRFC #&gt; fRRFC ndfRRFC ddfRRFC pRRFC #&gt; 1 4.694058 1 3 0.1188379 #&gt; #&gt; $ciDiffTrtRRFC #&gt; Treatment Estimate StdErr DF t PrGTt CILower #&gt; 1 TrtTREAT1-TrtTREAT2 0.01085482 0.005010122 3 2.166577 0.1188379 -0.005089627 #&gt; CIUpper #&gt; 1 0.02679926 #&gt; #&gt; $ciAvgRdrEachTrtRRFC #&gt; Treatment Area StdErr DF CILower CIUpper #&gt; 1 TrtTREAT1 0.8477499 0.01109801 3 0.8124311 0.8830687 #&gt; 2 TrtTREAT2 0.8368951 0.00777173 3 0.8121620 0.8616282 10.5 Explanation of the output The function returns a long unwieldy list. Let us consider them one by one. The function UtilOutputReport(), which can generate an Excel file report, making it much easier to visualize the results, is described in another chapter. 10.5.1 FOMs fomArray contains the [1:2,1:4] FOM values. ret$fomArray #&gt; RdrREADER_1 RdrREADER_2 RdrREADER_3 RdrREADER_4 #&gt; TrtTREAT1 0.8534600 0.8649932 0.8573044 0.8152420 #&gt; TrtTREAT2 0.8496156 0.8435097 0.8401176 0.8143374 This shows the 2 x 4 array of FOM values. 10.5.2 Pseudovalue ANOVA table anovaY, where the Y denotes that these are pseudovalue based, is the ANOVA table. ret$anovaY #&gt; Source SS DF MS #&gt; 1 Row1_T 0.02356541 1 0.023565410 #&gt; 2 Row2_R 0.20521800 3 0.068406000 #&gt; 3 Row3_C 52.52839868 99 0.530589886 #&gt; 4 Row4_TR 0.01506079 3 0.005020264 #&gt; 5 Row5_TC 6.41004881 99 0.064747968 #&gt; 6 Row6_RC 39.24295381 297 0.132131158 #&gt; 7 Row7_TRC 22.66007764 297 0.076296558 #&gt; 8 Row8_Total 121.08532315 799 NA 10.5.3 Pseudovalue ANOVA table, each treatment anovaYi is the ANOVA table for individual treatments. ret$anovaYi #&gt; Source DF TrtTREAT1 TrtTREAT2 #&gt; 1 R 3 0.04926635 0.02415991 #&gt; 2 C 99 0.29396753 0.30137032 #&gt; 3 RC 297 0.10504787 0.10337984 The 0 and 1 headers come from the treatment names. 10.5.4 Pseudovalue Variance Components varComp is the variance components (needed for sample size estimation). ret$varComp #&gt; varR varC varTR varTC varRC varErr #&gt; 1 3.775568e-05 0.05125091 -0.0007127629 -0.002887147 0.0279173 0.07629656 10.5.5 Random-reader random-case (RRRC) analysis ret$FTestStatsRRRC$fRRRC is the F-statistic for testing the NH that the treatments have identical FOMs. RRRC means random-reader random-case generalization. ret$FTestStatsRRRC$fRRRC #&gt; [1] 4.694058 10.5.5.1 F-statistic and p-value for RRRC analysis ret$FTestStatsRRRC$ddfRRRC is the denominator degrees of freedom of the F-statistic. ret$FTestStatsRRRC$ddfRRRC #&gt; [1] 3 ret$FTestStatsRRRC$pRRRC is the p-value of the test. ret$FTestStatsRRRC$pRRRC #&gt; [1] 0.1188379 10.5.5.2 Confidence Intervals for RRRC analysis ciDiffTrtRRRC is the 95% confidence interval of reader-averaged differences between treatments. ret$ciDiffTrtRRRC #&gt; TrtDiff Estimate StdErr DF t PrGTt CILower #&gt; 1 TrtTREAT1-TrtTREAT2 0.01085482 0.005010122 3 2.166577 0.1188379 -0.005089627 #&gt; CIUpper #&gt; 1 0.02679926 ciAvgRdrEachTrtRRRC is the 95% confidence interval of reader-averaged FOMs for each treatments. ret$ciAvgRdrEachTrtRRRC #&gt; Treatment Area StdErr DF CILower CIUpper #&gt; 1 TrtTREAT1 0.8477499 0.02440215 70.12179 0.7990828 0.8964170 #&gt; 2 TrtTREAT2 0.8368951 0.02356642 253.64403 0.7904843 0.8833058 10.5.6 Fixed-reader random-case (FRRC) analysis 10.5.6.1 F-statistic and p-value for FRRC analysis ret$FTestStatsFRRC$fFRRC is the F-statistic for fixed-reader random-case analysis. ret$FTestStatsFRRC$fFRRC #&gt; [1] 0.363956 ret$FTestStatsFRRC$ndfFRRC is the numerator degrees of freedom of the F-statistic, always one less than the number of treatments. ret$FTestStatsFRRC$ndfFRRC #&gt; [1] 1 ret$FTestStatsFRRC$ddfFRRC is the denominator degreesof freedom of the F-statistic, for fixed-reader random-case analysis. ret$FTestStatsFRRC$ddfFRRC #&gt; [1] 99 ret$FTestStatsFRRC$pFRRC is the p-value for fixed-reader random-case analysis. ret$FTestStatsFRRC$pFRRC #&gt; [1] 0.547697 10.5.6.2 Confidence Intervals for FRRC analysis ciDiffTrtFRRC is the 95% CI of reader-average differences between treatments for fixed-reader random-case analysis ret$ciDiffTrtFRRC #&gt; Treatment Estimate StdErr DF t PrGTt CILower #&gt; 1 TrtTREAT1-TrtTREAT2 0.01085482 0.01799277 99 0.6032876 0.547697 -0.02484675 #&gt; CIUpper #&gt; 1 0.04655638 ret$ciAvgRdrEachTrtFRRC is the 95% CI of reader-average FOMs of each treatment for fixed-reader random-case analysis ret$ciAvgRdrEachTrtFRRC #&gt; Treatment Area StdErr DF CILower CIUpper #&gt; 1 TrtTREAT1 0.8477499 0.02710939 99 0.7939590 0.9015408 #&gt; 2 TrtTREAT2 0.8368951 0.02744860 99 0.7824311 0.8913591 10.5.6.3 ANOVA for FRRC analysis ret$msAnovaEachRdrFRRC is the mean-squares ANOVA for each reader ret$msAnovaEachRdrFRRC #&gt; Source DF RdrREADER_1 RdrREADER_2 RdrREADER_3 RdrREADER_4 #&gt; 1 T 1 0.0007389761 0.02307702 0.01476929 4.091217e-05 #&gt; 2 C 99 0.2038747746 0.22344191 0.21424677 2.854199e-01 #&gt; 3 TC 99 0.0915587344 0.08027926 0.06122898 6.057067e-02 10.5.6.4 Confidence Intervals for FRRC analysis ciDiffTrtFRRC is the CI for reader-averaged treatment differences, for fixed-reader random-case analysis ret$ciDiffTrtFRRC #&gt; Treatment Estimate StdErr DF t PrGTt CILower #&gt; 1 TrtTREAT1-TrtTREAT2 0.01085482 0.01799277 99 0.6032876 0.547697 -0.02484675 #&gt; CIUpper #&gt; 1 0.04655638 10.5.7 Random-reader fixed-case (RRFC) analysis 10.5.7.1 F-statistic and p-value for RRFC analysis ret$FTestStatsRRFC$fRRFC is the F-statistic for for random-reader fixed-case analysis ret$FTestStatsRRFC$fRRFC #&gt; [1] 4.694058 ret$FTestStatsRRFC$ddfRRFC is the ddf for for random-reader fixed-case analysis ret$FTestStatsRRFC$ddfRRFC #&gt; [1] 3 ret$FTestStatsRRFC$pRRFC is the p-value for for random-reader fixed-case analysis ret$FTestStatsRRFC$pRRFC #&gt; [1] 0.1188379 10.5.7.2 Confidence Intervals for RRFC analysis ciDiffTrtRRFC is the CI for reader-averaged inter-treatment FOM differences for random-reader fixed-case analysis ret$ciDiffTrtRRFC #&gt; Treatment Estimate StdErr DF t PrGTt CILower #&gt; 1 TrtTREAT1-TrtTREAT2 0.01085482 0.005010122 3 2.166577 0.1188379 -0.005089627 #&gt; CIUpper #&gt; 1 0.02679926 ciAvgRdrEachTrtRRFC is the CI for treatment FOMs for each reader for random-reader fixed-case analysis ret$ciAvgRdrEachTrtRRFC #&gt; Treatment Area StdErr DF CILower CIUpper #&gt; 1 TrtTREAT1 0.8477499 0.01109801 3 0.8124311 0.8830687 #&gt; 2 TrtTREAT2 0.8368951 0.00777173 3 0.8121620 0.8616282 10.6 ORH significance testing Simply change method = \"DBMH\" to method = \"ORH\". ret &lt;- StSignificanceTesting(dataset03, FOM = &quot;Wilcoxon&quot;, method = &quot;ORH&quot;) str(ret) #&gt; List of 14 #&gt; $ fomArray : num [1:2, 1:4] 0.853 0.85 0.865 0.844 0.857 ... #&gt; ..- attr(*, &quot;dimnames&quot;)=List of 2 #&gt; .. ..$ : chr [1:2] &quot;TrtTREAT1&quot; &quot;TrtTREAT2&quot; #&gt; .. ..$ : chr [1:4] &quot;RdrREADER_1&quot; &quot;RdrREADER_2&quot; &quot;RdrREADER_3&quot; &quot;RdrREADER_4&quot; #&gt; $ meanSquares :&#39;data.frame&#39;: 1 obs. of 3 variables: #&gt; ..$ msT : num 0.000236 #&gt; ..$ msR : num 0.000684 #&gt; ..$ msTR: num 5.02e-05 #&gt; $ varComp :&#39;data.frame&#39;: 1 obs. of 6 variables: #&gt; ..$ varR : num 2.33e-05 #&gt; ..$ varTR: num -0.000684 #&gt; ..$ cov1 : num 0.000792 #&gt; ..$ cov2 : num 0.000484 #&gt; ..$ cov3 : num 0.000513 #&gt; ..$ var : num 0.00153 #&gt; $ FTestStatsRRRC :&#39;data.frame&#39;: 1 obs. of 4 variables: #&gt; ..$ fRRRC : num 4.69 #&gt; ..$ ndfRRRC: num 1 #&gt; ..$ ddfRRRC: num 3 #&gt; ..$ pRRRC : num 0.119 #&gt; $ ciDiffTrtRRRC :&#39;data.frame&#39;: 1 obs. of 8 variables: #&gt; ..$ Treatment: chr &quot;TrtTREAT1-TrtTREAT2&quot; #&gt; ..$ Estimate : num 0.0109 #&gt; ..$ StdErr : num 0.00501 #&gt; ..$ DF : num 3 #&gt; ..$ t : num 2.17 #&gt; ..$ PrGTt : num 0.119 #&gt; ..$ CILower : num -0.00509 #&gt; ..$ CIUpper : num 0.0268 #&gt; $ ciAvgRdrEachTrtRRRC :&#39;data.frame&#39;: 2 obs. of 6 variables: #&gt; ..$ Treatment: Factor w/ 2 levels &quot;TrtTREAT1&quot;,&quot;TrtTREAT2&quot;: 1 2 #&gt; ..$ Area : num [1:2] 0.848 0.837 #&gt; ..$ StdErr : num [1:2] 0.0244 0.0236 #&gt; ..$ DF : num [1:2] 70.1 253.6 #&gt; ..$ CILower : num [1:2] 0.799 0.79 #&gt; ..$ CIUpper : num [1:2] 0.896 0.883 #&gt; $ FTestStatsFRRC :&#39;data.frame&#39;: 1 obs. of 4 variables: #&gt; ..$ fFRRC : num 0.364 #&gt; ..$ ndfFRRC: num 1 #&gt; ..$ ddfFRRC: num Inf #&gt; ..$ pFRRC : num 0.546 #&gt; $ ciDiffTrtFRRC :&#39;data.frame&#39;: 1 obs. of 8 variables: #&gt; ..$ Treatment: chr &quot;TrtTREAT1-TrtTREAT2&quot; #&gt; ..$ Estimate : num 0.0109 #&gt; ..$ StdErr : num 0.018 #&gt; ..$ DF : num Inf #&gt; ..$ t : num 0.603 #&gt; ..$ PrGTt : num 0.546 #&gt; ..$ CILower : num -0.0244 #&gt; ..$ CIUpper : num 0.0461 #&gt; $ ciAvgRdrEachTrtFRRC :&#39;data.frame&#39;: 2 obs. of 6 variables: #&gt; ..$ Treatment: Factor w/ 2 levels &quot;TrtTREAT1&quot;,&quot;TrtTREAT2&quot;: 1 2 #&gt; ..$ Area : num [1:2] 0.848 0.837 #&gt; ..$ StdErr : num [1:2] 0.0271 0.0274 #&gt; ..$ DF : num [1:2] Inf Inf #&gt; ..$ CILower : num [1:2] 0.795 0.783 #&gt; ..$ CIUpper : num [1:2] 0.901 0.891 #&gt; $ ciDiffTrtEachRdrFRRC:&#39;data.frame&#39;: 4 obs. of 9 variables: #&gt; ..$ Reader : Factor w/ 4 levels &quot;RdrREADER_1&quot;,..: 1 2 3 4 #&gt; ..$ Treatment: Factor w/ 1 level &quot;TrtTREAT1-TrtTREAT2&quot;: 1 1 1 1 #&gt; ..$ Estimate : num [1:4] 0.003844 0.021483 0.017187 0.000905 #&gt; ..$ StdErr : num [1:4] 0.0428 0.0401 0.035 0.0348 #&gt; ..$ DF : num [1:4] Inf Inf Inf Inf #&gt; ..$ t : num [1:4] 0.0898 0.5362 0.4911 0.026 #&gt; ..$ PrGTt : num [1:4] 0.928 0.592 0.623 0.979 #&gt; ..$ CILower : num [1:4] -0.08 -0.0571 -0.0514 -0.0673 #&gt; ..$ CIUpper : num [1:4] 0.0877 0.1 0.0858 0.0691 #&gt; $ varCovEachRdr :&#39;data.frame&#39;: 4 obs. of 3 variables: #&gt; ..$ Reader: Factor w/ 4 levels &quot;RdrREADER_1&quot;,..: 1 2 3 4 #&gt; ..$ Var : num [1:4] 0.00148 0.00152 0.00138 0.00173 #&gt; ..$ Cov1 : num [1:4] 0.000562 0.000716 0.000765 0.001124 #&gt; $ FTestStatsRRFC :&#39;data.frame&#39;: 1 obs. of 4 variables: #&gt; ..$ fRRFC : num 4.69 #&gt; ..$ ndfRRFC: num 1 #&gt; ..$ ddfRRFC: num 3 #&gt; ..$ pRRFC : num 0.119 #&gt; $ ciDiffTrtRRFC :&#39;data.frame&#39;: 1 obs. of 8 variables: #&gt; ..$ Treatment: chr &quot;TrtTREAT1-TrtTREAT2&quot; #&gt; ..$ Estimate : num 0.0109 #&gt; ..$ StdErr : num 0.00501 #&gt; ..$ DF : num 3 #&gt; ..$ t : num 2.17 #&gt; ..$ PrGTt : num 0.119 #&gt; ..$ CILower : num -0.00509 #&gt; ..$ CIUpper : num 0.0268 #&gt; $ ciAvgRdrEachTrtRRFC :&#39;data.frame&#39;: 2 obs. of 6 variables: #&gt; ..$ Treatment: Factor w/ 2 levels &quot;TrtTREAT1&quot;,&quot;TrtTREAT2&quot;: 1 2 #&gt; ..$ Area : num [1:2] 0.848 0.837 #&gt; ..$ StdErr : num [1:2] 0.0111 0.00777 #&gt; ..$ DF : num [1:2] 3 3 #&gt; ..$ CILower : num [1:2] 0.812 0.812 #&gt; ..$ CIUpper : num [1:2] 0.883 0.862 10.7 References REFERENCES "],
["QuickStartDBM2.html", "Chapter 11 QUICK START DBM2 11.1 Introduction 11.2 Generating the Excel output file 11.3 ORH significance testing", " Chapter 11 QUICK START DBM2 11.1 Introduction This chapter illustrates significance testing using the DBMH method. But, instead of the unwieldy output in QuickStartDBMH.html, it generates an Excel output file containing the following worksheets: Summary FOMs RRRC FRRC RRFC ANOVA 11.2 Generating the Excel output file This illustrates the UtilOutputReport() function. The significance testing method is “DBMH”, the default, and the figure of merit FOM is “Wilcoxon”. Note ReportFileExt = “xlsx”` telling the function to create an Excel output file. The Excel output is created in a temporary file. ret &lt;- UtilOutputReport(dataset03, FOM = &quot;Wilcoxon&quot;, overWrite = TRUE, ReportFileExt = &quot;xlsx&quot;) #&gt; #&gt; Output file name is: /var/folders/d1/mx6dcbzx3v39r260458z2b200000gn/T//Rtmp7b5rzM/RJafrocUtilOutputReport64f44fc4add1.xlsx 11.3 ORH significance testing Simply change method = \"DBMH\" (the default) to method = \"ORH\". ret &lt;- UtilOutputReport(dataset03, FOM = &quot;Wilcoxon&quot;, method = &quot;ORH&quot;, overWrite = TRUE, ReportFileExt = &quot;xlsx&quot;) #&gt; #&gt; Output file name is: /var/folders/d1/mx6dcbzx3v39r260458z2b200000gn/T//Rtmp7b5rzM/RJafrocUtilOutputReport64f431c542bf.xlsx "],
["SSFDistr.html", "Chapter 12 BACKGROUND ON THE F-DISTRIBUTION 12.1 Introduction 12.2 Effect of ncp for ndf = 2 and ddf = 10 12.3 Comments 12.4 Effect of ncp for ndf = 2 and ddf = 100 12.5 Comments 12.6 Effect of ncp for ndf = 1, ddf = 100 12.7 Comments 12.8 Summary 12.9 References", " Chapter 12 BACKGROUND ON THE F-DISTRIBUTION 12.1 Introduction Since it plays an important role in sample size estimation, it is helpful to examine the behavior of the F-distribution. In the following ndf = numerator degrees of freedom, ddf = denominator degrees of freedom and ncp = non-centrality parameter (i.e., the \\(\\Delta\\) appearing in Eqn. (11.6) of (Chakraborty 2017)). The use of three R functions is demonstrated. qf(p,ndf,ddf) is the quantile function of the F-distribution for specified values of p, ndf and ddf, i.e., the value x such that fraction p of the area under the F-distribution lies to the right of x. Since ncp is not included as a parameter, the default value, i.e., zero, is used. This is called the central F-distribution. df(x,ndf,ddf,ncp) is the probability density function (pdf) of the F-distribution, as a function of x, for specified values of ndf, ddf and ncp. pf(x,ndf,ddf,ncp) is the probability (or cumulative) distribution function of the F-distribution for specified values of ndf, ddf and ncp. 12.2 Effect of ncp for ndf = 2 and ddf = 10 Four values of ncp are considered (0, 2, 5, 10) for ddf = 10. fCrit is the critical value of the F distribution, i.e., that value such that fraction \\(\\alpha\\) of the area is to the right of the critical value, i.e., fCrit is identical to: \\[\\begin{equation*} F_{1-\\alpha ,ndf,ddf} \\end{equation*}\\] ndf &lt;- 2;ddf &lt;- 10;ncp &lt;- c(0,2,5,10) alpha &lt;- 0.05 fCrit &lt;- qf(1-alpha, ndf,ddf) x &lt;- seq(1, 20, 0.1) myLabel &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;) myLabelIndx &lt;- 1 pFgtFCrit &lt;- NULL for (i in 1:length(ncp)) { y &lt;- df(x,ndf,ddf,ncp=ncp[i]) pFgtFCrit &lt;- c(pFgtFCrit, 1-pf(fCrit, ndf, ddf, ncp = ncp[i])) } for (i in 1:length(ncp)) { y &lt;- df(x,ndf,ddf,ncp=ncp[i]) curveData &lt;- data.frame(x = x, pdf = y) curvePlot &lt;- ggplot(data = curveData, mapping = aes(x = x, y = pdf)) + geom_line() + ggtitle(myLabel[myLabelIndx]);myLabelIndx &lt;- myLabelIndx + 1 print(curvePlot) } fCrit_2_10 &lt;- fCrit # convention fCrit_ndf_ddf ndf ddf fCrit ncp pFgtFCrit A 2 10 4.102821 0 0.0500000 B 2 10 4.102821 2 0.1775840 C 2 10 4.102821 5 0.3876841 D 2 10 4.102821 10 0.6769776 12.3 Comments 12.3.1 Fig. A This corresponds to ncp = 0, i.e., the central F-distribution. The integral under this distribution is unity (this is also true for all plots in this chapter). The critical value, fCrit in the above code block, is the value of x such that the probability of exceeding x is \\(\\alpha\\). The corresponding parameter alpha is defined above as 0.05. In the current example fCrit = 4.102821. Notice the use of the quantile function qf() to determine this value, and the default value of ncp, namely zero, is used; specifically, one does not pass a 4th argument to qf(). The decision rule for rejecting the NH uses the NH distribution of the F-statistic, i.e., reject the NH if F &gt;= fCrit. As expected, prob &gt; fCrit = 0.05 because this is how fCrit was defined. 12.3.2 Fig. B This corresponds to ncp = 2, ndf = 2 and ddf = 10. The distribution is slightly shifted to the right as compared to Fig. A, thereby making it more likely that the observed value of the F-statistic will exceed the critical value determined for the NH distribution. In fact, prob &gt; fCrit = 0.177584, i.e., the statistical power (compare this to Fig. A where prob &gt; fCrit was 0.05). 12.3.3 Fig. C This corresponds to ncp = 5, ndf = 2 and ddf = 10. Now prob &gt; fCrit = 0.3876841. Power has increased compared to Fig. B. 12.3.4 Fig. D This corresponds to ncp = 10, ndf = 2 and ddf = 10. Now prob &gt; fCrit is 0.6769776. Power has increased compared to Fig. C. The effect of the shift is most obvious in Fig. C and Fig. D. Considering a vertical line at x = 4.102821, fraction 0.6769776 of the probability distribution in Fig. D lies to the right of this line Therefore the NH is likely to be rejected with probability 0.6769776. 12.3.5 Summary The larger that non-centrality parameter, the greater the shift to the right of the F-distribution, and the greater the statistical power. 12.4 Effect of ncp for ndf = 2 and ddf = 100 ndf ddf fCrit ncp pFgtFCrit A 2 10 4.102821 0 0.0500000 B 2 10 4.102821 2 0.1775840 C 2 10 4.102821 5 0.3876841 D 2 10 4.102821 10 0.6769776 E 2 100 3.087296 0 0.0500000 F 2 100 3.087296 2 0.2199264 G 2 100 3.087296 5 0.4910802 H 2 100 3.087296 10 0.8029764 12.5 Comments All comparisons in this sections are at the same values of ncp defined above. And between ddf = 100 and ddf = 10. 12.5.1 Fig. E This corresponds to ncp = 0, ndf = 2 and ddf = 100. The critical value is fCrit_2_100 = 3.0872959. Notice the decrease compared to the previous value for ncp = 0, i.e., 4.102821, for ddf = 10. One expects that increasing ddf will make it more likely that the NH will be rejected, and this is confirmed below. All else equal, statistical power increases with increasing ddf. 12.5.2 Fig. F This corresponds to ncp = 2, ndf = 2 and ddf = 100. The probability of exceeding the critical value is prob &gt; fCrit_2_100 = 0.2199264, greater than the previous value, i.e., 0.177584 for ddf = 10. 12.5.3 Fig. G This corresponds to ncp = 5, ndf = 2 and ddf = 100. The probability of exceeding the critical value is prob &gt; fCrit_2_100 = 0.4910802. This is greater than the previous value, i.e., 0.3876841 for ddf = 10. 12.5.4 Fig. H This corresponds to ncp = 10, ndf = 2 and ddf = 100. The probability of exceeding the critical value is prob &gt; fCrit_2_100 is 0.8029764. This is greater than the previous value, i.e., 0.6769776 for ddf = 10. 12.6 Effect of ncp for ndf = 1, ddf = 100 ndf ddf fCrit ncp pFgtFCrit A 2 10 4.102821 0 0.0500000 B 2 10 4.102821 2 0.1775840 C 2 10 4.102821 5 0.3876841 D 2 10 4.102821 10 0.6769776 E 2 100 3.087296 0 0.0500000 F 2 100 3.087296 2 0.2199264 G 2 100 3.087296 5 0.4910802 H 2 100 3.087296 10 0.8029764 I 1 100 3.936143 0 0.0500000 J 1 100 3.936143 2 0.2883607 K 1 100 3.936143 5 0.6004962 L 1 100 3.936143 10 0.8793619 12.7 Comments All comparisons in this sections are at the same values of ncp defined above and at ddf = 100. And between ndf = 1 and ndf = 2. 12.7.1 Fig. I This corresponds to ncp = 0, ndf = 1 and ddf = 100. The critical value is fCrit_1_100 = 3.936143. Notice the increase in the critical value as compared to the corresponding value for ndf = 2, i.e., 3.0872959. One might expect power to decrease, but see below. 12.7.2 Fig. J This corresponds to ncp = 2, ndf = 1 and ddf = 100. Now prob &gt; fCrit_1_100 = 0.2883607, larger than the previous value 0.2199264. The power has actually increased. 12.7.3 Fig. K This corresponds to ncp = 5, ndf = 1 and ddf = 100`’, Now prob &gt; fCrit_1_100 = 0.6004962, larger than the previous value 0.4910802. Again, the power has actually increased. 12.7.4 Fig. L This corresponds to ncp = 10, ndf = 1 and ddf = 100 Now prob &gt; fCrit_1_100 is 0.8793619, larger than the previous value 0.8029764. The power has actually increased. 12.8 Summary Power increases with increasing ddf and ncp. The effect of increasing ncp is quite dramatic. This is because power depends on the square of ncp. Decreasing ndf also increases power. At first glance this may seem counterintuitive, as fCrit has gone up, but is explained by the differing shapes of the two distributions: the pdf is broader for ndf = 1 as compared to ndf = 2 (compare Fig. L to H). 12.9 References REFERENCES "],
["SSRocFirstPrinciples.html", "Chapter 13 ROC-DBMH sample size from first principles 13.1 Introduction 13.2 Sample size estimation using the DBMH method 13.3 Summary 13.4 References", " Chapter 13 ROC-DBMH sample size from first principles 13.1 Introduction The starting point is a pilot study. The variability in this dataset (specifically the variance components, subsequently converted to mean squares), obtained by running the significance testing function StSignificanceTesting(), is used to extrapolate to the necessary numbers of readers and cases, in the pivotal study, to achieve the desired power. In this example, the observed effect size in the pilot study is used as the anticipated effect size for the pivotal study – this is generally not a good idea as discussed in Chapter 11 under “Cautionary notes”. Shown below, and the reader should confirm, is a first principles implementation of the relevant formulae in Chapter 11. 13.2 Sample size estimation using the DBMH method The Van Dyke dataset in file VanDyke.lrc, in \"MRMC\" format, is regarded as a pilot study. The command rocData &lt;- DfReadDataFile(fileName, format = \"MRMC\") reads the data and saves it to a dataset object rocData. For more on data formats click here. The next line uses the function StSignificanceTesting() to apply method = \"DBMH\" analysis, the default, using the FOM = \"Wilcoxon\" figure of merit. The next line extracts the variance components varYTR, varYTC and varYEps (the Y’s denote pseudovalue based values). The next line extracts the effect size. alpha &lt;- 0.05 rocData &lt;- dataset02 ##&quot;VanDyke.lrc&quot; #fileName &lt;- dataset03 ## &quot;Franken1.lrc&quot; retDbm &lt;- StSignificanceTesting(dataset = rocData, FOM = &quot;Wilcoxon&quot;, method = &quot;DBMH&quot;) varYTR &lt;- retDbm$varComp$varTR;varYTC &lt;- retDbm$varComp$varTC;varYEps &lt;- retDbm$varComp$varErr effectSize &lt;- retDbm$ciDiffTrtRRRC$Estimate The observed effect size is effectSize = -0.0438003, which, in this example, is used as the anticipated effect size, generally not a good idea. See Chapter 11 for nuances regarding the choice of this all important value. The following code snippet reveals the names and array indexing of the pseudovalue variance components. retDbm$varComp #&gt; varR varC varTR varTC varRC varErr #&gt; 1 0.001534999 0.02724923 0.0002004025 0.0119753 0.01226473 0.0399716 For example, the treatment-reader pseudovalue variance component is the third element of retDbm$varComp. 13.2.1 Random reader random case (RRRC) This illustrates random reader random case sample size estimation. Assumed are 10 readers and 163 cases in the pivotal study. The non-centrality parameter is defined by: \\[\\begin{equation*} \\Delta =\\frac{JK\\sigma _{Y;\\tau }^{2}}{\\left( \\sigma _{Y;\\varepsilon }^{2}+\\sigma _{Y;\\tau RC}^{2} \\right)+K\\sigma _{Y;\\tau R}^{2}+J\\max \\left( \\sigma _{Y;\\tau C}^{2},0 \\right)} \\end{equation*}\\] The sampling distribution of the F-statistic under the AH is: \\[\\begin{equation*} {F_{\\left. AH \\right|R}}\\equiv \\frac{MST}{MSTC}\\sim{F_{I-1,\\left( I-1 \\right)\\left(K-1 \\right),\\Delta}} \\end{equation*}\\] Also, \\(\\sigma _{Y;\\tau }^{2}={d^{2}}/2\\), where d is the observed effect size, i.e., effectSize. The formulae for calculating the mean-squares are in (Hillis and Berbaum 2004), implemented in UtilMeanSquares(). #RRRC J &lt;- 10;K &lt;- 163 ncp &lt;- (0.5*J*K*(effectSize)^2)/(K*varYTR+max(J*varYTC,0)+varYEps) MS &lt;- UtilMeanSquares(rocData, FOM = &quot;Wilcoxon&quot;, method = &quot;DBMH&quot;) ddf &lt;- (MS$msTR+max(MS$msTC-MS$msTRC,0))^2/(MS$msTR^2)*(J-1) FCrit &lt;- qf(1 - alpha, 1, ddf) Power1 &lt;- 1-pf(FCrit, 1, ddf, ncp = ncp) The next line calculates the non centrality parameter, ncp = 8.1269825. Note that effectSize enters as the square. The UtilMeanSquares() function returns the mean-squares as a list (ignore the last two rows of output for now). str(MS) #&gt; List of 9 #&gt; $ msT : num 0.547 #&gt; $ msR : num 0.437 #&gt; $ msC : num 0.397 #&gt; $ msTR : num 0.0628 #&gt; $ msTC : num 0.0521 #&gt; $ msRC : num 0.0645 #&gt; $ msTRC : num 0.04 #&gt; $ msCSingleT: num [1:2] 0.336 0.16 #&gt; $ msCSingleR: num [1:5] 0.1222 0.2127 0.1365 0.0173 0.1661 The next line calculates ddf = 12.822129. The remaining lines calculate the critical value of the F-distribution, FCrit = 4.680382 and statistical power = 0.7494133, which by design is close to 80%, i.e., the numbers of readers and cases were chosen to achieve this value. 13.2.2 Fixed reader random case (FRRC) This code illustrates fixed reader random case sample size estimation. Assumed are 10 readers and 133 cases in the pivotal study. The formulae are: \\[\\begin{equation*} \\Delta =\\frac{JK\\sigma _{Y;\\tau }^{2}}{\\sigma _{Y;\\varepsilon }^{2}+\\sigma _{Y;\\tau RC}^{2}+J\\sigma _{Y;\\tau C}^{2}} \\end{equation*}\\] The sampling distribution of the F-statistic under the AH is: \\[\\begin{equation*} {F_{\\left. AH \\right|R}}\\equiv \\frac{MST}{MSTC}\\sim{F_{I-1,\\left( I-1 \\right)\\left( K-1 \\right),\\Delta }} \\end{equation*}\\] #FRRC ncp &lt;- (0.5*J*K*(effectSize)^2)/(max(J*varYTC,0)+varYEps) ddf &lt;- (K-1) FCrit &lt;- qf(1 - alpha, 1, ddf) Power2 &lt;- 1-pf(FCrit, 1, ddf, ncp = ncp) This time non centrality parameter, ncp = 7.9873835, ddf = 132, FCrit = 3.912875 and statistical power = 0.8011167. Again, be design, this is close to 80%. Note that when readers are regarded as a fixed effect, fewer cases are needed to achieve the desired power. Freezing out a source of variability results in a more stable measurement and hence fewer cases are needed to achieve the desired power. 13.2.3 Random reader fixed case (RRFC) This code illustrates random reader random case sample size estimation. Assumed are 10 readers and 53 cases in the pivotal study. The formulae are: \\[\\begin{equation*} \\Delta =\\frac{JK\\sigma _{Y;\\tau }^{2}}{\\sigma _{Y;\\varepsilon }^{2}+\\sigma _{Y;\\tau RC}^{2}+K\\sigma _{Y;\\tau R}^{2}} \\end{equation*}\\] The sampling distribution of the F-statistic under the AH is: \\[\\begin{equation*} {F_{\\left. AH \\right|C}}\\equiv \\frac{MST}{MSTR}\\sim{F_{I-1,\\left( I-1 \\right)\\left( J-1 \\right),\\Delta }} \\end{equation*}\\] #RRFC ncp &lt;- (0.5*J*K*(effectSize)^2)/(K*varYTR+varYEps) ddf &lt;- (J-1) FCrit &lt;- qf(1 - alpha, 1, ddf) Power3 &lt;- 1-pf(FCrit, 1, ddf, ncp = ncp) This time non centrality parameter, ncp = 10.0487164, ddf = 9, FCrit = 5.117355 and statistical power = 0.8049666. Again, be design, this is close to 80%. 13.3 Summary For 10 readers, the numbers of cases needed for 80% power is largest (163) for RRRC, intermediate (133) for FRRC and least for RRFC (53). For all three analyses, the expectation of 80% power is met. 13.4 References REFERENCES "],
["SSRocDBMHRJafroc.html", "Chapter 14 ROC-DBMH sample size using RJafroc 14.1 Introduction 14.2 Illustration of SsPowerGivenJK() using method = \"DBMH\" 14.3 Illustration of SsPowerTable() using method = \"DBMH\" 14.4 Illustration of SsSampleSizeKGivenJ() using method = \"DBMH\"", " Chapter 14 ROC-DBMH sample size using RJafroc 14.1 Introduction This illustrates the RJafroc implementation of sample-size estimation. Default \\(\\alpha\\) is 0.05 and default power (1-\\(\\beta\\)) is 0.8. Three functions are provided. Each of these functions can be used with method \"DBMH\" (illustrated here, the default) or method = \"ORH\" (next chapter). Illustrated below, for the most part, is the random-reader random-case (RRRC) option, i.e., option = \"RRRC\". The last two examples illustrate fixed-reader random-case (FRRC) option = \"FRRC\" and random-reader fixed-case (RRFC) option = \"RRFC\" options. SsPowerGivenJK() Statistical power for specified numbers of readers and cases in an ROC study. SsPowerTable() Generate a power table, i.e., combinations of numbers of readers and cases yielding the desired power. SsSampleSizeKGivenJ Number of cases, for specified number of readers, to achieve desired power. 14.2 Illustration of SsPowerGivenJK() using method = \"DBMH\" The selected dataset corresponds to the Van Dyke data. power &lt;- SsPowerGivenJK(dataset02, FOM = &quot;Wilcoxon&quot;, J = 6, K = 112, option = &quot;RRRC&quot;) The returned value is a list containing the expected power power, the non-centrality parameter ncp, the denominator degrees of freedom ddf and the F-statistic f. The numerator degrees of freedom ndf is always I - 1, i.e., unity for this dataset. str(power) #&gt; &#39;data.frame&#39;: 1 obs. of 4 variables: #&gt; $ powerRRRC: num 0.556 #&gt; $ ncpRRRC : num 4.8 #&gt; $ ddfHRRRC : num 23.1 #&gt; $ fRRRC : num 4.28 Expected power is 0.5555789. 14.3 Illustration of SsPowerTable() using method = \"DBMH\" powTab &lt;- SsPowerTable(dataset02, FOM = &quot;Wilcoxon&quot;, method = &quot;DBMH&quot;, option = &quot;RRRC&quot;) Now show the power table powTab. Note that the last column is always close to 0.8, the desired power. The 2nd and 3rd columns show the number of readers and number of cases to achieve the desired power. powTab #&gt; numReaders numCases power #&gt; 1 3 &gt;2000 &lt;NA&gt; #&gt; 2 3 &gt;2000 &lt;NA&gt; #&gt; 3 4 1089 0.8 #&gt; 4 4 1089 0.8 #&gt; 5 5 344 0.801 #&gt; 6 5 344 0.801 #&gt; 7 6 251 0.801 #&gt; 8 6 251 0.801 #&gt; 9 7 211 0.801 #&gt; 10 7 211 0.801 #&gt; 11 8 188 0.801 #&gt; 12 8 188 0.801 #&gt; 13 9 173 0.801 #&gt; 14 9 173 0.801 #&gt; 15 10 163 0.802 #&gt; 16 10 163 0.802 #&gt; 17 11 155 0.801 #&gt; 18 11 155 0.801 #&gt; 19 12 149 0.802 #&gt; 20 12 149 0.802 #&gt; 21 13 144 0.801 #&gt; 22 13 144 0.801 #&gt; 23 14 140 0.802 #&gt; 24 14 140 0.802 #&gt; 25 15 137 0.802 #&gt; 26 15 137 0.802 #&gt; 27 16 134 0.802 #&gt; 28 16 134 0.802 #&gt; 29 17 131 0.801 #&gt; 30 17 131 0.801 #&gt; 31 18 129 0.801 #&gt; 32 18 129 0.801 #&gt; 33 19 127 0.801 #&gt; 34 19 127 0.801 #&gt; 35 20 126 0.802 #&gt; 36 20 126 0.802 #&gt; 37 21 124 0.801 #&gt; 38 21 124 0.801 #&gt; 39 22 123 0.802 #&gt; 40 22 123 0.802 #&gt; 41 23 122 0.802 #&gt; 42 23 122 0.802 #&gt; 43 24 121 0.803 #&gt; 44 24 121 0.803 #&gt; 45 25 120 0.802 #&gt; 46 25 120 0.802 #&gt; 47 26 119 0.802 #&gt; 48 26 119 0.802 #&gt; 49 27 118 0.802 #&gt; 50 27 118 0.802 #&gt; 51 28 117 0.801 #&gt; 52 28 117 0.801 #&gt; 53 29 117 0.803 #&gt; 54 29 117 0.803 #&gt; 55 30 116 0.802 #&gt; 56 30 116 0.802 #&gt; 57 31 115 0.801 #&gt; 58 31 115 0.801 #&gt; 59 32 115 0.803 #&gt; 60 32 115 0.803 #&gt; 61 33 114 0.801 #&gt; 62 33 114 0.801 #&gt; 63 34 114 0.803 #&gt; 64 34 114 0.803 #&gt; 65 35 113 0.801 #&gt; 66 35 113 0.801 #&gt; 67 36 113 0.802 #&gt; 68 36 113 0.802 #&gt; 69 37 112 0.8 #&gt; 70 37 112 0.8 #&gt; 71 38 112 0.802 #&gt; 72 38 112 0.802 #&gt; 73 39 112 0.803 #&gt; 74 39 112 0.803 #&gt; 75 40 111 0.801 #&gt; 76 40 111 0.801 #&gt; 77 41 111 0.802 #&gt; 78 41 111 0.802 #&gt; 79 42 111 0.803 #&gt; 80 42 111 0.803 #&gt; 81 43 110 0.801 #&gt; 82 43 110 0.801 #&gt; 83 44 110 0.802 #&gt; 84 44 110 0.802 #&gt; 85 45 110 0.802 #&gt; 86 45 110 0.802 #&gt; 87 46 110 0.803 #&gt; 88 46 110 0.803 #&gt; 89 47 109 0.801 #&gt; 90 47 109 0.801 #&gt; 91 48 109 0.802 #&gt; 92 48 109 0.802 #&gt; 93 49 109 0.802 #&gt; 94 49 109 0.802 #&gt; 95 50 109 0.803 #&gt; 96 50 109 0.803 #&gt; 97 51 108 0.8 #&gt; 98 51 108 0.8 #&gt; 99 52 108 0.801 #&gt; 100 52 108 0.801 #&gt; 101 53 108 0.802 #&gt; 102 53 108 0.802 #&gt; 103 54 108 0.802 #&gt; 104 54 108 0.802 #&gt; 105 55 108 0.803 #&gt; 106 55 108 0.803 #&gt; 107 56 107 0.8 #&gt; 108 56 107 0.8 #&gt; 109 57 107 0.801 #&gt; 110 57 107 0.801 #&gt; 111 58 107 0.801 #&gt; 112 58 107 0.801 #&gt; 113 59 107 0.802 #&gt; 114 59 107 0.802 #&gt; 115 60 107 0.802 #&gt; 116 60 107 0.802 #&gt; 117 61 107 0.803 #&gt; 118 61 107 0.803 #&gt; 119 62 107 0.803 #&gt; 120 62 107 0.803 #&gt; 121 63 106 0.8 #&gt; 122 63 106 0.8 #&gt; 123 64 106 0.801 #&gt; 124 64 106 0.801 #&gt; 125 65 106 0.801 #&gt; 126 65 106 0.801 #&gt; 127 66 106 0.802 #&gt; 128 66 106 0.802 #&gt; 129 67 106 0.802 #&gt; 130 67 106 0.802 #&gt; 131 68 106 0.802 #&gt; 132 68 106 0.802 #&gt; 133 69 106 0.803 #&gt; 134 69 106 0.803 #&gt; 135 70 106 0.803 #&gt; 136 70 106 0.803 #&gt; 137 71 106 0.804 #&gt; 138 71 106 0.804 #&gt; 139 72 105 0.8 #&gt; 140 72 105 0.8 #&gt; 141 73 105 0.801 #&gt; 142 73 105 0.801 #&gt; 143 74 105 0.801 #&gt; 144 74 105 0.801 #&gt; 145 75 105 0.801 #&gt; 146 75 105 0.801 #&gt; 147 76 105 0.802 #&gt; 148 76 105 0.802 #&gt; 149 77 105 0.802 #&gt; 150 77 105 0.802 #&gt; 151 78 105 0.802 #&gt; 152 78 105 0.802 #&gt; 153 79 105 0.803 #&gt; 154 79 105 0.803 #&gt; 155 80 105 0.803 #&gt; 156 80 105 0.803 #&gt; 157 81 105 0.803 #&gt; 158 81 105 0.803 #&gt; 159 82 105 0.803 #&gt; 160 82 105 0.803 #&gt; 161 83 104 0.8 #&gt; 162 83 104 0.8 #&gt; 163 84 104 0.8 #&gt; 164 84 104 0.8 #&gt; 165 85 104 0.801 #&gt; 166 85 104 0.801 #&gt; 167 86 104 0.801 #&gt; 168 86 104 0.801 #&gt; 169 87 104 0.801 #&gt; 170 87 104 0.801 #&gt; 171 88 104 0.801 #&gt; 172 88 104 0.801 #&gt; 173 89 104 0.802 #&gt; 174 89 104 0.802 #&gt; 175 90 104 0.802 #&gt; 176 90 104 0.802 #&gt; 177 91 104 0.802 #&gt; 178 91 104 0.802 #&gt; 179 92 104 0.802 #&gt; 180 92 104 0.802 #&gt; 181 93 104 0.802 #&gt; 182 93 104 0.802 #&gt; 183 94 104 0.803 #&gt; 184 94 104 0.803 #&gt; 185 95 104 0.803 #&gt; 186 95 104 0.803 #&gt; 187 96 104 0.803 #&gt; 188 96 104 0.803 #&gt; 189 97 104 0.803 #&gt; 190 97 104 0.803 #&gt; 191 98 104 0.804 #&gt; 192 98 104 0.804 #&gt; 193 99 104 0.804 #&gt; 194 99 104 0.804 #&gt; 195 100 103 0.8 #&gt; 196 100 103 0.8 14.4 Illustration of SsSampleSizeKGivenJ() using method = \"DBMH\" This function illustrates how the number of cases for 10 readers, used in Chapter TBA, were chosen. In all but one example the default value of the desiredPower argument is used, namely 0.8 (if the argument is absent, its default value is used). 14.4.1 RRRC ncases &lt;- SsSampleSizeKGivenJ(dataset02, FOM = &quot;Wilcoxon&quot;, J = 10, method = &quot;DBMH&quot;, option = &quot;RRRC&quot;) str(ncases) #&gt; &#39;data.frame&#39;: 1 obs. of 2 variables: #&gt; $ KRRRC : num 163 #&gt; $ powerRRRC: num 0.802 ncases is a list containing the number of cases 163 and expected power 0.8015625. Compare the number of cases to the RRRC value used in Chapter TBA. 14.4.1.1 Non default value of desiredPower This is illustrated below for 90% desired power. ncases &lt;- SsSampleSizeKGivenJ(dataset02, FOM = &quot;Wilcoxon&quot;, J = 10, method = &quot;DBMH&quot;, option = &quot;RRRC&quot;, desiredPower = 0.9) str(ncases) #&gt; &#39;data.frame&#39;: 1 obs. of 2 variables: #&gt; $ KRRRC : num 236 #&gt; $ powerRRRC: num 0.9 The required number of cases is 236 and expected power is 0.9003501. 14.4.2 FRRC ncases &lt;- SsSampleSizeKGivenJ(dataset02, FOM = &quot;Wilcoxon&quot;, J = 10, method = &quot;DBMH&quot;, option = &quot;FRRC&quot;) str(ncases) #&gt; &#39;data.frame&#39;: 1 obs. of 2 variables: #&gt; $ KFRRC : num 133 #&gt; $ powerFRRC: num 0.801 The required number of cases is 133 and expected power is 0.8011167. Compare the number of cases to the FRRC value used in Chapter TBA. 14.4.3 RRFC ncases &lt;- SsSampleSizeKGivenJ(dataset02, FOM = &quot;Wilcoxon&quot;, J = 10, method = &quot;DBMH&quot;, option = &quot;RRFC&quot;) str(ncases) #&gt; &#39;data.frame&#39;: 1 obs. of 2 variables: #&gt; $ KRRFC : num 53 #&gt; $ powerRRFC: num 0.805 The required number of cases is 53 and expected power is 0.8049666. Compare the number of cases to the RRFC value used in Chapter TBA. "],
["SSRocORHRJafroc.html", "Chapter 15 ROC-ORH sample size using RJafroc 15.1 Introduction 15.2 Illustration of SsPowerGivenJK() using method = \"ORH\" 15.3 Illustration of SsPowerTable() using method = \"ORH\" 15.4 Illustrations of SsSampleSizeKGivenJ() using method = \"ORH\"", " Chapter 15 ROC-ORH sample size using RJafroc 15.1 Introduction The use of the functions introduced in Chapter TBA, but this time using the ORH method to estimate the variance components, is illustrated here. The reader should confirm that these give the same results as the corresponding ones obtained using the DBMH method. When the figure of merit is the empirical AUC, the two methods can be shown to be identical. 15.2 Illustration of SsPowerGivenJK() using method = \"ORH\" power &lt;- SsPowerGivenJK(dataset02, FOM = &quot;Wilcoxon&quot;, J = 6, K = 251, method = &quot;ORH&quot;, option = &quot;RRRC&quot;) The returned value is a list containing the expected power, the non-centrality parameter, the denominator degrees of freedom and the F-statistic (the numerator degrees of freedom is always one less than the number of treatments, i.e., unity in this example). str(power) #&gt; &#39;data.frame&#39;: 1 obs. of 4 variables: #&gt; $ powerRRRC: num 0.801 #&gt; $ ncpRRRC : num 8.91 #&gt; $ ddfHRRRC : num 16.1 #&gt; $ fRRRC : num 4.49 Expected power is 0.8005403. 15.3 Illustration of SsPowerTable() using method = \"ORH\" powTab &lt;- SsPowerTable(dataset02, FOM = &quot;Wilcoxon&quot;, method = &quot;ORH&quot;, option = &quot;RRRC&quot;) Now show the power table powTab. powTab #&gt; numReaders numCases power #&gt; 1 3 &gt;2000 &lt;NA&gt; #&gt; 2 3 &gt;2000 &lt;NA&gt; #&gt; 3 4 1089 0.8 #&gt; 4 4 1089 0.8 #&gt; 5 5 344 0.801 #&gt; 6 5 344 0.801 #&gt; 7 6 251 0.801 #&gt; 8 6 251 0.801 #&gt; 9 7 211 0.801 #&gt; 10 7 211 0.801 #&gt; 11 8 188 0.801 #&gt; 12 8 188 0.801 #&gt; 13 9 173 0.801 #&gt; 14 9 173 0.801 #&gt; 15 10 163 0.802 #&gt; 16 10 163 0.802 #&gt; 17 11 155 0.801 #&gt; 18 11 155 0.801 #&gt; 19 12 149 0.802 #&gt; 20 12 149 0.802 #&gt; 21 13 144 0.801 #&gt; 22 13 144 0.801 #&gt; 23 14 140 0.802 #&gt; 24 14 140 0.802 #&gt; 25 15 137 0.802 #&gt; 26 15 137 0.802 #&gt; 27 16 134 0.802 #&gt; 28 16 134 0.802 #&gt; 29 17 131 0.801 #&gt; 30 17 131 0.801 #&gt; 31 18 129 0.801 #&gt; 32 18 129 0.801 #&gt; 33 19 127 0.801 #&gt; 34 19 127 0.801 #&gt; 35 20 126 0.802 #&gt; 36 20 126 0.802 #&gt; 37 21 124 0.801 #&gt; 38 21 124 0.801 #&gt; 39 22 123 0.802 #&gt; 40 22 123 0.802 #&gt; 41 23 122 0.802 #&gt; 42 23 122 0.802 #&gt; 43 24 121 0.803 #&gt; 44 24 121 0.803 #&gt; 45 25 120 0.802 #&gt; 46 25 120 0.802 #&gt; 47 26 119 0.802 #&gt; 48 26 119 0.802 #&gt; 49 27 118 0.802 #&gt; 50 27 118 0.802 #&gt; 51 28 117 0.801 #&gt; 52 28 117 0.801 #&gt; 53 29 117 0.803 #&gt; 54 29 117 0.803 #&gt; 55 30 116 0.802 #&gt; 56 30 116 0.802 #&gt; 57 31 115 0.801 #&gt; 58 31 115 0.801 #&gt; 59 32 115 0.803 #&gt; 60 32 115 0.803 #&gt; 61 33 114 0.801 #&gt; 62 33 114 0.801 #&gt; 63 34 114 0.803 #&gt; 64 34 114 0.803 #&gt; 65 35 113 0.801 #&gt; 66 35 113 0.801 #&gt; 67 36 113 0.802 #&gt; 68 36 113 0.802 #&gt; 69 37 112 0.8 #&gt; 70 37 112 0.8 #&gt; 71 38 112 0.802 #&gt; 72 38 112 0.802 #&gt; 73 39 112 0.803 #&gt; 74 39 112 0.803 #&gt; 75 40 111 0.801 #&gt; 76 40 111 0.801 #&gt; 77 41 111 0.802 #&gt; 78 41 111 0.802 #&gt; 79 42 111 0.803 #&gt; 80 42 111 0.803 #&gt; 81 43 110 0.801 #&gt; 82 43 110 0.801 #&gt; 83 44 110 0.802 #&gt; 84 44 110 0.802 #&gt; 85 45 110 0.802 #&gt; 86 45 110 0.802 #&gt; 87 46 110 0.803 #&gt; 88 46 110 0.803 #&gt; 89 47 109 0.801 #&gt; 90 47 109 0.801 #&gt; 91 48 109 0.802 #&gt; 92 48 109 0.802 #&gt; 93 49 109 0.802 #&gt; 94 49 109 0.802 #&gt; 95 50 109 0.803 #&gt; 96 50 109 0.803 #&gt; 97 51 108 0.8 #&gt; 98 51 108 0.8 #&gt; 99 52 108 0.801 #&gt; 100 52 108 0.801 #&gt; 101 53 108 0.802 #&gt; 102 53 108 0.802 #&gt; 103 54 108 0.802 #&gt; 104 54 108 0.802 #&gt; 105 55 108 0.803 #&gt; 106 55 108 0.803 #&gt; 107 56 107 0.8 #&gt; 108 56 107 0.8 #&gt; 109 57 107 0.801 #&gt; 110 57 107 0.801 #&gt; 111 58 107 0.801 #&gt; 112 58 107 0.801 #&gt; 113 59 107 0.802 #&gt; 114 59 107 0.802 #&gt; 115 60 107 0.802 #&gt; 116 60 107 0.802 #&gt; 117 61 107 0.803 #&gt; 118 61 107 0.803 #&gt; 119 62 107 0.803 #&gt; 120 62 107 0.803 #&gt; 121 63 106 0.8 #&gt; 122 63 106 0.8 #&gt; 123 64 106 0.801 #&gt; 124 64 106 0.801 #&gt; 125 65 106 0.801 #&gt; 126 65 106 0.801 #&gt; 127 66 106 0.802 #&gt; 128 66 106 0.802 #&gt; 129 67 106 0.802 #&gt; 130 67 106 0.802 #&gt; 131 68 106 0.802 #&gt; 132 68 106 0.802 #&gt; 133 69 106 0.803 #&gt; 134 69 106 0.803 #&gt; 135 70 106 0.803 #&gt; 136 70 106 0.803 #&gt; 137 71 106 0.804 #&gt; 138 71 106 0.804 #&gt; 139 72 105 0.8 #&gt; 140 72 105 0.8 #&gt; 141 73 105 0.801 #&gt; 142 73 105 0.801 #&gt; 143 74 105 0.801 #&gt; 144 74 105 0.801 #&gt; 145 75 105 0.801 #&gt; 146 75 105 0.801 #&gt; 147 76 105 0.802 #&gt; 148 76 105 0.802 #&gt; 149 77 105 0.802 #&gt; 150 77 105 0.802 #&gt; 151 78 105 0.802 #&gt; 152 78 105 0.802 #&gt; 153 79 105 0.803 #&gt; 154 79 105 0.803 #&gt; 155 80 105 0.803 #&gt; 156 80 105 0.803 #&gt; 157 81 105 0.803 #&gt; 158 81 105 0.803 #&gt; 159 82 105 0.803 #&gt; 160 82 105 0.803 #&gt; 161 83 104 0.8 #&gt; 162 83 104 0.8 #&gt; 163 84 104 0.8 #&gt; 164 84 104 0.8 #&gt; 165 85 104 0.801 #&gt; 166 85 104 0.801 #&gt; 167 86 104 0.801 #&gt; 168 86 104 0.801 #&gt; 169 87 104 0.801 #&gt; 170 87 104 0.801 #&gt; 171 88 104 0.801 #&gt; 172 88 104 0.801 #&gt; 173 89 104 0.802 #&gt; 174 89 104 0.802 #&gt; 175 90 104 0.802 #&gt; 176 90 104 0.802 #&gt; 177 91 104 0.802 #&gt; 178 91 104 0.802 #&gt; 179 92 104 0.802 #&gt; 180 92 104 0.802 #&gt; 181 93 104 0.802 #&gt; 182 93 104 0.802 #&gt; 183 94 104 0.803 #&gt; 184 94 104 0.803 #&gt; 185 95 104 0.803 #&gt; 186 95 104 0.803 #&gt; 187 96 104 0.803 #&gt; 188 96 104 0.803 #&gt; 189 97 104 0.803 #&gt; 190 97 104 0.803 #&gt; 191 98 104 0.804 #&gt; 192 98 104 0.804 #&gt; 193 99 104 0.804 #&gt; 194 99 104 0.804 #&gt; 195 100 103 0.8 #&gt; 196 100 103 0.8 Since the default FOM = \"Wilcoxon\", the table is identical to that generated in chapter TBA, which used method = \"DBMH\". 15.4 Illustrations of SsSampleSizeKGivenJ() using method = \"ORH\" 15.4.1 For RRRC generalization ncases &lt;- SsSampleSizeKGivenJ(dataset02, FOM = &quot;Wilcoxon&quot;, J = 10, method = &quot;ORH&quot;, option = &quot;RRRC&quot;) ncases is a list containing the number of cases ncases$KRRRC and expected power ncases$powerRRRC. str(ncases) #&gt; &#39;data.frame&#39;: 1 obs. of 2 variables: #&gt; $ KRRRC : num 163 #&gt; $ powerRRRC: num 0.802 The required number of cases is 163 and expected power is 0.8015625. 15.4.2 For FRRC generalization ncases &lt;- SsSampleSizeKGivenJ(dataset02, FOM = &quot;Wilcoxon&quot;, J = 10, method = &quot;ORH&quot;, option = &quot;FRRC&quot;) The required number of cases is 133 and expected power is 0.8011167. 15.4.3 For RRFC generalization ncases &lt;- SsSampleSizeKGivenJ(dataset02, FOM = &quot;Wilcoxon&quot;, J = 10, method = &quot;ORH&quot;, option = &quot;RRFC&quot;) The required number of cases is 53 and expected power is 0.8049666. "],
["SSJafrocEffectSize.html", "Chapter 16 Choosing a realistic effect size 16.1 Introduction 16.2 Illustration of SsPowerGivenJK() using method = \"ORH\" 16.3 References", " Chapter 16 Choosing a realistic effect size 16.1 Introduction The value of the true FOM difference between the treatments, i.e., the true effect-size (ES) is, of course, unknown. If it were known, there would be no need to conduct an ROC study. One would simply adopt the treatment with the higher FOM. Sample-size estimation involves making an educated guess regarding the ES, called the anticipated ES, and denoted by d. To quote (ICRU 2008): “any calculation of power amounts to specification of the anticipated effect-size”. Increasing the anticipated ES will increase statistical power but may represent an unrealistic expectation of the true difference between the treatments, in the sense that it overestimates the ability of technology to achieve this much improvement. An unduly small might be clinically insignificant, besides requiring a very large sample-size to achieve sufficient power. There is a key difference between statistical significance and clinical significance. An effect-size in AUC units could be so small, e.g., 0.001, as to be clinically insignificant, but by employing a sufficiently large sample size one could design a study to detect this small and clinically meaningless difference with high probability, i.e., high statistical power. What determines clinical significance? A small effect-size, e.g., 0.01 AUC units, could be clinically significant if it applies to a large population, where the small benefit in detection rate is amplified by the number of patients benefiting from the new treatment. In contrast, for an “orphan” disease, i.e., one with very low prevalence, an effect-size of 0.05 might not be enough to justify the additional cost of the new treatment. The improvement might have to be 0.1 before it is worth it for a new treatment to be brought to market. One hates to monetize life and death issues, but there is no getting away from it, as cost/benefit issues determine clinical significance. The arbiters of clinical significance are engineers, imaging scientists, clinicians, epidemiologists, insurance companies and those who set government health care policies. The engineers and imaging scientists determine whether the effect-size the clinicians would like is feasible from technical and scientific viewpoints. The clinician determines, based on incidence of disease and other considerations, e.g., altruistic, malpractice, cost of the new device and insurance reimbursement, what effect-size is justifiable. Cohen has suggested that d values of 0.2, 0.5, and 0.8 be considered small, medium, and large, respectively, but he has also argued against their indiscriminate usage. However, after a study is completed, clinicians often find that an effect-size that biostatisticians label as small may, in certain circumstances, be clinically significant and an effect-size that they label as large may in other circumstances be clinically insignificant. Clearly, this is a complex issue. Some suggestions on choosing a clinically significant effect size are made in Chapter 11. Does one even need to perform a pivotal study? If the pilot study returns a significant difference, one has rejected the NH and that is all there is to it. There is no need to perform the pivotal study, unless one “tweaks” the new treatment and/or casts a wider sampling net to make a stronger argument, perhaps to the FDA, that the treatments are indeed generalizable, and that the difference is in the right direction (new treatment FOM &gt; conventional treatment FOM). If a significant difference is observed in the opposite direction (e.g., new treatment FOM &lt; conventional treatment FOM) one cannot justify a pivotal study with an expected effect-size in the “other or favored” direction; see example below. Since the Van Dyke pilot study came close to rejecting the NH and the observed effect size, see below, is not too small, a pivotal study is justified. This chapter discusses choosing a realistic effect size based on the pilot study. Illustrated first is using Van Dyke dataset, regarded as the pilot study. 16.2 Illustration of SsPowerGivenJK() using method = \"ORH\" rocData &lt;- dataset02 ##&quot;VanDyke.lrc&quot; #fileName &lt;- dataset03 ## &quot;Franken1.lrc&quot; retDbm &lt;- StSignificanceTesting(dataset = rocData, FOM = &quot;Wilcoxon&quot;, method = &quot;DBMH&quot;) str(retDbm$ciDiffTrtRRRC) #&gt; &#39;data.frame&#39;: 1 obs. of 8 variables: #&gt; $ TrtDiff : chr &quot;Trt0-Trt1&quot; #&gt; $ Estimate: num -0.0438 #&gt; $ StdErr : num 0.0207 #&gt; $ DF : num 15.3 #&gt; $ t : num -2.11 #&gt; $ PrGTt : num 0.0517 #&gt; $ CILower : num -0.088 #&gt; $ CIUpper : num 0.000359 Lacking any other information, the observed effect-size is the best estimate of the effect-size to be anticipated. The output shows that the FOM difference, for treatment 0 minus treatment 1, is -0.0438003. In the actual study treatment 1 is the new modality which hopes to improve upon 0, the conventional modality. Since the sign is negative, the difference is going the right way and is justified in moving forward with planning a pivotal study. [If the difference went the other way, there is little justification for a pivotal study]. The standard error of the difference is 0.0207486. An optimistic (but not unduly so) effect size is given by: effectSizeOpt &lt;- abs(retDbm$ciDiffTrtRRRC$Estimate) + 2*retDbm$ciDiffTrtRRRC$StdErr The observed effect-size is a realization of a random variable. The lower limit of the 95% confidence interval is given by -0.0879595 and the upper limit by 3.5885444^{-4}. CI’s generated like this, with independent sets of data, are expected to encompass the true value with 95% probability. The lower end (greatest magnitude of the difference) of the confidence interval is -0.0852976, and this is the optimistic estimate. Since the sign is immaterial, one uses as the optimistic estimate the value 0.0852976. While the sign is immaterial for sample size estimates, the decision to conduct the pivotal most certainly is material. If the sign went the other way, with the new modality lower than the conventional modality, one would be unjustified in conducting a pivotal study. 16.3 References REFERENCES "],
["SSFroc1.html", "Chapter 17 FROC sample size estimation and comparison to ROC 17.1 Introduction 17.2 Relating an ROC effect-size to a wAFROC effect-size 17.3 Computing the respective variance components 17.4 Comparing ROC power to wAFROC power for equivalent effect-sizes 17.5 References", " Chapter 17 FROC sample size estimation and comparison to ROC 17.1 Introduction FROC sample size estimation is not fundamentally different from the previously outlined procedure (see Chapter TBA) for the ROC paradigm. To recapitulate, based on analysis of a pilot ROC dataset and using a specified FOM, e.g., FOM = Wilcoxon, and either method = \"DBMH\" or method = \"ORH\" for significance testing, one estimates the intrinsic variability of the data expressed in terms of variance components or the covariance matrix. The second step is to postulate a clinically realistic effect-size, e.g., the anticipated AUC difference between the two treatments. Given these values, the sample size functions implemented in RJafroc (beginning with Ss) allow one to estimate the number of readers and cases necessary to detect (i.e., reject the null hypothesis) the specified effect size at specified Type II error rate, typically chosen to be 20% (corresponding to 80% statistical power) and specified Type I error rate, typically chosen to be 5%. In FROC analysis the only difference, indeed the critical difference, is the choice of FOM; e.g., FOM = \"wAFROC\" instead of the inferred ROC-AUC, FOM = \"HrAuc\". The FROC dataset is analyzed using either the DBMH or the ORH method. This yields the necessary variance components or the covariance matrix corresponding to the wAFROC-AUC. The next step is to specify the effect-size in wAFROC-AUC units, and therein lies the rub. What value does one use? The ROC-AUC has a historically well-known interpretation: the classification ability at separating diseased patients from non-diseased patients, while the wAFROC-AUC does not. Needed is a way of relating the effect-size in ROC-AUC units to one in wAFROC-AUC units: as should be obvious this requires a physical model, e.g., the RSM, that predicts both ROC and wAFROC curves and the respective AUCs. One chooses an ROC-AUC effect-size that is realistic, one that clinicians understand and can therefore participate in, in the effect-size postulation process. One converts the ROC effect-size to a wAFROC-AUC effect-size. The method for this is described in the next section. One uses the sample size tools in in RJafroc to determine sample size or power. It is important to recognize is that all quantities have to be in the same units. When performing ROC analysis, everything (variance components and effect-size) has to be in units of the selected FOM, e.g., FOM = \"Wilcoxon\" which is identical to the empirical ROC-AUC. When doing wAFROC analysis, everything has to be in units of the wAFROC-AUC. The variance components and effect-size in wAFROC-AUC units will be different from their corresponding ROC counterparts. In particular, as shown next, an ROC-AUC effect-size of 0.05 generally corresponds to a larger effect-size in wAFROC-AUC units. The reason for this is that the range over which wAFROC-AUC can vary, namely 0 to 1, is twice the corresponding ROC-AUC range. The next section explains the steps used to implement step #2 above. 17.2 Relating an ROC effect-size to a wAFROC effect-size If the original data is FROC, one needs to first convert it to ROC, using DfFroc2Roc(): the RSM fits ROC data. For each treatment and reader the inferred ROC data is fitted by FitRsmRoc(), yielding estimates of the RSM physical (or primed) parameters (not the intrinsic values). The following example uses the first two treatments of the “FED” dataset, dataset04, which is a 5 treatment 4 radiologist FROC dataset acquired by Dr. Federica Zanca et. al. (Zanca et al. 2009). The dataset has 5 treatments and 4 readers and 200 cases and was acquired on a 5-point integer scale, i.e., it is already binned. If not one needs to bin the dataset using DfBinDataset(). I need to emphasize this point: if the dataset represents continuous ratings, as with a CAD algorithm, one must bin the dataset to (ideally) about 5 bins. The number of parameters that must be estimated increases with the number of bins (for each bin one needs to estimate a cutoff parameter). The reason for using RSM parameter values only for the first two treatments is that these were found (Zanca et al. 2009) to be almost equivalent (more precisely, the NH could not be rejected for the first two treatments, so it makes sense to regard them as “almost” NH treatments. The following code block defines the pilot FROC data frocData (corresponding to dataset04, which is the “FED” dataset, but with only treatments 1 and 2 extracted, using DfExtractDataset()) and rocData, i.e., the highest-rating ROC dataset inferred from the FROC dataset using DfFroc2Roc(). frocData &lt;- DfExtractDataset(dataset04, trts = c(1,2)) rocData &lt;- DfFroc2Roc(frocData) The next code block determines lesDist, the lesion distribution array, which has Lmax (maximum number of lesions per diseased case over the dataset) rows and two columns. The first column contains the integers 1, 2, …, Lmax and the second column contains the fraction of diseased cases with the number of lesions per case specified in the first column. The second column will sum to unity. The RSM fitting algorithm needs to know how lesion-rich the dataset is, as the RSM predicted ROC-AUC depends on the lesion-richness of the dataset. For reasons that will become clear below, one also needs lesWghts, the distribution of the lesion weights. lesDistr &lt;- UtilLesionDistr(frocData) lesWghts &lt;- UtilLesionWeightsDistr(frocData) # this is needed later The meanings of lesDistr and lesWghts is clear from examining their values: print(lesDistr) #&gt; [,1] [,2] #&gt; [1,] 1 0.69 #&gt; [2,] 2 0.20 #&gt; [3,] 3 0.11 print(lesWghts) #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 1 1.0000000 -Inf -Inf #&gt; [2,] 2 0.5000000 0.5000000 -Inf #&gt; [3,] 3 0.3333333 0.3333333 0.3333333 For this dataset Lmax is 3, and 69 percent of the diseased cases have one lesion, 20 percent have two lesions and 11 percent have three lesions. Since the lesions are equally weighted, on cases with one lesion the weight of the lesion is unity, on cases with two lesions the weights of each lesion is 0.5 and on cases with three lesions the weight of each lesion is 1/3. The next code block determines the number of treatments and readers (I and J) from the dimensions of the frocData$NL array. It creates an array RsmParms to hold the RSM fitted parameter values. For each treatment and reader it applies the fitting algorithm FitRsmRoc(). The first three returned values are mu, lambdaP and nuP, corresponding to RSM parameters \\({\\mu}\\), \\({\\lambda^{&#39;}}\\) and \\({\\nu^{&#39;}}\\) . I &lt;- dim(frocData$NL)[1] J &lt;- dim(frocData$NL)[2] RsmParms &lt;- array(dim = c(I,J,3)) for (i in 1:I) { for (j in 1:J) { x1 &lt;- FitRsmRoc(rocData, trt = i, rdr = j, lesDistr) RsmParms[i,j,1] &lt;- x1[[1]] # mu RsmParms[i,j,2] &lt;- x1[[2]] # lambdaP RsmParms[i,j,3] &lt;- x1[[3]] # nuP } } I recommend taking the median of each of the parameters, over all treatment-reader indices, as representing the average NH dataset. The median is less sensitive to outliers than the mean. muMed &lt;- median(RsmParms[,,1]) lambdaPMed &lt;- median(RsmParms[,,2]) nuPMed &lt;- median(RsmParms[,,3]) The defining values of the fitting model are muMed = 3.3105557, lambdaPMed = 1.714368 and nuPMed = 0.7036567. Note that these obey the constraints lambdaPMed &gt; 0 and 0 &lt; nuP &lt; 1. One then converts the physical parameters to the intrinsic values: temp &lt;- UtilPhysical2IntrinsicRSM(muMed, lambdaPMed, nuPMed) lambdaMed &lt;- temp$lambda nuMed &lt;- temp$nu In terms of intrinsic parameters, the defining values of the fitting model are muMed = 3.3105557, lambdaMed = 5.6755108 and nuMed = 0.3673814. We are now ready to calcuate the expected NH FOMs using the ROC -AUC and the wAFROC FOM. aucRocNH &lt;- PlotRsmOperatingCharacteristics(muMed, lambdaMed, nuMed, lesDistr = lesDistr, lesWghtDistr = lesWghts, OpChType = &quot;ROC&quot;)$aucROC aucwAfrocNH &lt;- PlotRsmOperatingCharacteristics(muMed, lambdaMed, nuMed, lesDistr = lesDistr, lesWghtDistr = lesWghts, OpChType = &quot;wAFROC&quot;)$aucwAFROC The plotting function PlotRsmOperatingCharacteristics() returns a number of other objects, most importantly the plot, but here we use only the AUC, which is obtained by numerical integration of the predicted operating characteristics. However, it calls for the intrinsic RSM parameters, which is why we had to convert the physical to the intrinsic values. One has aucRocNH = 0.8791301 and aucwAfrocNH = 0.7198311. Note that the wAFROC-FOM is smaller than the ROC-FOM as it includes the localization constraint. To induce the alternative hypothesis condition, one increments \\(\\mu_{NH}\\) by \\(\\Delta_{\\mu}\\). The resulting ROC-AUC and wAFROC-AUC are calculated, again by numerical integration of the RSM predicted ROC and wAFROC curves, leading to the corresponding effect-sizes (note that in each equation below one takes the difference between the AH value minus the NH value): The next step is to calculate the effect size (new value minus the NH value) using ROC and wAFROC FOMs for a series of specifed deltaMu values. This generates values that can be used to interpolate a wAFROC effect size for a specified ROC effect size. deltaMu &lt;- seq(0.01, 0.2, 0.01) # values of deltaMu to scan below esRoc &lt;- array(dim = length(deltaMu));eswAfroc &lt;- array(dim = length(deltaMu)) for (i in 1:length(deltaMu)) { esRoc[i] &lt;- PlotRsmOperatingCharacteristics( muMed + deltaMu[i], lambdaMed, nuMed, lesDistr = lesDistr, lesWghtDistr = lesWghts, OpChType = &quot;ROC&quot;)$aucROC - aucRocNH eswAfroc[i] &lt;- PlotRsmOperatingCharacteristics( muMed+ deltaMu[i], lambdaMed, nuMed, lesDistr = lesDistr, lesWghtDistr = lesWghts, OpChType = &quot;wAFROC&quot;)$aucwAFROC - aucwAfrocNH cat(&quot;ES_ROC = &quot;, esRoc[i], &quot;, ES_wAFROC = &quot;, eswAfroc[i],&quot;\\n&quot;) } #&gt; ES_ROC = 0.0006197813 , ES_wAFROC = 0.001329066 #&gt; ES_ROC = 0.001234752 , ES_wAFROC = 0.002650005 #&gt; ES_ROC = 0.00184496 , ES_wAFROC = 0.003962878 #&gt; ES_ROC = 0.002450451 , ES_wAFROC = 0.005267748 #&gt; ES_ROC = 0.003051273 , ES_wAFROC = 0.006564677 #&gt; ES_ROC = 0.003647472 , ES_wAFROC = 0.007853725 #&gt; ES_ROC = 0.004239094 , ES_wAFROC = 0.009134954 #&gt; ES_ROC = 0.004826184 , ES_wAFROC = 0.01040842 #&gt; ES_ROC = 0.005408788 , ES_wAFROC = 0.0116742 #&gt; ES_ROC = 0.00598695 , ES_wAFROC = 0.01293233 #&gt; ES_ROC = 0.006560717 , ES_wAFROC = 0.01418289 #&gt; ES_ROC = 0.007130131 , ES_wAFROC = 0.01542592 #&gt; ES_ROC = 0.007695238 , ES_wAFROC = 0.0166615 #&gt; ES_ROC = 0.00825608 , ES_wAFROC = 0.01788967 #&gt; ES_ROC = 0.008812701 , ES_wAFROC = 0.0191105 #&gt; ES_ROC = 0.009365145 , ES_wAFROC = 0.02032404 #&gt; ES_ROC = 0.009913453 , ES_wAFROC = 0.02153036 #&gt; ES_ROC = 0.01045767 , ES_wAFROC = 0.0227295 #&gt; ES_ROC = 0.01099783 , ES_wAFROC = 0.02392152 #&gt; ES_ROC = 0.01153399 , ES_wAFROC = 0.02510649 Here is a plot of wAFROC effect size (y-axis) vs. ROC effect size. df &lt;- data.frame(es_ROC = esRoc, es_wAFROC = eswAfroc) p &lt;- ggplot(data = df, aes(x = es_ROC, y = es_wAFROC)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;, formula = y ~ x) + geom_point(size = 4) + scale_color_manual(values = &quot;black&quot;) + theme(axis.title.y = element_text(size = 10,face=&quot;bold&quot;), axis.title.x = element_text(size = 10,face=&quot;bold&quot;)) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) print(p) The plot is very close to linear. This makes it easy to design an interpolation function. In the following code block the first line fits eswAfroc vs. esRoc using the linear model lm() function constrained to pass through the origin (the minus one): scaleFactor &lt;- lm(eswAfroc ~ -1 + esRoc). One expects this constraint since for deltaMu = 0 the effect size must be zero no matter how it is measured. scaleFactor&lt;-lm(eswAfroc~-1+esRoc) # fit values to straight line thru origin effectSizeROC &lt;- seq(0.01, 0.1, 0.01) effectSizewAFROC &lt;- effectSizeROC*scaleFactor$coefficients[1] # r2 = summary(scaleFactor)$r.squared The scaleFactor of the straight line fit is scaleFactor, where scaleFactor = 2.1688609 and R2 = 0.9999904. Therefore, the conversion from ROC to wAFROC effect size is: effectSizewAFROC = scaleFactor * effectSizeROC. The wAFROC effect size is twice the ROC effect size. All that remains is to calculate the variance componenents using the two FOMs: 17.3 Computing the respective variance components The code block applies StSignificanceTesting() to rocData and frocData, using the appropriate FOM, and extracts the variance components. temp1 &lt;- StSignificanceTesting(rocData, FOM = &quot;Wilcoxon&quot;, method = &quot;DBMH&quot;, option = &quot;RRRC&quot;) temp2 &lt;- StSignificanceTesting(frocData, FOM = &quot;wAFROC&quot;, method = &quot;DBMH&quot;, option = &quot;RRRC&quot;) varCompROC &lt;- temp1$varComp varCompwAFROC &lt;- temp2$varComp The observed wAFROC effect-size is -0.0068562. This is a very small effect size; the corresponding ROC effect-size is -0.0051; the sign does not affect the calculations, which is too small to reach 80% power. It is not surprising that the study (Zanca et al. 2009) did not find a significant difference between these two treatments The respective variance components are: print(varCompROC) #&gt; varR varC varTR varTC varRC varErr #&gt; 1 0.000827738 0.03812335 0.0001526507 0.009644327 0.003544196 0.09484637 print(varCompwAFROC) #&gt; varR varC varTR varTC varRC varErr #&gt; 1 0.001854229 0.06117805 -0.0004439279 0.01016519 0.01355883 0.0967256 Only terms involving treatment are relevant to sample size. The wAFROC varTC and varError values are slightly larger than the ROC ones - as expected because, again, the range of the wAFROC FOM is twice that of the ROC FOM. 17.4 Comparing ROC power to wAFROC power for equivalent effect-sizes We are now ready to compare ROC and wAFROC powers for equivalent effect sizes. The following example is for 5 readers (JTest) and 100 cases (KTest) in the pivotal study. powerROC &lt;- array(dim = length(effectSizeROC));powerwAFROC &lt;- array(dim = length(effectSizeROC)) JTest &lt;- 5;KTest &lt;- 100 for (i in 1:length(effectSizeROC)) { varYTR &lt;- varCompROC$varTR # these are pseudovalue based variance components assuming FOM = &quot;Wilcoxon&quot; varYTC &lt;- varCompROC$varTC varYEps &lt;- varCompROC$varErr ret &lt;- SsPowerGivenJKDbmVarComp (J = JTest, K = KTest, effectSize = effectSizeROC[i], varYTR, varYTC, varYEps, option = &quot;RRRC&quot;) powerROC[i] &lt;- ret$powerRRRC varYTR &lt;- varCompwAFROC$varTR # these are pseudovalue based variance components assuming FOM = &quot;wAFROC&quot; varYTC &lt;- varCompwAFROC$varTC varYEps &lt;- varCompwAFROC$varErr ret &lt;- SsPowerGivenJKDbmVarComp (J = JTest, K = KTest, effectSize = effectSizewAFROC[i], varYTR, varYTC, varYEps, option = &quot;RRRC&quot;) powerwAFROC[i] &lt;- ret$powerRRRC cat(&quot;ROC-ES = &quot;, effectSizeROC[i], &quot;, wAFROC-ES = &quot;, effectSizewAFROC[i], &quot;, Power-ROC = &quot;, powerROC[i], &quot;, Power-wAFROC = &quot;, powerwAFROC[i], &quot;\\n&quot;) } #&gt; ROC-ES = 0.01 , wAFROC-ES = 0.02168861 , Power-ROC = 0.06443046 , Power-wAFROC = 0.1266316 #&gt; ROC-ES = 0.02 , wAFROC-ES = 0.04337722 , Power-ROC = 0.108789 , Power-wAFROC = 0.3605744 #&gt; ROC-ES = 0.03 , wAFROC-ES = 0.06506583 , Power-ROC = 0.1847115 , Power-wAFROC = 0.6686875 #&gt; ROC-ES = 0.04 , wAFROC-ES = 0.08675444 , Power-ROC = 0.2907927 , Power-wAFROC = 0.8897125 #&gt; ROC-ES = 0.05 , wAFROC-ES = 0.108443 , Power-ROC = 0.4195443 , Power-wAFROC = 0.9777308 #&gt; ROC-ES = 0.06 , wAFROC-ES = 0.1301317 , Power-ROC = 0.5573812 , Power-wAFROC = 0.9973522 #&gt; ROC-ES = 0.07 , wAFROC-ES = 0.1518203 , Power-ROC = 0.6881601 , Power-wAFROC = 0.9998172 #&gt; ROC-ES = 0.08 , wAFROC-ES = 0.1735089 , Power-ROC = 0.7983611 , Power-wAFROC = 0.9999927 #&gt; ROC-ES = 0.09 , wAFROC-ES = 0.1951975 , Power-ROC = 0.8809508 , Power-wAFROC = 0.9999998 #&gt; ROC-ES = 0.1 , wAFROC-ES = 0.2168861 , Power-ROC = 0.936068 , Power-wAFROC = 1 Since the wAFROC effect size is about a factor of two larger than the ROC effect size, wAFROC power is larger than that for ROC. The effect is magnified as the effect size enters as the square in the formula for the power (this overwhelms the slight increase in variability of wAFROC-FOM relative to ROC-FOM noted previously). The following is a plot of the respective powers. df &lt;- data.frame(power_ROC = powerROC, power_wAFROC = powerwAFROC) p &lt;- ggplot(mapping = aes(x = power_ROC, y = power_wAFROC)) + geom_line(data = df, size = 2)+ scale_color_manual(values = &quot;black&quot;) + theme(axis.title.y = element_text(size = 10,face=&quot;bold&quot;), axis.title.x = element_text(size = 10,face=&quot;bold&quot;)) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) print(p) 17.5 References REFERENCES "],
["SSFroc2.html", "Chapter 18 FROC sample size estimation using specified ROC effect 18.1 Introduction 18.2 Constructing the NH model for the dataset 18.3 Extracting the wAFROC variance components 18.4 wAFROC power for specified ROC effect size, number of readers J and number of cases K 18.5 wAFROC number of cases for 80% power for a given number of readers J 18.6 wAFROC Power for a given number of readers J and cases K 18.7 References", " Chapter 18 FROC sample size estimation using specified ROC effect 18.1 Introduction This example uses the FED dataset as a pilot FROC study and function SsFrocNhRsmModel() to construct the NH model (encapsulating some of the code in the previous chapter). 18.2 Constructing the NH model for the dataset One starts by extracting the first two treatments from dataset04, which represent the NH dataset, see previous chapter. Next one constructs the NH model - note that the lesion distribution lesionPmf can be specified here independently of that in the pilot dataset. This allows some control over selection of the diseased cases in the pivotal study. frocNhData &lt;- DfExtractDataset(dataset04, trts = c(1,2)) ret &lt;- SsFrocNhRsmModel(frocNhData, lesionPmf = c(0.7, 0.2, 0.1)) muMed &lt;- ret$muMed lambdaMed &lt;- ret$lambdaMed nuMed &lt;- ret$nuMed lesDistr &lt;- ret$lesDistr lesWghtDistr &lt;- ret$lesWghtDistr scaleFactor &lt;- ret$scaleFactor The fitting model is defined by muMed = r muMed, lambdaMed = 5.6140941 and nuMed = 0.3696988 and lesionPmf. The effect size scale factor is 2.1542102. aucRocNH &lt;- PlotRsmOperatingCharacteristics(muMed, lambdaMed, nuMed, lesDistr = lesDistr, lesWghtDistr = lesWghtDistr, OpChType = &quot;ROC&quot;)$aucROC aucwAfrocNH &lt;- PlotRsmOperatingCharacteristics(muMed, lambdaMed, nuMed, lesDistr = lesDistr, lesWghtDistr = lesWghtDistr, OpChType = &quot;wAFROC&quot;)$aucwAFROC The null hypothesis ROC AUC is 0.8790725 and the corresponding NH wAFROC AUC is 0.7231709. 18.3 Extracting the wAFROC variance components The next code block applies StSignificanceTesting() to frocNhData, using FOM = \"wAFROC\" and extracts the variance components. varCompwAFROC &lt;- StSignificanceTesting(frocNhData, FOM = &quot;wAFROC&quot;, method = &quot;DBMH&quot;, option = &quot;RRRC&quot;)$varComp 18.4 wAFROC power for specified ROC effect size, number of readers J and number of cases K The following example is for ROC effect size = 0.05, 5 readers (J) and 100 cases (K) in the pivotal study. ROC_ES &lt;- 0.05 effectSizewAFROC &lt;- scaleFactor * ROC_ES J &lt;- 5;K &lt;- 100 varYTR &lt;- varCompwAFROC$varTR varYTC &lt;- varCompwAFROC$varTC varYEps &lt;- varCompwAFROC$varErr ret &lt;- SsPowerGivenJKDbmVarComp (J = J, K = K, effectSize = effectSizewAFROC, varYTR, varYTC, varYEps, option = &quot;RRRC&quot;) powerwAFROC &lt;- ret$powerRRRC cat(&quot;ROC-ES = &quot;, ROC_ES, &quot;, wAFROC-ES = &quot;, ROC_ES * scaleFactor, &quot;, Power-wAFROC = &quot;, powerwAFROC, &quot;\\n&quot;) #&gt; ROC-ES = 0.05 , wAFROC-ES = 0.1077105 , Power-wAFROC = 0.976293 18.5 wAFROC number of cases for 80% power for a given number of readers J varYTR &lt;- varCompwAFROC$varTR varYTC &lt;- varCompwAFROC$varTC varYEps &lt;- varCompwAFROC$varErr ret2 &lt;- SsSampleSizeKGivenJ(dataset = NULL, J = 6, effectSize = effectSizewAFROC, method = &quot;DBMH&quot;, list(varYTR = varYTR, varYTC = varYTC, varYEps = varYEps)) cat(&quot;ROC-ES = &quot;, ROC_ES, &quot;, wAFROC-ES = &quot;, ROC_ES * scaleFactor, &quot;, K80RRRC = &quot;, ret2$KRRRC, &quot;, Power-wAFROC = &quot;, ret2$powerRRRC, &quot;\\n&quot;) #&gt; ROC-ES = 0.05 , wAFROC-ES = 0.1077105 , K80RRRC = 42 , Power-wAFROC = 0.804794 18.6 wAFROC Power for a given number of readers J and cases K ret3 &lt;- SsPowerGivenJK(dataset = NULL, J = 6, K = ret2$KRRRC, effectSize = effectSizewAFROC, method = &quot;DBMH&quot;, list(varYTR = varYTR, varYTC = varYTC, varYEps = varYEps)) cat(&quot;ROC-ES = &quot;, ROC_ES, &quot;, wAFROC-ES = &quot;, ROC_ES * scaleFactor, &quot;, powerRRRC = &quot;, ret3$powerRRRC, &quot;\\n&quot;) #&gt; ROC-ES = 0.05 , wAFROC-ES = 0.1077105 , powerRRRC = 0.804794 The estimated power is close to 80% as the number of cases (ret2$KRRRC = 42) was chosen deliberately from the previous code block. 18.7 References "],
["RsmOpCh.html", "Chapter 19 RSM predicted operating characteristics 19.1 Introduction 19.2 The distinction between predicted curves and empirical curves 19.3 The RSM model 19.4 The empirical wAFROC 19.5 The predicted wAFROC 19.6 The distribution of number of lesions and weights 19.7 Other operating characteristics 19.8 Summary 19.9 References", " Chapter 19 RSM predicted operating characteristics 19.1 Introduction The purpose of this chapter is to explain the operating characteristics predicted by the RSM. It relates to Chapter 17 in my book (Chakraborty 2017). This chapter is under development … Also to explain the difference between dataset members (lesionID, lesionWeight) and (lesDist, lesWghtDistr), which are RSM model parameters. 19.2 The distinction between predicted curves and empirical curves Operating characteristics predicted by a model have zero sampling variability. Empirical operating characteristics, which apply to datasets, have non-zero sampling variability. If the model is corect, as the numbers of cases in the dataset increases, the empirical operating characteristic asymptotically approaches the predicted curve. 19.3 The RSM model The 3 RSM parameters and two additional parameters characterizing the dataset determine the wAFROC curve. The 3 RSM parameters are \\(\\mu\\), \\(\\lambda\\) and \\(\\nu\\). The two dataset parameters are: The distribution of number of lesions per diseased case, lesDist. The distribution of lesion weights, lesWghtDistr. These parameters do not apply to individual cases; rather they refer to a large population (asymptotically infinite in size) of cases. str(dataset04$lesionID) #&gt; num [1:100, 1:3] 1 1 1 1 1 1 1 1 1 1 ... str(dataset04$lesionWeight) #&gt; num [1:100, 1:3] 1 1 1 1 1 1 1 1 1 1 ... Note that the first index of both arrays is the case index for the 100 abnormal cases in this dataset. With finite number of cases the empirical operating characteristic (or for that matter any fitted operating characteristic) will have sampling variability as in the following example. 19.4 The empirical wAFROC p &lt;- PlotEmpiricalOperatingCharacteristics(dataset04, opChType = &quot;wAFROC&quot;) p$Plot The piecewise linear nature of the plot, with sharp breaks, indicates that this is due to a finite dataset. In contrast the following code shows a smooth plot, because it is a model predicted plot. 19.5 The predicted wAFROC ## Following example is for mu = 2, lambda = 1, nu = 0.6. 20% of the diseased ## cases have a single lesion, 40% have two lesions, 10% have 3 lesions, ## and 30% have 4 lesions. lesDistr &lt;- rbind(c(1, 0.2), c(2, 0.4), c(3, 0.1), c(4, 0.3)) ## On cases with one lesion the weights are 1, on cases with 2 lesions the weights ## are 0.4 and 0.6, on cases with three lesions the weights are 0.2, 0.3 and 0.5, and ## on cases with 4 lesions the weights are 0.3, 0.4, 0.2 and 0.1: lesWghtDistr &lt;- rbind(c(1, 1.0, -Inf, -Inf, -Inf), c(2, 0.4, 0.6, -Inf, -Inf), c(3, 0.2, 0.3, 0.5, -Inf), c(4, 0.3, 0.4, 0.2, 0.1)) p &lt;- PlotRsmOperatingCharacteristics(mu = 2, lambda = 1, nu = 0.6, OpChType = &quot;wAFROC&quot;, lesDistr = lesDistr, lesWghtDistr = lesWghtDistr, legendPosition = &quot;bottom&quot;, nlfRange = c(0, 1), llfRange = c(0, 1)) p$wAFROCPlot 19.6 The distribution of number of lesions and weights lesDistr #&gt; [,1] [,2] #&gt; [1,] 1 0.2 #&gt; [2,] 2 0.4 #&gt; [3,] 3 0.1 #&gt; [4,] 4 0.3 lesWghtDistr #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] 1 1.0 -Inf -Inf -Inf #&gt; [2,] 2 0.4 0.6 -Inf -Inf #&gt; [3,] 3 0.2 0.3 0.5 -Inf #&gt; [4,] 4 0.3 0.4 0.2 0.1 The second column of lesDistr specifies the fraction of diseased cases with the number of lesions specified in the first column. The first column of lesWghtDistr is a copy of the first column of lesDistr. The remaining non--Inf entries are the weights. For cases with 1 lesion, the weight is 1. For cases with 2 lesions, the first lesion has weight 0.4 and the second lesion has weight 0.6, which sum to unity. For cases with 3 lesions, the respective weights are 0.2, 0.3 and 0.5, which sum to unity. For cases with 4 lesions, the respective weights are 0.3, 0.4, 0.2 and 0.1, which sum to unity. 19.7 Other operating characteristics By changing OpChType one can generate other operating characteristics. Note that lesiion weights argument is not needed for ROC curves. It is only needed for wAFROC and wAFROC1 curves. lesDistr &lt;- rbind(c(1, 0.2), c(2, 0.4), c(3, 0.1), c(4, 0.3)) p &lt;- PlotRsmOperatingCharacteristics(mu = 2, lambda = 1, nu = 0.6, OpChType = &quot;ROC&quot;, lesDistr = lesDistr, legendPosition = &quot;bottom&quot;) p$ROCPlot 19.8 Summary 19.9 References REFERENCES "],
["ImproperROCs.html", "Chapter 20 Improper ROCs 20.1 The binormal model 20.2 Improper ROCs 20.3 Reason for improper ROCs", " Chapter 20 Improper ROCs 20.1 The binormal model The binormal model has two parameters, a and b. The signal (or diseased cases) distribution has unit standard deviation. The noise (or non-diseased cases) distribution has standard deviation b. The a parameter is the separation of the two distributions. 20.2 Improper ROCs Binormal model fits invariably lead to ROC curves that inappropriately cross the chance diagonal, leading to a prediction of a region of the ROC curve where performance is worse than chance, even for expert observers. By convention, such curves are termed improper. This chapter illustrates improper ROCs predicted by the binormal model. aArray &lt;- c(0.7, 0.7, 1.5, 2) bArray &lt;- c(0.5, 1.5, 0.5, 0.5) chance_diag &lt;- data.frame(x = c(0,1), y = c(0,1)) p &lt;- PlotBinormalFit(aArray, bArray) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + theme(legend.position = c(0.85, 0.2)) p &lt;- p + geom_line(data = chance_diag, aes(x = x, y = y), linetype=&quot;dotted&quot;) print(p) The red plot is the clearest example of an improper ROC. This type of curve occurs whenever b &lt; 1. The chance line crossing near the upper right corner, around (0.919,0.919), and the fact that the ROC curve must eventually reach (1, 1) implies the curve must turn upwards as one approaches (1, 1), thereby displaying a “hook”. Whenever b != 1 the hook is there, regardless of whether it is easily visible or not. If b &lt; 1 the hook is near the upper right corner. If b &gt; 1 the hook is near the origin (see green line, corresponding to b = 1.5). With increasing a the hook is less prominent (blue line corresponding to a = 1.5, b = 0.5 and purple line corresponding to a = 2, b = 0.5). But it is there. 20.3 Reason for improper ROCs The reason for the “hook”\" becomes apparent upon examination of the pdfs. #&gt; a = 0.7 , b = 0.5 Since b &lt; 1, the diseased pdf is broader and has a lower peak (since the integral under each distribution is unity) than the non-diseased pdf. Sliding an imaginary threshold to the left, starting from the extreme right, one sees that initially, just below z = 7, the diseased distribution starts being “picked up” while the non-diseased distribution is not “picked up”, causing the ROC to start with infinite slope near the origin (because TPF is increasing while FPF is not). Around z = 2.5 the non-diseased distribution starts being “picked up”, causing the ROC slope to decrease. Around z = -3, almost all of the non-diseased distribution has been “picked up” which means FPF is near unity, but since not all of the broader diseased distribution has been “picked up”, TPF is less than unity. Here is a region where TPF &lt; FPF, meaning the operating point is below the chance diagonal. As the threshold is lowered further, TPF continues to increase, as the rest of the diseased distribution is “picked up” while FPF stays almost constant at unity. In this region, the ROC curve is approaching the upper right corner with almost infinite slope (because TPF is increasing but FPF is not). "],
["DegenerateDatasetsBinormalModel.html", "Chapter 21 Degenerate datasets in the binormal model 21.1 Two helper functions 21.2 Degenerate datasets 21.3 Understanding degenerate datasets 21.4 The exact fit is not unique 21.5 Comments on degeneracy 21.6 A reasonable fit to the degenerate dataset", " Chapter 21 Degenerate datasets in the binormal model 21.1 Two helper functions 21.2 Degenerate datasets Metz defined binormal degenerate data sets as those that result in exact-fit binormal ROC curves of inappropriate shape consisting of a series of horizontal and/or vertical line segments in which the ROC “curve” crosses the chance line. The crossing of the chance line occurs because the degenerate data sets can be fitted exactly by infinite or zero values for the model slope parameter b, and infinite values for the decision thresholds, or both. 21.3 Understanding degenerate datasets To understand this, consider that the non-diseased distribution is a Dirac delta function centered at zero (by definition such a function integrates to unity) and the unit variance diseased distribution is centered at 0.6744898. In other words this binormal model is characterized by a = 0.6744898 and b = 0. What is the expected ROC curve? As the threshold \\(\\zeta\\) is moved from the far right, gradually to the left, TPF will increase but FPF is stuck at zero until the threshold reaches zero. Just before reaching this point, the coordinates of the ROC operating point are (0, 0.75). The 0.75 is due to the fact that z = 0 is -0.6744898 units relative to the center of the diseased distribution, so the area under the diseased distribution below z = 0 is 0.25. Since pnorm is the probability below the threshold, TPF must be its complement, namely 0.75. This explains the operating point (0,0.75), which lies on the y-axis. As the threshold crosses the zero-width delta function, FPF shoots up from 0 to 1, but TPF stays constant. Therefore, the operating point has jumped from (0, 0.75) to (1, 0.75). When the threshold is reduced further, the operating point moves up vertically, along the right side of the ROC plot, until the threshold is so small that virtually all of diseased distribution exceeds it and the operating point reaches (1, 1). The ROC curve is illustrated in plot A. plotOP &lt;- data.frame(FPF = 0, TPF = 0.75) a &lt;- 0.6744898; b &lt;- 0 plotCurve &lt;- BMPoints(a, b) figA &lt;- ggplot(mapping = aes(x = FPF, y = TPF)) + geom_line(data = plotCurve) + geom_point(data = plotOP) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + ggtitle(&quot;A&quot;) print(figA) This is an extreme example of an ROC curve with a “hook”. If the data is such that the only operating point provided by the observer is (0,0.75) then this curve will be an exact fit to the operating point. 21.4 The exact fit is not unique Actually, given one operating point (0, 0.75) the preceding fit is not even unique. If the diseased distribution is shifted appropriately to the right of its previous position, and one can determine the necessary value of a, then the ROC curve will shoot upwards through the operating point (0, 0.75) to (0, 0.9), as in plot B, before proceeding horizontally to (1, 0.9) and then completing the curve to (1, 1). If the diseased distribution is shifted well to the right, i.e., a is very large, then the ROC curve will shoot upwards past the operating point, as in plot C, all the way to (0,1) before proceeding horizontally to (1, 1). a &lt;- 1.281552; b &lt;- 0 plotCurve &lt;- BMPoints(a, b) figB &lt;- ggplot(mapping = aes(x = FPF, y = TPF)) + geom_line(data = plotCurve) + geom_point(data = plotOP) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + ggtitle(&quot;B&quot;) a &lt;- Inf; b &lt;- 0 plotCurve &lt;- BMPoints(a, b) figC &lt;- ggplot(mapping = aes(x = FPF, y = TPF)) + geom_line(data = plotCurve) + geom_point(data = plotOP) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + ggtitle(&quot;C&quot;) print(figB);print(figC) All of these represent exact fits to the observed operating point, with b = 0 and different values of a. Not one of them is reasonable. 21.5 Comments on degeneracy Degeneracy occurs if the observer does not provide any interior operating points. So why worry about it? Perhaps one has a non-cooperating observer, who is not heeding the instructions to spread the ratings, use all the bins. A simple example shows that the observer could if fact be cooperating fully and is still unable to provide any interior data points. Consider 100 diseased cases consisting of 75 easy cases and 25 difficult ones and 100 easy non-diseased cases. The observer is expected to rate the 75 easy diseased cases as fives, the difficult ones as ones and the 100 non-diseased cases are rated ones. No amount of coaxing please, please spread your ratings is going to convince this observer to rate with twos, threes and fours any of the 75 easy diseased cases. If the cases are obviously diseased, and that is what is meant by easy cases, they are supposed to be rated fives: definitely diseased. Forcing them to rate some of them as probably diseased or possibly diseased would be irrational and guilty of bending the reading paradigm to fit the convenience of the researcher (early in his research career, the author used to believe in the existence of non-cooperating observers, so Metz’s advice to spread the ratings did not seem unreasonable at that time). 21.6 A reasonable fit to the degenerate dataset If the dataset yields a single operating point (0, 0.75), what is a reasonable ROC plot? There is a theorem that given an observed operating point, the line connecting that point to (1, 1) represents a lower bound on achievable performance by the observer. The observer using a guessing mechanism to classify the remaining cases achieves the lower bound. Here is an explanation of this theorem. Having rated the 75 easy diseased cases as fives, the observer is left with 25 diseased cases and 100 non-diseased cases, all of which appear definitely non-diseased to the observer. Suppose the observer randomly rates 20% of the remaining cases as fours. This would pick up five of the actually diseased cases and 20 non-diseased ones. Therefore, the total number of diseased cases rated four or higher is 80, and the corresponding number of non-diseased cases is 20. The new operating point of the observer is (0.20, 0.80). Now, one has two operating points, the original one on the y-axis at (0, 0.75) and an interior point (0.20, 0.80). Next, instead of randomly rating 20% of the remaining cases as fours, the observer rates 40% of them as fours, then the interior point would have been (0.40, 0.85). The reader can appreciate that simply by increasing the fraction of remaining cases that are randomly rated fours, the observer can move the operating point along the straight line connecting (0, 0.75) and (1, 1), as in plot D. Since a guessing mechanism is being used, this must represent a lower bound on performance. The resulting ROC curve is proper and the net AUC = 0.875. mu &lt;- Inf; alpha &lt;- 0.75 plotCurve &lt;- CBMPoints(mu, alpha) figD &lt;- ggplot(mapping = aes(x = FPF, y = TPF)) + geom_line(data = plotCurve) + geom_point(data = plotOP) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + ggtitle(&quot;D&quot;) print(figD) For this dataset this is in fact the fit yielded by the contaminated binormal model (CBM) and the radiological search model (RSM). Why should one select the lowest possible performance consistent with the data? Because it yields a unique value for performance: any higher performance would not be unique. "],
["ProperROCs.html", "Chapter 22 Proper ROCs 22.1 Helper functions 22.2 Definitions of PROPROC parameters in terms of binormal model parameters 22.3 Main code and output 22.4 Discussion", " Chapter 22 Proper ROCs 22.1 Helper functions 22.2 Definitions of PROPROC parameters in terms of binormal model parameters \\[\\begin{eqnarray*} c &amp; = &amp; \\frac{b-1}{b+1}\\\\ d_a &amp; = &amp; \\frac{\\sqrt{2}a}{\\sqrt{1+{b^{2}}}} \\end{eqnarray*}\\] 22.3 Main code and output c1Arr &lt;- c(-0.1322804, 0.2225588); daArr &lt;- c(1.197239, 1.740157) myLabel &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;) myLabelIndx &lt;- 1 for (i in 1:2) { c1 &lt;- c1Arr[i] da &lt;- daArr[i] ret &lt;- Transform2ab(da, c1) a &lt;- ret$a;b &lt;- ret$b if (i == 1) z &lt;- seq(-3, 0, by = 0.01) # may need to adjust limits to view detail of slope plot if (i == 2) z &lt;- seq(-3, 5, by = 0.01) # may need to adjust limits to view detail of slope plot FPF &lt;- seq(0.0, 1, 0.001) TPF &lt;- rocY(FPF, a, b) rocPlot &lt;- data.frame(FPF = FPF, TPF = TPF) plotRoc &lt;- ggplot(rocPlot, aes(x = FPF, y = TPF)) + geom_line() + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + ggtitle(myLabel[myLabelIndx]);myLabelIndx &lt;- myLabelIndx + 1 slope &lt;-b*dnorm(a-b*z)/dnorm(-z) # same as likelihood ratio slopePlot &lt;- data.frame(z = z, slope = slope) p &lt;- ggplot(slopePlot, aes(x = z, y = slope)) + geom_line() + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + ggtitle(myLabel[myLabelIndx]);myLabelIndx &lt;- myLabelIndx + 1 print(plotRoc);print(p) } 22.4 Discussion Plot A is for c1 = -0.1322804, da = 1.197239 while plot C is for c1 = 0.2225588, da = 1.740157. Plots B and D are the corresponding slope plots as functions of the binormal model z-sample. In plot A, the slope is infinite near the origin and the curve approaches the upper-right corner with finite slope. The situation is reversed in plot C where the slope is finite near the origin and the curve approaches the upper-right corner with zero slope. These two readers are from a clinical dataset, dataset01. Highest rating inferred ROC data from original FROC data, were analyzed by PROPROC and the resulting parameter values are coded here. They were chosen as they demonstrate key differences in the shapes of proper ROC plots. Plot A corresponds to a negative value of c1, which implies b &lt; 1. The slope of the proper ROC is infinite near the origin and approaches a positive constant near the upper right corner of the ROC. Plot C is for a positive value of c1, i.e., for b &gt; 1. Now the slope of the proper ROC is finite near the origin and approaches zero near the upper right corner. Considering plot D, as one “cuts” the slope axis horizontally with a sliding threshold, starting with very high values and moving downwards, the slope of the ROC curve starts at the origin with a large but finite value. This corresponds to the peak in plot D. Above the peak, there are no solutions for z. The slope decreases monotonically to zero, corresponding to the flattening out of the slope at zero for z ~ -2. The two values of z corresponding to each cut implies, of course, that the binormal model based proper algorithm has to do a lot of bookkeeping, since each horizontal cut splits the decision axis into 3 regions. One can think of shrinking each of plots B &amp; D horizontally to zero width, and all that remains is the slope axis with a thick vertical line superimposed on it, corresponding to the horizontally collapsed curves. In plot B the vertical line extends from positive infinity down to about 0.1, and represents the range of decision variable samples encountered by the observer on the likelihood ratio scale. In plot D the vertical line extends from a finite value (~9.4) to zero. For the stated binormal model parameters values outside of these ranges are not possible. "],
["MetzEqn36.html", "Chapter 23 Metz Eqn36 numerical check 23.1 Helper functions 23.2 Main code and output 23.3 Discussion", " Chapter 23 Metz Eqn36 numerical check 23.1 Helper functions 23.2 Main code and output npts &lt;- 10000 for (i in 1:2) { for (j in 1:5) { C &lt;- c1[i,j] da &lt;- d_a1[i,j] ret &lt;- GetLimits(da,C) LL &lt;- ret$LL;UL &lt;- ret$UL vc &lt;- seq (LL, UL, length.out = npts) TPF &lt;- TruePositiveFraction (vc, da, C) FPF &lt;- FalsePositiveFraction (vc, da, C) FPF &lt;- rev(FPF);TPF &lt;- rev(TPF) df2 &lt;- data.frame(FPF = FPF, TPF = TPF) # do integral numerically numAuc &lt;- trapz(FPF, TPF) # Implement Eqn. 36 from Metz-Pan paper rho &lt;- -(1-C^2)/(1+C^2);sigma &lt;- rbind(c(1, rho), c(rho, 1)) lower &lt;- rep(-Inf,2);upper &lt;- c(-da/sqrt(2),0) aucProproc &lt;- pnorm(da/sqrt(2)) + 2 * pmvnorm(lower, upper, sigma = sigma) aucProproc &lt;- as.numeric(aucProproc) cat(&quot;i = &quot;, i,&quot;j = &quot;, j,&quot;C = &quot;, C, &quot;, da = &quot;, da, &quot;aucProproc =&quot;, aucProproc, &quot;Norm. Diff. = &quot;, (aucProproc-numAuc)/aucProproc,&quot;\\n&quot;) } } #&gt; i = 1 j = 1 C = -0.1322804 , da = 1.197239 aucProproc = 0.8014164 Norm. Diff. = 3.520017e-08 #&gt; i = 1 j = 2 C = -0.08696513 , da = 1.771176 aucProproc = 0.8947898 Norm. Diff. = 4.741875e-08 #&gt; i = 1 j = 3 C = -0.1444419 , da = 1.481935 aucProproc = 0.8526605 Norm. Diff. = 3.515431e-08 #&gt; i = 1 j = 4 C = 0.08046016 , da = 1.513757 aucProproc = 0.8577776 Norm. Diff. = 4.971428e-08 #&gt; i = 1 j = 5 C = 0.2225588 , da = 1.740157 aucProproc = 0.8909392 Norm. Diff. = 2.699855e-08 #&gt; i = 2 j = 1 C = -0.08174248 , da = 0.6281251 aucProproc = 0.6716574 Norm. Diff. = 2.801793e-08 #&gt; i = 2 j = 2 C = 0.04976448 , da = 0.9738786 aucProproc = 0.7544739 Norm. Diff. = 5.275242e-08 #&gt; i = 2 j = 3 C = -0.1326126 , da = 1.155871 aucProproc = 0.7931787 Norm. Diff. = 3.472577e-08 #&gt; i = 2 j = 4 C = 0.1182226 , da = 1.620176 aucProproc = 0.8740274 Norm. Diff. = 3.922161e-08 #&gt; i = 2 j = 5 C = 0.0781033 , da = 0.8928816 aucProproc = 0.7360989 Norm. Diff. = 3.798459e-08 23.3 Discussion Note the close correspondence between the formula, Eqn. 36 in the Metz-Pan paper and the numerical estimate. As a historical note, Eqn. 31 and Eqn. 36 (they differ only in parameterizations) in the referenced publication are provided without proof – it was probably obvious to Prof Metz or he wanted to leave it to us “mere mortals” to figure it out, as a final parting gesture of his legacy. The author once put a significant effort into proving it and even had a bright graduate student from the biostatistics department work on it to no avail. The author has observed that these equations always yield very close to the numerical estimates, to within numerical precisions, so the theorem is correct empirically, but he has been unable to prove it analytically. It is left as an exercise for a gifted reader to prove/disprove these equations. "],
["CbmPlots.html", "Chapter 24 CBM Plots 24.1 Helper functions 24.2 Main code and output 24.3 Comments 24.4 pdf plots 24.5 Comments 24.6 likelihood ratio plots 24.7 Comments", " Chapter 24 CBM Plots 24.1 Helper functions 24.2 Main code and output #&gt; Fig. A : mu = 1 , alpha = 0.2 #&gt; Fig. B : mu = 3 , alpha = 0.2 #&gt; Fig. C : mu = 1 , alpha = 0.8 #&gt; Fig. D : mu = 3 , alpha = 0.8 24.3 Comments Plots A - D show ROC curves predicted by the CBM model; the corresponding values of the \\(mu\\) and \\(alpha\\) parameters are indicated above the plots. For small \\(mu\\) and/or \\(alpha\\) the curve approaches the chance diagonal, consistent with the notion that if the lesion is not visible, performance can be no better than chance level. 24.4 pdf plots #&gt; Fig. E : mu = 1 , alpha = 0.2 #&gt; Fig. F : mu = 3 , alpha = 0.2 #&gt; Fig. G : mu = 1 , alpha = 0.8 #&gt; Fig. H : mu = 3 , alpha = 0.8 24.5 Comments The dark line is the diseased distribution. The grey line is the non-diseased distribution. The bimodal diseased distribution is clearly evident in plots F and H. 24.6 likelihood ratio plots #&gt; Fig. I : mu = 1 , alpha = 0.2 #&gt; Fig. J : mu = 3 , alpha = 0.2 #&gt; Fig. K : mu = 1 , alpha = 0.8 #&gt; Fig. L : mu = 3 , alpha = 0.8 24.7 Comments Close examination of the region near the flat part shows it does not plateau at zero; rather the minimum is at 1 - \\(alpha\\), explaining the non-zero limiting slope of the predicted curve near (1, 1). "],
["ROIDataStr.html", "Chapter 25 ROI paradigm data 25.1 Introduction; this chapter is under construction! 25.2 An example ROI dataset 25.3 The ROI Excel data file 25.4 Next, TBA 25.5 References", " Chapter 25 ROI paradigm data 25.1 Introduction; this chapter is under construction! In the region-of-interest (ROI) paradigm (Obuchowski 1997, @RN55) each case is regarded as consisting of \\({Q_{k}}\\) (\\({Q_{k}}\\ge 1\\)) “quadrants” or “regions-of-interest” or ROIs, where k is the case index (\\(k=1,2,...,K\\)) and \\(K\\) is the total number of cases (i.e., case-level non-diseased plus case-level diseased cases). Each ROI needs to be classified, by the investigator, as either ROI-level-non-diseased (i.e., it has no lesions) or ROI-level-diseased (i.e., it has at least one lesion). Note the distinction between case-level and ROI-level truth states. One can have ROI-level non-diseased regions in a case-level diseased case. A case-level diseased case must contain at least one ROI-level diseased region and a case-level non-diseased case cannot have any ROI-level diseased regions. The observer gives a single rating (in fact an ordered label) to each ROI, denoted \\({R_{kr}}\\) (\\(r\\) = 1, 2, …, \\({Q_k}\\)). Here \\(r\\) is the ROI index and \\(k\\) is the case index. The rating can be an integer or quasi- continuous (e.g., 0 – 100), or a floating point value, as long as higher numbers represent greater confidence in presence of one or more lesions in the ROI. The ROI paradigm is not restricted to 4 or even a constant number of ROIs per case. That is the reason for the k subscript in \\({Q_k}\\). The ROI data structure is a special case of the FROC data structure, the essential difference being that the number of ratings per case is an a-priori known value, equal to \\({Q_{k}}\\). ROI-level non-diseased region ratings are stored in the NL field and ROI-level diseased region ratings are stored in the LL field. One can think of the ROI paradigm as similar to the FROC paradigm, but with localization accuracy restricted to belonging to a region (one cannot distinguish multiple lesions within a region). Unlike the FROC paradigm, a rating is required for every ROI. 25.2 An example ROI dataset An example simulated ROI dataset is included as datasetROI. str(datasetROI) #&gt; List of 8 #&gt; $ NL : num [1:2, 1:5, 1:90, 1:4] 0.95 0.927 0.556 0.805 1.421 ... #&gt; $ LL : num [1:2, 1:5, 1:40, 1:4] 1.57 2.31 2.3 2.34 2.34 ... #&gt; $ lesionVector: int [1:40] 2 3 2 2 3 3 1 2 3 3 ... #&gt; $ lesionID : num [1:40, 1:4] 2 1 1 1 1 2 4 1 1 1 ... #&gt; $ lesionWeight: num [1:40, 1:4] 0.5 0.333 0.5 0.5 0.333 ... #&gt; $ dataType : chr &quot;ROI&quot; #&gt; $ modalityID : Named chr [1:2] &quot;1&quot; &quot;2&quot; #&gt; ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;1&quot; &quot;2&quot; #&gt; $ readerID : Named chr [1:5] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... #&gt; ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... datasetROI$NL[1,1,1,] #&gt; [1] 0.9498680 -0.0582497 -0.7763780 0.0120730 mean(datasetROI$NL[,,1:50,]) #&gt; [1] 0.1014348 datasetROI$NL[1,1,51,] #&gt; [1] 1.01867 0.34710 -Inf -Inf datasetROI$lesionVector[1] #&gt; [1] 2 datasetROI$LL[1,1,1,] #&gt; [1] 1.56928 2.05945 -Inf -Inf x &lt;- datasetROI$LL;mean(x[is.finite(x)]) #&gt; [1] 1.815513 Examination of the output reveals that: This is a 2-treatment 5-reader dataset, with 50 non-diseased cases and 40 diseased cases, and \\({Q_k}=4\\) for all k. For treatment 1, reader 1, case 1 (the 1st non-diseased case) the 4 ratings are 0.949868, -0.0582497, -0.776378, 0.012073. The mean of all ratings on non-diseased cases is 0.1014348. For treatment 1, reader 1, case 51 (the 1st diseased case) the NL ratings are 1.01867, 0.3471. There are only two finite values because this case has two ROI-level-diseased regions, and 2 plus 2 makes for the assumed 4-regions per case. The corresponding $lesionVector field is 2. The ratings of the 2 ROI-level-diseased ROIs on this case are 1.56928, 2.05945. The mean rating over all ROI-level-diseased ROIs is 1.8155127. 25.3 The ROI Excel data file An Excel file in JAFROC format containing simulated ROI data corresponding to datasetROI, is included with the distribution. The first command (below) finds the location of the file and the second command reads it and saves it to a dataset object ds. !!!DPC!!! The DfReadDataFile function automatically recognizes that this is an ROI dataset. Its structure is similar to the JAFROC format Excel file, with some important differences, noted below. It contains three worksheets: ## fileName &lt;- system.file( ## &quot;extdata&quot;, &quot;RoiData.xlsx&quot;, package = &quot;RJafroc&quot;, mustWork = TRUE) ## ds &lt;- DfReadDataFile(fileName) ## ds$dataType FIGURE 25.1: Fig. 1 two views of Truth worksheet The Truth worksheet, Fig. 1, indicates which cases are diseased and which are non-diseased and the number of ROI-level-diseased region on each case. There are 50 non-diseased cases (labeled 1-50) under column CaseID and 40 diseased cases (labeled 51-90). The LesionID field for each non-diseased case (e.g., CaseID = 1) is zero and there is one row per case. For diseased cases, this field has a variable number of entries, ranging from 1 to 4. As an example, there are two rows for CaseID = 51 in the Excel file: one with LesionID = 2 and one with LesionID = 3. The Weights field is always zero (this field is not used in ROI analysis). FIGURE 25.2: Fig. 2 two views of FP worksheet The FP (or NL) worksheet - this lists the ratings of ROI-level-non-diseased regions. For ReaderID = 1, ModalityID = 1 and CaseID = 1 there are 4 rows, corresponding to the 4 ROI-level-non-diseased regions in this case. The corresponding ratings are 0.949868, -0.0582497, -0.776378, 0.012073. The pattern repeats for other treatments and readers, but the rating are, of course, different. Each CaseID is represented in the FP worksheet (a rare exception could occur if a case-level diseased case has 4 diseased regions). FIGURE 25.3: Fig. 2 TP worksheet The TP (or LL) worksheet - this lists the ratings of ROI-level-diseased regions. Because non-diseased cases generate TPs, one does not find any entry with CaseID = 1-50 in the TP worksheet. The lowest CaseID in the TP worksheet is 51, which corresponds to the first diseased case. There are two entries for this case, corresponding to the two ROI-level-diseased regions present in this case. Recall that corresponding to this CaseID in the Truth worksheet there were two entries with LesionID = 2 and 3. These must match the LesionID’s listed for this case in the TP worksheet. Complementing these two entries, in the FP worksheet for CaseID = 51, there are 2 entries corresponding to the two ROI-level-non-diseased regions in this case. One should confirm that for each diseased case the sum of the number of entries in the TP and FP worksheets is always 4. 25.4 Next, TBA The next chapter illustrates significance testing for this paradigm. 25.5 References REFERENCES "],
["ROIDataAnalysis.html", "Chapter 26 Analyzing data acquired according to the ROI paradigm 26.1 Introduction; this chapter is under construction! 26.2 Note to self (10/29/19) !!!DPC!!! 26.3 Introduction 26.4 The ROI figure of merit 26.5 Calculation of the ROI figure of merit. 26.6 Significance testing 26.7 Summary 26.8 References", " Chapter 26 Analyzing data acquired according to the ROI paradigm 26.1 Introduction; this chapter is under construction! 26.2 Note to self (10/29/19) !!!DPC!!! The FOM and DeLong method implementations need checking with a toy dataset. 26.3 Introduction For an ROI dataset StSignificanceTesting() automatically defaults to method = \"ORH\", covEstMethod = \"DeLong\" and FOM = \"ROI\". The covariance estimation method is based on the original DeLong method (DeLong, DeLong, and Clarke-Pearson 1988), which is valid only for the trapezoidal AUC, i.e. ROC data, as extended by (Obuchowski 1997) to ROI data, see formula below. The essential differences from conventional ROC analyses are in the definition of the ROI figure of merit, see below, and the procedure developed by (Obuchowski 1997) for estimating the covariance matrix. Once the covariances are known, method = \"ORH\" can be applied to perform significance testing, as described in (Obuchowski and Rockette 1995) and (Chakraborty 2017, Chapter 10). 26.4 The ROI figure of merit Let \\({X_{kr}}\\) denote the rating for the rth lesion-containing ROI in the kth case and let \\(n_{k}^L\\) be the total number of lesion-containing ROIs in the kth case. Similarly, let \\({Y_{kr}}\\) denote the rating for the rth lesion-free ROI in the kth case and \\(n_{k}^N\\) denote the total number of lesion-free ROIs in the kth case. Let \\({N_L}\\) denote the total number of lesion-containing ROIs in the image set and \\(N_N\\) denote the total number of lesion-free ROIs. These are given by: \\[N_L=\\sum\\nolimits_{k}{n_{k}^L}\\] and \\[N_N=\\sum\\nolimits_{k}{n_{k}^N}\\] The ROI figure of merit \\(\\theta\\) is defined by: \\[\\begin{equation*} \\theta =\\frac{1}{N_LN_N}\\sum\\nolimits_k{\\sum\\nolimits_{k&#39;}{\\sum\\limits_{r=1}^{n_{k}^{L}}{\\sum\\limits_{r&#39;=1}^{n_{k&#39;}^{N}}{\\psi (X_{kr},{Y_{k&#39;r&#39;}})}}}} \\end{equation*}\\] The kernel function \\(\\Psi(X,Y)\\) is defined by: \\[\\begin{equation*} \\psi\\left ( X,Y \\right ) =\\begin{bmatrix} 1 &amp; \\text{if}&amp; {X &lt; Y}\\\\ 0.5 &amp; \\text{if}&amp; {X = Y}\\\\ 0 &amp; \\text{if}&amp; {X &gt; Y} \\end{bmatrix} \\end{equation*}\\] The ROIs are effectively regarded as mini-cases and one calculates the FOM as the Wilcoxon statistic considering the mini-cases as actual cases. The correlations between the ratings of ROIs on the same case are accounted for in the analysis. 26.5 Calculation of the ROI figure of merit. UtilFigureOfMerit(datasetROI, FOM = &quot;ROI&quot;) #&gt; Rdr1 Rdr2 Rdr3 Rdr4 Rdr5 #&gt; Trt1 0.9057239 0.8842834 0.8579279 0.9350207 0.8352103 #&gt; Trt2 0.9297186 0.9546035 0.8937652 0.9531716 0.8770076 fom &lt;- UtilFigureOfMerit(datasetROI, FOM = &quot;ROI&quot;) If the correct FOM is not supplied, it defaults to FOM = ROI. This is a 2-treatment 5-reader dataset. For treatment 1, reader 1 the figure of merit is 0.9057239. For treatment 2, reader 5 the figure of merit is 0.8770076. Etc. 26.6 Significance testing When dataset$dataType == \"ROI\" the FOM defaults to “ROI” (meaning the above formula) and the covariance estimation method defaults to covEstMethod = \"DeLong\". ret &lt;- StSignificanceTesting(datasetROI, FOM = &quot;Wilcoxon&quot;) #&gt; ROI dataset: forcing method = `ORH`, covEstMethod = `DeLong` and FOM = `ROI`. str(ret) #&gt; List of 14 #&gt; $ fomArray : num [1:2, 1:5] 0.906 0.93 0.884 0.955 0.858 ... #&gt; ..- attr(*, &quot;dimnames&quot;)=List of 2 #&gt; .. ..$ : chr [1:2] &quot;Trt1&quot; &quot;Trt2&quot; #&gt; .. ..$ : chr [1:5] &quot;Rdr1&quot; &quot;Rdr2&quot; &quot;Rdr3&quot; &quot;Rdr4&quot; ... #&gt; $ meanSquares :&#39;data.frame&#39;: 1 obs. of 3 variables: #&gt; ..$ msT : num 0.00361 #&gt; ..$ msR : num 0.00256 #&gt; ..$ msTR: num 0.000207 #&gt; $ varComp :&#39;data.frame&#39;: 1 obs. of 6 variables: #&gt; ..$ varR : num 0.00108 #&gt; ..$ varTR: num 0.000153 #&gt; ..$ cov1 : num 0.000247 #&gt; ..$ cov2 : num 0.000187 #&gt; ..$ cov3 : num 0.000154 #&gt; ..$ var : num 0.000333 #&gt; $ FTestStatsRRRC :&#39;data.frame&#39;: 1 obs. of 4 variables: #&gt; ..$ fRRRC : num 9.76 #&gt; ..$ ndfRRRC: num 1 #&gt; ..$ ddfRRRC: num 12.8 #&gt; ..$ pRRRC : num 0.00817 #&gt; $ ciDiffTrtRRRC :&#39;data.frame&#39;: 1 obs. of 8 variables: #&gt; ..$ Treatment: chr &quot;Trt1-Trt2&quot; #&gt; ..$ Estimate : num -0.038 #&gt; ..$ StdErr : num 0.0122 #&gt; ..$ DF : num 12.8 #&gt; ..$ t : num -3.12 #&gt; ..$ PrGTt : num 0.00817 #&gt; ..$ CILower : num -0.0643 #&gt; ..$ CIUpper : num -0.0117 #&gt; $ ciAvgRdrEachTrtRRRC :&#39;data.frame&#39;: 2 obs. of 6 variables: #&gt; ..$ Treatment: Factor w/ 2 levels &quot;Trt1&quot;,&quot;Trt2&quot;: 1 2 #&gt; ..$ Area : num [1:2] 0.884 0.922 #&gt; ..$ StdErr : num [1:2] 0.0232 0.0197 #&gt; ..$ DF : num [1:2] 12.2 10.1 #&gt; ..$ CILower : num [1:2] 0.833 0.878 #&gt; ..$ CIUpper : num [1:2] 0.934 0.966 #&gt; $ FTestStatsFRRC :&#39;data.frame&#39;: 1 obs. of 4 variables: #&gt; ..$ fFRRC : num 16.6 #&gt; ..$ ndfFRRC: num 1 #&gt; ..$ ddfFRRC: num Inf #&gt; ..$ pFRRC : num 4.58e-05 #&gt; $ ciDiffTrtFRRC :&#39;data.frame&#39;: 1 obs. of 8 variables: #&gt; ..$ Treatment: chr &quot;Trt1-Trt2&quot; #&gt; ..$ Estimate : num -0.038 #&gt; ..$ StdErr : num 0.00933 #&gt; ..$ DF : num Inf #&gt; ..$ t : num -4.08 #&gt; ..$ PrGTt : num 4.58e-05 #&gt; ..$ CILower : num -0.0563 #&gt; ..$ CIUpper : num -0.0197 #&gt; $ ciAvgRdrEachTrtFRRC :&#39;data.frame&#39;: 2 obs. of 6 variables: #&gt; ..$ Treatment: Factor w/ 2 levels &quot;Trt1&quot;,&quot;Trt2&quot;: 1 2 #&gt; ..$ Area : num [1:2] 0.884 0.922 #&gt; ..$ StdErr : num [1:2] 0.0163 0.0129 #&gt; ..$ DF : num [1:2] Inf Inf #&gt; ..$ CILower : num [1:2] 0.852 0.896 #&gt; ..$ CIUpper : num [1:2] 0.916 0.947 #&gt; $ ciDiffTrtEachRdrFRRC:&#39;data.frame&#39;: 5 obs. of 9 variables: #&gt; ..$ Reader : Factor w/ 5 levels &quot;Rdr1&quot;,&quot;Rdr2&quot;,..: 1 2 3 4 5 #&gt; ..$ Treatment: Factor w/ 1 level &quot;Trt1-Trt2&quot;: 1 1 1 1 1 #&gt; ..$ Estimate : num [1:5] -0.024 -0.0703 -0.0358 -0.0182 -0.0418 #&gt; ..$ StdErr : num [1:5] 0.01025 0.01448 0.01648 0.00928 0.01398 #&gt; ..$ DF : num [1:5] Inf Inf Inf Inf Inf #&gt; ..$ t : num [1:5] -2.34 -4.86 -2.17 -1.96 -2.99 #&gt; ..$ PrGTt : num [1:5] 1.93e-02 1.20e-06 2.97e-02 5.05e-02 2.79e-03 #&gt; ..$ CILower : num [1:5] -0.0441 -0.0987 -0.0681 -0.0363 -0.0692 #&gt; ..$ CIUpper : num [1:5] -3.90e-03 -4.19e-02 -3.53e-03 3.88e-05 -1.44e-02 #&gt; $ varCovEachRdr :&#39;data.frame&#39;: 5 obs. of 3 variables: #&gt; ..$ Reader: Factor w/ 5 levels &quot;Rdr1&quot;,&quot;Rdr2&quot;,..: 1 2 3 4 5 #&gt; ..$ Var : num [1:5] 0.000269 0.000227 0.000481 0.000168 0.000522 #&gt; ..$ Cov1 : num [1:5] 0.000216 0.000122 0.000345 0.000125 0.000424 #&gt; $ FTestStatsRRFC :&#39;data.frame&#39;: 1 obs. of 4 variables: #&gt; ..$ fRRFC : num 17.5 #&gt; ..$ ndfRRFC: num 1 #&gt; ..$ ddfRRFC: num 4 #&gt; ..$ pRRFC : num 0.0139 #&gt; $ ciDiffTrtRRFC :&#39;data.frame&#39;: 1 obs. of 8 variables: #&gt; ..$ Treatment: chr &quot;Trt1-Trt2&quot; #&gt; ..$ Estimate : num -0.038 #&gt; ..$ StdErr : num 0.00909 #&gt; ..$ DF : num 4 #&gt; ..$ t : num -4.18 #&gt; ..$ PrGTt : num 0.0139 #&gt; ..$ CILower : num -0.0633 #&gt; ..$ CIUpper : num -0.0128 #&gt; $ ciAvgRdrEachTrtRRFC :&#39;data.frame&#39;: 2 obs. of 6 variables: #&gt; ..$ Treatment: Factor w/ 2 levels &quot;Trt1&quot;,&quot;Trt2&quot;: 1 2 #&gt; ..$ Area : num [1:2] 0.884 0.922 #&gt; ..$ StdErr : num [1:2] 0.0175 0.0157 #&gt; ..$ DF : num [1:2] 4 4 #&gt; ..$ CILower : num [1:2] 0.835 0.878 #&gt; ..$ CIUpper : num [1:2] 0.932 0.965 While ret is a list with many (22) members, their meanings should be clear from the notation. As an example: The variance components are given by: ret$varComp #&gt; varR varTR cov1 cov2 cov3 var #&gt; 1 0.001082359 0.0001526084 0.0002465125 0.0001870571 0.0001543764 0.0003333119 26.6.1 RRRC analysis ret$FTestStatsRRRC$fRRRC #&gt; [1] 9.763602 ret$FTestStatsRRRC$ndfRRRC #&gt; [1] 1 ret$FTestStatsRRRC$ddfRRRC #&gt; [1] 12.82259 ret$FTestStatsRRRC$pRRRC #&gt; [1] 0.008173042 The F-statistic is , with ndf = 1 and ddf = , which yields a p-value of . The confidence interval for the reader averaged difference between the two treatments is given by: ret$ciDiffTrtRRRC #&gt; Treatment Estimate StdErr DF t PrGTt CILower #&gt; 1 Trt1-Trt2 -0.03802005 0.01216768 12.82259 -3.124676 0.008173042 -0.06434373 #&gt; CIUpper #&gt; 1 -0.01169636 The FOM difference (treatment 1 minus 2) is -0.03802, which is significant, p-value = 0.008173, F-statistic = 9.7636016, ddf = 12.8225898. The confidence interval is (-0.0643437, -0.0116964). 26.6.2 FRRC analysis ret$FTestStatsFRRC$fFRRC #&gt; [1] 16.6135 ret$FTestStatsFRRC$ndfFRRC #&gt; [1] 1 ret$FTestStatsFRRC$ddfFRRC #&gt; [1] Inf ret$FTestStatsFRRC$pFRRC #&gt; [1] 4.582365e-05 The F-statistic is 16.6135014, with ndf = 1 and ddf = Inf, which yields a p-value of 4.5823651^{-5}. The confidence interval for the reader averaged difference between the two treatments is given by: ret$ciDiffTrtFRRC #&gt; Treatment Estimate StdErr DF t PrGTt CILower #&gt; 1 Trt1-Trt2 -0.03802005 0.009327861 Inf -4.075966 4.582365e-05 -0.05630232 #&gt; CIUpper #&gt; 1 -0.01973778 26.6.3 RRFC analysis ret$FTestStatsRRFC$fRRFC #&gt; [1] 17.48107 ret$FTestStatsRRFC$ndfRRFC #&gt; [1] 1 ret$FTestStatsRRFC$ddfRRFC #&gt; [1] 4 ret$FTestStatsRRFC$pRRFC #&gt; [1] 0.01390667 The F-statistic is 17.4810666, with ndf = 1 and ddf = 4, which yields a p-value of 0.0139067. The confidence interval for the reader averaged difference between the two treatments is given by: ret$ciDiffTrtRRFC #&gt; Treatment Estimate StdErr DF t PrGTt CILower #&gt; 1 Trt1-Trt2 -0.03802005 0.00909345 4 -4.181037 0.01390667 -0.06326751 #&gt; CIUpper #&gt; 1 -0.01277258 26.7 Summary TBA 26.8 References REFERENCES "],
["myEquations.html", "Chapter 27 EQUATIONS", " Chapter 27 EQUATIONS \\[\\begin{equation*} \\theta =\\frac{1}{N_LN_N}\\sum\\nolimits_k{\\sum\\nolimits_{k&#39;}{\\sum\\limits_{r=1}^{n_{k}^{L}}{\\sum\\limits_{r&#39;=1}^{n_{k&#39;}^{N}}{\\psi (X_{kr},{Y_{k&#39;r&#39;}})}}}} \\end{equation*}\\] \\[\\begin{equation*} \\frac{d}{dx}\\left( \\int_{a}^{x} f(u)\\,du\\right)=f(x) \\end{equation*}\\] \\[\\begin{equation*} \\theta =\\frac{1}{N_L N_N} \\end{equation*}\\] "],
["references-16.html", "REFERENCES", " REFERENCES Chakraborty, Dev P. 1989. “Maximum Likelihood Analysis of Free-Response Receiver Operating Characteristic (Froc) Data.” Journal Article. Medical Physics 16 (4): 561–68. ———. 2017. Observer Performance Methods for Diagnostic Imaging - Foundations, Modeling, and Applications with R-Based Examples. Book. Boca Raton, FL: CRC Press. DeLong, E. R., D. M. DeLong, and D. L. Clarke-Pearson. 1988. “Comparing the Areas Under Two or More Correlated Receiver Operating Characteristic Curves: A Nonparametric Approach.” Journal Article. Biometrics 44: 837–45. Franken, Jr., Edmund A., Kevin S. Berbaum, Susan M. Marley, Wilbur L. Smith, Yutaka Sato, Simon C. S. Kao, and Steven G. Milam. 1992. “Evaluation of a Digital Workstation for Interpreting Neonatal Examinations: A Receiver Operating Characteristic Study.” Journal Article. Investigative Radiology 27 (9): 732–37. http://journals.lww.com/investigativeradiology/Fulltext/1992/09000/Evaluation_of_a_Digital_Workstation_for.16.aspx. Hillis, Stephen L., and K. S. Berbaum. 2004. “Power Estimation for the Dorfman-Berbaum-Metz Method.” Journal Article. Acad. Radiol. 11 (11): 1260–73. ICRU. 2008. “Statistical Analysis and Power Estimation.” Book Section. In JOURNAL of the Icru, 8:37–40. https://doi.org/10.1093/jicru/ndn012. Metz, C. E. 1978. “Basic Principles of Roc Analysis.” Journal Article. Seminars in Nuclear Medicine 8 (4): 283–98. Obuchowski, Nancy A. 1997. “Nonparametric Analysis of Clustered Roc Curve Data.” Journal Article. Biometrics 53: 567–78. Obuchowski, Nancy A., Michael L. Lieber, and Kimerly A. Powell. 2000. “Data Analysis for Detection and Localization of Multiple Abnormalities with Application to Mammography.” Journal Article. Acad. Radiol. 7 (7): 516–25. Obuchowski, N. A., and H. E. Rockette. 1995. “Hypothesis Testing of the Diagnostic Accuracy for Multiple Diagnostic Tests: An Anova Approach with Dependent Observations.” Journal Article. Communications in Statistics: Simulation and Computation 24: 285–308. Zanca, Federica, Jurgen Jacobs, Chantal Van Ongeval, Filip Claus, Valerie Celis, Catherine Geniets, Veerle Provost, Herman Pauwels, Guy Marchal, and Hilde Bosmans. 2009. “Evaluation of Clinical Image Processing Algorithms Used in Digital Mammography.” Journal Article. Medical Physics 36 (3): 765–75. "]
]
