[
["ORHAnalysis.html", "Chapter 8 Obuchowski Rockette Hillis (ORH) Analysis 8.1 Introduction 8.2 Single-reader multiple-treatment model 8.3 Significance testing 8.4 Multiple-reader multiple-treatment ORH model 8.5 Discussion/Summary 8.6 References", " Chapter 8 Obuchowski Rockette Hillis (ORH) Analysis 8.1 Introduction The previous chapter described the DBM significance testing procedure5 for analyzing MRMC ROC data, along with improvements suggested by Hillis. Because the method depends on the assumption that jackknife pseudovalues can be regarded as independent and identically distributed case-level figures of merit, it has been criticized by Hillis who states that the method “works” but lacks firm statistical foundations (Hillis et al. 2005; Hillis 2007; Hillis, Berbaum, and Metz 2008). I believes that if a method “works” there must be good reasons why it “works” and the last section of the previous chapter, §9.13, gave a justification for why the method “works”. Specifically, the empirical AUC pseudovalues qualify as case-level FOM-like quantities; this property was also noted in 1997 by Hanley and Hajian-Tilaki (Hajian-Tilaki et al. 1997). However, this justification only applies to the empirical AUC, so an alternate approach is desirable. This chapter presents Hillis’ preferred alternative to the DBMH approach. He has argued that the DBMH method can be regarded as a “working model that gives the right results”, but a method based on an earlier publication (Obuchowski and Rockette 1995) by Obuchowski and Rockette, which does not depend on pseudovalues, and predicts more or less the same results, is preferable from a conceptual viewpoint. Since, besides showing the correspondence, Hillis has made significant improvements to the original methodology, this chapter is named “ORH Analysis”, where ORH stands for Obuchowski, Rockette and Hillis. The ORH method has advantages in being able to handle more complex study designs (Hillis 2014) that are outside the scope of this book (the author acknowledges a private communication from Dr. Obuchowski, ca. 2006, that demonstrates the flexibility afforded by the OR approach) and it is possible that applications to other paradigms (e.g., the FROC paradigm uses a rather different FOM from empirical ROC-AUC) are better performed with the ORH method. This chapter starts with a “gentle” introduction to the Obuchowski and Rockette method. The reason for the “gentle” introduction is that the method was rather opaque to me, an I suspect, most users. Part of the problem, in my opinion, is the notation, namely lack of usage of the case-set index \\(\\{c\\}\\), whose absence can be confusing. The notational issue is highlighted in a key difference of the Obuchowski and Rockette method from DBMH, namely in how the error term is modeled by a covariance matrix. In this chapter the structure of the covariance matrix is examined in some detail, as it is key to understanding the ORH method. In the first step of the gentle introduction a single reader interpreting a case-set in multiple treatments is modeled and the results compared to those obtained using DBMH fixed-reader analysis described in the previous chapter. In the second step multiple readers interpreting a case-set in multiple treatments is modeled. The two analyses, DBMH and ORH, are compared for the same dataset. The special cases of fixed-reader and fixed-case analyses are described. Single treatment analysis, where interest is in comparing average performance of readers to a fixed value, is described. Three methods of estimating the covariance matrix are described. 8.2 Single-reader multiple-treatment model Consider a single-reader providing ROC interpretations of a common case-set \\(\\{c\\}\\) in multiple-treatments \\(i\\) (\\(i\\) = 1, 2, …, \\(I\\)). Before proceeding, we note that this is not homologous (i.e., formally equivalent) to multiple-readers providing ROC interpretations in a single treatment, §10.7; this is because reader is a random factor while treatment is not. The figure of merit \\(\\theta\\) is modeled as: \\[\\begin{equation} \\theta_{i\\{c\\}}=\\mu+\\tau_i+\\epsilon_{i\\{c\\}} \\tag{8.1} \\end{equation}\\] In the Obuchowski and Rockette method (Obuchowski and Rockette 1995) one models the figure-of-merit, not the pseudovalues, indeed this is one of the key differences from the DBMH method. Recall that \\(\\{c\\}\\) denotes a set of cases. Eqn. (10.1) models the observed figure-of-merit \\(\\theta_{i\\{c\\}}\\) as a constant term \\(\\mu\\) plus a treatment dependent term \\(\\tau_i\\) (the treatment-effect) with the constraint: \\[\\begin{equation} \\sum_{i=1}^{I}\\tau_i=0 \\tag{8.2} \\end{equation}\\] The c-index was introduced in (book) Chapter 07. The left hand side of Eqn. (10.1) is the figure-of-merit \\(\\theta_i\\{c\\}\\) for treatment \\(i\\) and case-set index \\(\\{c\\}\\), where \\(c\\) = 1, 2, …, \\(C\\) denotes different independent case-sets sampled from the population, i.e., different collections of \\(K_1\\) non-diseased and \\(K_2\\) diseased cases, not individual cases. This is one place the case-set index is essential for clarity; without it \\(\\theta_i\\) is a fixed quantity - the figure of merit estimate for treatment \\(i\\) - lacking any index allowing for sampling related variability. Obuchowski and Rockette use a k-index, defined as the “kth repetition of the study involving the same diagnostic test, reader and patient (sic)”. In the author’s opinion, what is meant is a case-set index instead of a repetition index. Repeating a study with the same treatment, reader and cases yields within-reader variability, which is different from sampling the population of cases with new case-sets, which yields case-sampling plus within-reader variability. As noted earlier, within-reader variability cannot be “turned off” and affects the interpretations of all case-sets. Interest is in extrapolating to the population of cases and the only way to do this is to sample different case-sets. It is shown below that usage of the case-set index interpretation yields the same results using the DBMH or the ORH methods. Finally, and this is where I had some difficulty understanding what is going on, there is an additive random error term whose sampling behavior is described by a multivariate normal distribution with an I-dimensional zero mean vector and an I x I dimensional covariance matrix \\(\\Sigma\\): \\[\\begin{equation} \\epsilon_{i\\{c\\}} \\sim N_I\\left ( \\vec{0} , \\Sigma\\right ) \\tag{8.3} \\end{equation}\\] Here \\(N_I\\) is the I-variate normal distribution (i.e., each sample yields \\(I\\) random numbers). Obuchowski and Rockette assumed the following structure for the covariance matrix (they describe a more general model, but here one restricts to the simpler one): \\[\\begin{equation} \\Sigma=Cov\\left ( \\epsilon_{i\\{c\\}}, \\epsilon_{i&#39;\\{c\\}} \\right )\\\\ =Var \\Rightarrow i=i&#39;\\\\ =Cov_1 \\Rightarrow i\\neq i&#39; \\tag{8.4} \\end{equation}\\] The reason for the subscript “1” in \\(Cov_1\\) will become clear when one extends this model to multiple readers. The \\(I \\times I\\) covariance matrix \\(\\Sigma\\) is: \\[\\begin{equation} \\Sigma= \\begin{pmatrix} Var &amp; Cov_1 &amp; \\ldots &amp; Cov_1 &amp; Cov_1 \\\\ Cov_1 &amp; Var &amp; \\ldots &amp;Cov_1 &amp; Cov_1 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ Cov_1 &amp; Cov_1 &amp; \\ldots &amp; Var &amp; Cov_1 \\\\ Cov_1 &amp; Cov_1 &amp; \\ldots &amp; Cov_1 &amp; Var \\end{pmatrix} \\tag{8.5} \\end{equation}\\] If \\(I\\) = 2 then \\(\\Sigma\\) is a symmetric 2 x 2 matrix, whose diagonal terms are the common variances in the two treatments (each assumed equal to \\(Var\\)) and whose off-diagonal terms (each assumed equal to \\(Cov_1\\)) are the co-variances. With \\(I\\) = 3 one has a 3 x 3 symmetric matrix with all diagonal elements equal to \\(Var\\) and all off-diagonal terms are equal to \\(Cov_1\\), etc. An important aspect of the Obuchowski and Rockette model is that the variances and co-variances are assumed to be treatment independent. This implies that \\(Var\\) estimates need to be averaged over all treatments. Likewise, \\(Cov_1\\) estimates need to be averaged over all distinct treatment-treatment pairings. A more complex model, with more parameters and therefore more difficult to work with, would allow the variances to be treatment dependent, and the covariances to depend on the specific treatment pairings. For obvious reasons (“Occam’s Razor” or the law of parsimony ) one wishes to start with the simplest model that, one hopes, captures essential characteristics of the data. Some elementary statistical results are presented next. 8.2.1 Definitions of covariance and correlation The covariance of two scalar random variables X and Y is defined by: \\[\\begin{equation} Cov(x,y) =\\frac{\\sum_{i=1}^{N}(x_{i}-x_{\\bullet})(y_{i}-y_{\\bullet})}{N-1}=E(XY)-E(X)-E(Y) \\tag{8.6} \\end{equation}\\] Here \\(E*X)\\) is the expectation value of the random variable \\(X\\), i.e., the integral of x multiplied by its \\(pdf\\): \\[E(X)=\\int pdf(x) x dx\\] The integral is over the range of \\(x\\). The covariance can be thought of as variance of two random variables that is common to both of them. The variance, a special case of covariance, of \\(X\\) is defined by: \\[Var(x,y) =Cov(X,X)=E(X^2)-(E(X))^2=\\sigma_x^2\\] It can be shown using the Cauchy–Schwarz inequality that \\[\\mid Cov(X,Y) \\mid^2 \\le Var(X)Var(Y)\\] A related quantity, the correlation \\(\\rho\\) is defined by (the \\(\\sigma\\)s are standard deviations): \\[\\rho_{XY} \\equiv Cor(X,Y)=\\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}\\] It has the property: \\[\\mid \\rho_{XY} \\mid \\le 1\\] 8.2.2 Special case Assuming \\(X\\) and \\(Y\\) have the same variance: \\[Var(X)=Var(Y)\\equiv Var\\equiv \\sigma^2\\] A useful theorem applicable to the OR single-reader multiple-treatment model is: \\[\\begin{equation} Var(X-Y)=Var(X)+Var(Y)-2Cov(X,Y)=2(Var-Cov) \\tag{8.7} \\end{equation}\\] The first line of the above equation is general, the second line specializes to the OR single-reader multiple-treatment model where the variances are equal and likewise all covariances in Eqn. (10.5) are equal) The correlation \\(\\rho_1\\) is defined by (the reason for the subscript 1 on \\(\\rho\\) is the same as the reason for the subscript 1 on \\(Cov_1\\), which will be explained later): \\[\\rho_1=\\frac{Cov_1}{Var}\\] The I x I covariance matrix \\(\\Sigma\\) can be written alternatively as (shown below is the matrix for I = 5; as the matrix is symmetric one need only show elements at and above the diagonal): \\[\\begin{equation} \\Sigma = \\begin{bmatrix} \\sigma^2 &amp; \\rho_1\\sigma^2 &amp; \\rho_1\\sigma^2 &amp; \\rho_1\\sigma^2 &amp; \\rho_1\\sigma^2\\\\ &amp; \\sigma^2 &amp; \\rho_1\\sigma^2 &amp; \\rho_1\\sigma^2 &amp; \\rho_1\\sigma^2\\\\ &amp; &amp; \\sigma^2 &amp; \\rho_1\\sigma^2 &amp; \\rho_1\\sigma^2\\\\ &amp; &amp; &amp; \\sigma^2 &amp; \\rho_1\\sigma^2\\\\ &amp; &amp; &amp; &amp; \\sigma^2 \\end{bmatrix} \\tag{8.8} \\end{equation}\\] 8.2.3 Estimation of the covariance matrix An unbiased estimate of the covariance Eqn. (10.4) follows from: \\[\\begin{equation} \\Sigma_{ii&#39;}=\\frac{1}{C-1}\\sum_{c=1}^{C} \\left ( \\theta_{i\\{c\\}} - \\theta_{i\\{\\bullet\\}} \\right) \\left ( \\theta_{i&#39;\\{c\\}} - \\theta_{i&#39;\\{\\bullet\\}} \\right) \\tag{8.9} \\end{equation}\\] Sampling different case-sets, as required by Eqn. (10.16), is unrealistic and in reality one is stuck with \\(C\\) = 1, i.e., a single dataset. Therefore direct application of this formula is impossible. However, as seen when this situation was encountered before in (book) Chapter 07, one uses resampling methods to realize, for example, different bootstrap samples, which are resampling-based “stand-ins” for actual case-sets. If \\(B\\) is the number of bootstraps, then the estimation formula is: \\[\\begin{equation} \\Sigma_{ii&#39;}\\mid_{bs} =\\frac{1}{B-1}\\sum_{b=1}^{B} \\left ( \\theta_{i\\{b\\}} - \\theta_{i\\{\\bullet\\}} \\right) \\left ( \\theta_{i&#39;\\{b\\}} - \\theta_{i&#39;\\{\\bullet\\}} \\right) \\tag{8.10} \\end{equation}\\] The bootstrap method of estimating the covariance matrix, Eqn. (10.17), is a direct translation of Eqn. (10.16). Alternatively, one could have used the jackknife FOM values \\(\\theta_{i(k)}\\), i.e., the figure of merit with a particular case removed, for all cases, to estimate the covariance matrix: \\[\\begin{equation} \\Sigma_{ii&#39;}\\mid_{jk} =\\frac{(K-1)^2}{K} \\left [ \\frac{1}{K-1}\\sum_{k=1}^{K} \\left ( \\theta_{i\\{k\\}} - \\theta_{i\\{\\bullet\\}} \\right) \\left ( \\theta_{i&#39;\\{k\\}} - \\theta_{i&#39;\\{\\bullet\\}} \\right) \\right ] \\tag{8.11} \\end{equation}\\] For simplicity, in this section we depart from the usual two-subscript convention to index each case. So \\(k\\) ranges from 1 to \\(K\\), where the first \\(K_1\\) values represent non-diseased and the following \\(K_2\\) values represent diseased cases. Jackknife figure of merit values are not to be confused with jackknife pseudovalues. The jackknife FOM value corresponding to a particular case is the FOM with the particular case removed. Unlike pseudovalues, jackknife FOM values cannot be regarded as independent and identically distributed. Notice the use of the subscript enclosed in parenthesis \\((k)\\) to denote the FOM with case \\(k\\) removed, i.e., a single case, while in the bootstrap equation one uses the curly brackets \\(\\{b\\}\\) to denote the bth bootstrap case-set, i.e., a whole set of \\(K_1\\) non-diseased and \\(K_2\\) diseased cases, sampled with replacement from the original dataset. Furthermore, the expression for the jackknife covariance contains a variance inflation factor: \\[\\begin{equation} \\frac{(K-1)^2}{K} \\tag{8.12} \\end{equation}\\] This factor multiplies the traditional expression for the covariance, shown in square brackets in Eqn. (10.18). A third method of estimating the covariance, namely the DeLong et al. method (DeLong, DeLong, and Clarke-Pearson 1988), applicable only to the empirical AUC, is described later. 8.2.4 Meaning of the covariance matrix in Eqn. (10.5) Suppose one has the luxury of repeatedly sampling case-sets, each consisting of \\(K\\) cases from the population. A single radiologist interprets these cases in \\(I\\) treatments. Therefore, each case-set \\(\\{c\\}\\) yields \\(I\\) figures of merit. The final numbers at ones disposal are \\(\\theta_{i\\{c\\}}\\), where \\(i\\) = 1,2,…,\\(I\\) and \\(c\\) = 1,2,…,\\(C\\). Considering treatment \\(i\\), the variance of the FOM-values for the different case-sets \\(c\\) = 1,2,…,\\(C\\), is an estimate of \\(Var_i\\) for this treatment: \\[\\begin{equation} \\sigma_i^2 \\equiv Var_i = \\frac{1}{C-1}\\sum_{c=1}^{C}\\left ( \\theta_{i\\{c\\}} - \\theta_{i\\{\\bullet\\}} \\right) \\left ( \\theta_{i\\{c\\}} - \\theta_{i\\{\\bullet\\}} \\right) \\tag{8.13} \\end{equation}\\] The process is repeated for all treatments and the \\(I\\)-variance values are averaged. This is the final estimate of \\(Var\\) appearing in (8.3). To estimate the covariance matrix one considers pairs of FOM values for the same case-set \\(\\{c\\}\\) but different treatments, i.e., \\(\\theta_{i\\{c\\}}\\) and \\(\\theta_{i&#39;\\{c\\}}\\); by definition primed and un-primed indices are different. Since they are derived from the same case-set, one expects the values to be correlated. For a particularly easy case-set one expects all I-estimates to be collectively higher than usual. The process is repeated for different case-sets and one calculates the correlation \\(\\rho_{1;ii&#39;}\\) between the two \\(C\\)-length arrays \\(\\theta_{i\\{c\\}}\\) and \\(\\theta_{i&#39;\\{c\\}}\\): \\[\\begin{equation} \\rho_{1;ii&#39;} = \\frac{1}{C-1}\\sum_{c=1}^{C} \\frac {\\left ( \\theta_{i\\{c\\}} - \\theta_{i\\{\\bullet\\}} \\right) \\left ( \\theta_{i&#39;\\{c\\}} - \\theta_{i&#39;\\{\\bullet\\}} \\right)}{\\sigma_i \\sigma_{i&#39;} } \\tag{8.14} \\end{equation}\\] The entire process is repeated for different treatment pairings and the resulting \\(I(I-1)/2\\) distinct values are averaged yielding the final estimate of \\(\\rho_1\\) in Eqn. (10.15). According to Eqn. (10.14) one expects the covariance to be smaller than the variance determined as in the previous paragraph. In most situations one expects \\(\\rho_1\\) (for ROC studies) to be positive. There is, perhaps unlikely, a scenario that could lead to anti-correlation and negative. This could occur, with “complementary” treatments, e.g., CT vs. MRI, where one treatment is good for bone imaging and the other for soft-tissue imaging. In this situation what constitutes an easy case-set in one treatment could be a difficult case-set in the other treatment. 8.2.5 Code illustrating the covariance matrix As indicated above, the covariance matrix can be estimated using the jackknife or the bootstrap. If the figure of merit is the Wilcoxon statistic, then one can also use the DeLong et al method (DeLong, DeLong, and Clarke-Pearson 1988). In Chapter 07, these methods were described in the context of estimating the variance of AUC. Eqn. (10.17) and Eqn. (10.18) extend the jackknife and the bootstrap methods, respectively, to estimating the covariance of AUC (whose diagonal elements are the variances estimated in the earlier chapter). The extension of the DeLong method to covariances is described in Online Appendix 10.A and implemented in file VarCovMtrxDLStr.R. It has been confirmed by the author that the implementation of the DeLong method (DeLong, DeLong, and Clarke-Pearson 1988) in file VarCovMtrxDLStr.R gives identical results to those yielded by the SAS macro attributed to DeLong. The file name stands for “variance covariance matrix according to the DeLong structural components method” described in five unnumbered equation following Eqn. 4 in the cited reference. The jackknife, bootstrap and the DeLong methods are used in file mainVarCov1.R, a listing and explanation of which appears in Online Appendix 10.B. Source the file yielding the following code output: seed &lt;- 1;set.seed(seed) jSelect &lt;- 1 # selects the reader to be analyzed rocData1R &lt;- DfExtractDataset(dataset02, rdrs = jSelect) zik1 &lt;- rocData1R$NL[,1,,1];K &lt;- dim(zik1)[2];I &lt;- dim(zik1)[1] zik2 &lt;- rocData1R$LL[,1,,1];K2 &lt;- dim(zik2)[2];K1 &lt;- K-K2;zik1 &lt;- zik1[,1:K1] # jk = jackknife # rjjk = RJafroc, covEstMethod = &quot;jackknife&quot; # rjbs = RJafroc, covEstMethod = &quot;bootstrap&quot; # bs = bootstrap # dl = DeLong ret1 &lt;- VarCov1_Jk(zik1, zik2) Var &lt;- ret1$Var;Cov1 &lt;- ret1$Cov1 # use these (i.e., jackknife) as default values data.frame (&quot;Cov_jk&quot; = Cov1, &quot;Var_jk&quot; = Var) #&gt; Cov_jk Var_jk #&gt; 1 0.0003734661 0.0006989006 ret4 &lt;- UtilVarComponentsOR(rocData1R, FOM = &quot;Wilcoxon&quot;)$varComp data.frame (&quot;Cov_rjjk&quot; = ret4$cov1, &quot;Var_rjjk&quot; = ret4$var) #&gt; Cov_rjjk Var_rjjk #&gt; 1 0.0003734661 0.0006989006 ret2 &lt;- VarCov1_Bs(zik1, zik2) data.frame (&quot;Cov_bs&quot; = ret2$Cov1, &quot;Var_bs&quot; = ret2$Var) # local code uses 2000 nBoots #&gt; Cov_bs Var_bs #&gt; 1 0.0003466804 0.0006738506 ret5 &lt;- UtilVarComponentsOR(rocData1R, FOM = &quot;Wilcoxon&quot;, covEstMethod = &quot;bootstrap&quot;, nBoots = 200)$varComp data.frame (&quot;Cov_rjbs&quot; = ret5$cov1, &quot;Var_rjbs&quot; = ret5$var) #&gt; Cov_rjbs Var_rjbs #&gt; 1 0.000283905 0.0005845354 ret5 &lt;- UtilVarComponentsOR(rocData1R, FOM = &quot;Wilcoxon&quot;, covEstMethod = &quot;bootstrap&quot;, nBoots = 2000)$varComp data.frame (&quot;Cov_rjbs&quot; = ret5$cov1, &quot;Var_rjbs&quot; = ret5$var) # matches local code with 2000 nBoots, provided seeds are identical #&gt; Cov_rjbs Var_rjbs #&gt; 1 0.0003466804 0.0006738506 ret5 &lt;- UtilVarComponentsOR(rocData1R, FOM = &quot;Wilcoxon&quot;, covEstMethod = &quot;bootstrap&quot;, nBoots = 20000)$varComp data.frame (&quot;Cov_rjbs&quot; = ret5$cov1, &quot;Var_rjbs&quot; = ret5$var) #&gt; Cov_rjbs Var_rjbs #&gt; 1 0.0003680714 0.0006862668 mtrxDLStr &lt;- VarCovMtrxDLStr(rocData1R) ret3 &lt;- VarCovs(mtrxDLStr) data.frame (&quot;Cov_dl&quot; = ret3$cov1, &quot;Var_dl&quot; = ret3$var) #&gt; Cov_dl Var_dl #&gt; 1 0.0003684357 0.0006900766 8.2.6 TBA Discussion of above code 8.3 Significance testing The covariance matrix is needed for significance testing. Define the mean square corresponding to the treatment effect, denoted \\(MS(T)\\), by: \\[\\begin{equation} MS(T)=\\frac{1}{I-1}\\sum_{i=1}^{I}(\\theta_i-\\theta_\\bullet)^2 \\tag{8.15} \\end{equation}\\] Unlike the previous chapter, all mean square quantities defined in this chapter are based on FOMs, not pseudovalues. It can be shown that under the null hypothesis (that all treatments have identical performances) the test statistic \\(\\chi_{1R}\\) defined below (the \\(1R\\) subscript denotes single-reader analysis) is distributed approximately as a \\(\\chi^2\\) distribution with \\(I-1\\) degrees of freedom, i.e., \\[\\begin{equation} \\chi_{1R} \\equiv \\frac{(I-1)MS(T)}{Var-Cov_1} \\sim \\chi_{I-1}^{2} \\tag{8.16} \\end{equation}\\] (8.16) is from §5.4 (Hillis 2007) with two covariance terms “zeroed out” because they are multiplied by \\(J-1 = 0\\) (since, in this example, we are restricting to \\(J=1\\)). Or equivalently, in terms of the F-distribution (Hillis et al. 2005): \\[\\begin{equation} F_{1R} \\equiv \\frac{MS(T)}{Var-Cov_1} \\sim F_{I-1, \\infty} \\tag{8.16} \\end{equation}\\] 8.3.1 An aside on the relation between the chisquare and the F-distribution with infinite ddf Define \\(D_{1-\\alpha}\\), the \\(1-\\alpha\\) quantile of distribution \\(D\\), as that “cutoff” value such that the probability of observing a random sample \\(d\\) less than or equal to \\(D_{1-\\alpha}\\) is \\(1-\\alpha\\). In other words, \\[\\begin{equation} \\Pr(d\\leq D_{1-alpha} \\mid d \\sim D)=1-\\alpha \\tag{8.17} \\end{equation}\\] With definition (8.17), the \\(1-\\alpha\\) quantile of the \\(\\chi_{I-1}^2\\) distribution, i.e., \\(\\chi_{1-\\alpha,I-1}^2\\), is related to the \\(1-\\alpha\\) quantile of the \\(F_{I-1,\\infty}\\) distribution, i.e., \\(F_{1-\\alpha,I-1,\\infty}\\), as follows (see Hillis et al. 2005, Eq. 22): \\[\\begin{equation} \\frac{\\chi_{1-\\alpha,I-1}^{2}}{I-1} = F_{1-\\alpha,I-1,\\infty} \\tag{8.18} \\end{equation}\\] (8.18) implies that the \\(1-\\alpha\\) quantile of the F-distribution with \\(ndf=I-1\\) and \\(ddf=\\infty\\) equals the \\(1-\\alpha\\) quantile of the \\(\\chi_{I-1}^2\\) distribution divided by \\(I-1\\). Here is an R illustration of this theorem for \\(I-1 = 4\\) and \\(\\alpha = 0.05\\): qf(0.05, 4, Inf) #&gt; [1] 0.1776808 qchisq(0.05,4)/4 #&gt; [1] 0.1776808 8.3.2 p-value and confidence interval The p-value is the probability that a sample from the \\(F_{I-1,\\infty}\\) distribution is greater than or equal to the observed value of the test statistic, namely: \\[\\begin{equation} p\\equiv \\Pr(f&gt;F_{1R} \\mid f \\sim F_{I-1,\\infty}) \\tag{8.19} \\end{equation}\\] The \\((1-\\alpha)\\) confidence interval for the inter-treatment FOM difference is given by: \\[\\begin{equation} CI_{1-\\alpha} = (\\theta_{i\\bullet} - \\theta_{i&#39;\\bullet}) \\pm t_{\\alpha/2,\\infty} \\sqrt{2(Var-Cov_1)} \\tag{8.20} \\end{equation}\\] Comparing (8.20) to (8.7) shows that the term \\(\\sqrt{2(Var-Cov_1)}\\) is the standard error of the inter-treatment FOM difference, whose square root is the standard deviation. The term \\(t_{\\alpha/2,\\infty}\\) is -1.96 and \\(t_{1-\\alpha/2,\\infty}\\) is +1.96. Therefore, the confidence interval is constructed by adding and subtracting 1.96 times the standard deviation of the difference from the central value. [One has probably encountered the rule that a 95% confidence interval is plus or minus two standard deviations from the central value. The “2” comes from rounding up 1.96.] 8.3.3 Comparing DBM to Obuchowski and Rockette for single-reader multiple-treatments We have shown two methods for analyzing a single reader in multiple treatments: the DBMH method, involving jackknife derived pseudovalues and the Obuchowski and Rockette method that does not have to use the jackknife, since it could use the bootstrap to get the covariance matrix, or some other methods such as the DeLong method, if one restricts to the Wilcoxon statistic for the figure of merit (empirical ROC-AUC). Since one is dealing with a single reader in multiple treatments, for DBMH one needs the fixed-reader analysis described in §9.8 the previous chapter (with just one reader the conclusions apply to the specific reader, so reader must be a fixed factor). Source the file MainOrDbmh1R.R, a listing of which appears in Online Appendix 10.C. For convenience, a few relevant lines are shown here: # seed &lt;- 1;set.seed(seed) alpha &lt;- 0.05 theta_i &lt;- c(0,0);for (i in 1:I) theta_i[i] &lt;- Wilcoxon(zik1[i,], zik2[i,]) MS_T &lt;- 0 for (i in 1:I) { MS_T &lt;- MS_T + (theta_i[i]-mean(theta_i))^2 } MS_T &lt;- MS_T/(I-1) F_1R &lt;- MS_T/(Var - Cov1) pValue &lt;- 1 - pf(F_1R, I-1, Inf) trtDiff &lt;- array(dim = c(I,I)) for (i1 in 1:(I-1)) { for (i2 in (i1+1):I) { trtDiff[i1,i2] &lt;- theta_i[i1]- theta_i[i2] } } trtDiff &lt;- trtDiff[!is.na(trtDiff)] nDiffs &lt;- I*(I-1)/2 CI_DIFF_FOM_1RMT &lt;- array(dim = c(nDiffs, 3)) for (i in 1 : nDiffs) { CI_DIFF_FOM_1RMT[i,1] &lt;- qt(alpha/2,df = Inf)*sqrt(2*(Var - Cov1)) + trtDiff[i] CI_DIFF_FOM_1RMT[i,2] &lt;- trtDiff[i] CI_DIFF_FOM_1RMT[i,3] &lt;- qt(1-alpha/2,df = Inf)*sqrt(2*(Var - Cov1)) + trtDiff[i] print(data.frame(&quot;theta_1&quot; = theta_i[1], &quot;theta_2&quot; = theta_i[2], &quot;Var&quot; = Var, &quot;Cov1&quot; = Cov1, &quot;MS_T&quot; = MS_T, &quot;F_1R&quot; = F_1R, &quot;pValue&quot; = pValue, &quot;Lower&quot; = CI_DIFF_FOM_1RMT[i,1], &quot;Mid&quot; = CI_DIFF_FOM_1RMT[i,2], &quot;Upper&quot; = CI_DIFF_FOM_1RMT[i,3])) } #&gt; theta_1 theta_2 Var Cov1 MS_T F_1R pValue #&gt; 1 0.9196457 0.9478261 0.0006989006 0.0003734661 0.0003970662 1.220111 0.2693389 #&gt; Lower Mid Upper #&gt; 1 -0.07818322 -0.02818035 0.02182251 # compare to RJafroc ret_rj &lt;- StSignificanceTesting(rocData1R, FOM = &quot;Wilcoxon&quot;, method = &quot;ORH&quot;) print(data.frame(&quot;theta_1&quot; = ret_rj$fomArray[1], &quot;theta_2&quot; = ret_rj$fomArray[2], &quot;Var&quot; = ret_rj$varComp$var, &quot;Cov1&quot; = ret_rj$varComp$cov, &quot;MS_T&quot; = ret_rj$msT, &quot;F_1R&quot; = ret_rj$FTestStatsFRRC$fFRRC, &quot;pValue&quot; = ret_rj$FTestStatsFRRC$pFRRC, &quot;Lower&quot; = ret_rj$ciDiffTrtFRRC$CILower, &quot;Mid&quot; = ret_rj$ciDiffTrtFRRC$Estimate, &quot;Upper&quot; = ret_rj$ciDiffTrtFRRC$CIUpper)) #&gt; theta_1 theta_2 Var Cov1 MS_T F_1R pValue #&gt; 1 0.9196457 0.9478261 0.0006989006 0.0003734661 0.0003970662 1.220111 0.2693389 #&gt; Lower Mid Upper #&gt; 1 -0.07818322 -0.02818035 0.02182251 8.4 Multiple-reader multiple-treatment ORH model The previous sections served as a “gentle” introduction to the single-reader multiple-treatment Obuchowski and Rockette method. This section extends it to multiple-readers interpreting a common case-set in multiple-treatments (MRMC). The extension is, in principle, fairly straightforward. Compared to (8.1), one needs an additional \\(j\\) index to index readers, and additional random terms to model reader and treatment-reader variability, and the error term needs to be modified to account for the additional random reader factor. The general Obuchowski and Rockette model for fully paired multiple-reader multiple-treatment interpretations is: \\[\\begin{equation} \\theta_{ij\\{c\\}}=\\mu+\\tau_i+R_j+(\\tau R)_{ij}+\\epsilon_{ij\\{c\\}} \\tag{8.21} \\end{equation}\\] The fixed treatment effect \\(\\tau_i\\) is subject to the usual constraint, (8.2). The first two terms on the right hand side of Eqn. (10.26) have their usual meanings: a constant term \\(\\mu\\) representing performance averaged over treatments and readers, and a treatment effect \\(\\tau_i\\) (\\(i\\) = 1,2, …, \\(I\\)). The following two terms are, by assumption, mutually independent random samples specified as follows: \\(R_j\\) denotes the random treatment-independent contribution to the figure-of-merit of reader \\(j\\) (\\(j\\) = 1,2, …, \\(J\\)), modeled as a sample from a zero-mean normal distribution with variance \\(\\sigma_R^2\\); \\((\\tau R)_{ij}\\) denotes the treatment-dependent random contribution of reader \\(j\\) in treatment \\(i\\), modeled as a sample from a zero-mean normal distribution with variance \\(\\sigma_{\\tau R}^2\\). There could be a perceived notational clash with similar variance component terms defined for the DBMH model – except in that case they applied to pseudovalues. The meaning should be clear from the context. Summarizing: \\[\\begin{equation} R_j \\sim N(0,\\sigma_R^2)\\\\ (\\tau R) \\sim N(0,\\sigma_{\\tau R}^2) \\tag{8.22} \\end{equation}\\] For a single dataset \\(c\\) = 1. An estimate of \\(\\mu\\) follows from averaging over the \\(i\\) and \\(j\\) indices (the averages over the random terms are zeroes): \\[\\begin{equation} \\mu = \\theta_{\\bullet \\bullet \\{1\\}} \\tag{8.23} \\end{equation}\\] As before the dot subscript denotes an average over the replaced index. Averaging over the j index and performing a subtraction yields an estimate of : \\[\\begin{equation} \\tau_i = \\theta_{i \\bullet \\{1\\}} - \\theta_{\\bullet \\bullet \\{1\\}} \\tag{8.24} \\end{equation}\\] The \\(\\tau_i\\) estimates obey the constraint (8.2). For example, with two treatments, the values of \\(\\tau_i\\) must be the negatives of each other: \\(\\tau_1=-\\tau_2\\). The error term on the right hand side of (8.21) is more complex than the corresponding DBM model error term. Obuchowski and Rockette model this term with a multivariate normal distribution with a length \\((IJ)\\) zero-mean vector and a \\((IJ \\times IJ)\\) dimensional covariance matrix \\(\\Sigma\\). In other words, \\[\\begin{equation} \\epsilon_{ij\\{c\\}} \\sim N_{IJ}(\\vec{0},\\Sigma) \\tag{8.25} \\end{equation}\\] Here \\(N_{IJ}\\) is the \\(N_{IJ}\\) variate normal distribution. The covariance matrix \\(\\Sigma\\) is defined by 4 parameters, \\(Var, Cov_1, Cov_2, Cov_3\\), defined as follows: \\[\\begin{equation} Cov(\\epsilon_{ij\\{c\\}},\\epsilon_{i&#39;j&#39;\\{c\\}}) = \\left\\{\\begin{matrix} = Var \\Rightarrow (i=i&#39;,j=j&#39;) \\\\ = Cov1 \\Rightarrow (i\\ne i&#39;,j=j&#39;)\\\\ = Cov2 \\Rightarrow (i = i&#39;,j \\ne j&#39;)\\\\ = Cov3 \\Rightarrow (i\\ne i&#39;,j \\ne j&#39;) \\end{matrix}\\right\\} \\tag{8.26} \\end{equation}\\] Apart from fixed effects, the model in (8.26) contains 6 parameters: \\[\\sigma_R^2,\\sigma_{\\tau R}^2,Var,Cov_1,Cov_2,Cov_3\\] This is the same number of variance component parameters as in the DBMH model, which should not be a surprise since one is modeling the data with equivalent models. The Obuchowski and Rockette model (8.21) “looks” simpler because four covariance terms are “hidden” in the \\(\\epsilon\\) term. As with the singe-reader multiple-treatment model, the covariance matrix is assumed to be independent of treatment or reader, as allowing treatment and reader dependencies would greatly increase the number of parameters that would need to be estimated. It is implicit in the Obuchowski-Rockette model that the \\(Var\\), \\(Cov_1\\), Cov_2$, and \\(Cov_3\\), estimates need to be averaged over all applicable treatment-reader combinations. 8.4.1 Structure of the covariance matrix To understand the structure of this matrix, recall that the diagonal elements of a (square) covariance matrix are variances and the off-diagonal elements are covariances. With two indices \\(ij\\) one can still imagine a square matrix where each dimension is labeled by a pair of indices \\(ij\\). One \\(ij\\) pair corresponds to the horizontal direction, and the other \\(ij\\) pair corresponds to the vertical direction. To visualize this let consider the simpler situation of two treatments (\\(I = 2\\)) and three readers (\\(J = 3\\)). The resulting 6x6 covariance matrix would look like this: \\[ \\Sigma= \\begin{bmatrix} (11,11) &amp; (12,11) &amp; (13,11) &amp; (21,11) &amp; (22,11) &amp; (23,11) \\\\ &amp; (12,12) &amp; (13,12) &amp; (21,12) &amp; (22,12) &amp; (23,12) \\\\ &amp; &amp; (13,13) &amp; (21,13) &amp; (22,13) &amp; (23,13) \\\\ &amp; &amp; &amp; (21,21) &amp; (22,21) &amp; (23,21) \\\\ &amp; &amp; &amp; &amp; (22,22) &amp; (23,22) \\\\ &amp; &amp; &amp; &amp; &amp; (23,23) \\end{bmatrix} \\] Shown in each cell of the matrix is a pair of ij-values, serving as column indices, followed by a pair of ij-values serving as row indices, and a comma separates the pairs. For example, the first column is labeled by (11,xx), where xx depends on the row. The second column is labeled (12,xx), the third column is labeled (13,xx), and the remaining columns are successively labeled (21,xx), (22,xx) and (23,xx). Likewise, the first row is labeled by (yy,11), where yy depends on the column. The following rows are labeled (yy,12), (yy,13), (yy,21), (yy,22)and (yy,23). Note that the reader index increments faster than the treatment index. The diagonal elements are evidently those cells where the row and column index-pairs are equal. These are (11,11), (12,12), (13,13), (21,21), (22,22) and (23,23). According to Eqn. (10.31) the entries in these cells would be \\(Var\\). \\[ \\Sigma= \\begin{bmatrix} Var &amp; (12,11) &amp; (13,11) &amp; (21,11) &amp; (22,11) &amp; (23,11) \\\\ &amp; Var &amp; (13,12) &amp; (21,12) &amp; (22,12) &amp; (23,12) \\\\ &amp; &amp; Var &amp; (21,13) &amp; (22,13) &amp; (23,13) \\\\ &amp; &amp; &amp; Var &amp; (22,21) &amp; (23,21) \\\\ &amp; &amp; &amp; &amp; Var &amp; (23,22) \\\\ &amp; &amp; &amp; &amp; &amp; Var \\end{bmatrix} \\] According to Eqn. (10.31) the entries in cells with different treatment index pairs but identical reader index pairs would be \\(Cov_1\\) (as an example, the cell (21,11) has the same reader index, namely reader 1, but different treatment indices, 2 and 1, so it is replaced by \\(Cov_1\\)): \\[ \\Sigma= \\begin{bmatrix} Var &amp; (12,11) &amp; (13,11) &amp; Cov_1 &amp; (22,11) &amp; (23,11) \\\\ &amp; Var &amp; (13,12) &amp; (21,12) &amp; Cov_1 &amp; (23,12) \\\\ &amp; &amp; Var &amp; (21,13) &amp; (22,13) &amp; Cov_1 \\\\ &amp; &amp; &amp; Var &amp; (22,21) &amp; (23,21) \\\\ &amp; &amp; &amp; &amp; Var &amp; (23,22) \\\\ &amp; &amp; &amp; &amp; &amp; Var \\end{bmatrix} \\] Similarly, the entries in cells with identical treatment index pairs but different reader index pairs would be \\(Cov_2\\): \\[ \\Sigma= \\begin{bmatrix} Var &amp; Cov_2 &amp; Cov_2 &amp; Cov_1 &amp; (22,11) &amp; (23,11) \\\\ &amp; Var &amp; Cov_2 &amp; (21,12) &amp; Cov_1 &amp; (23,12) \\\\ &amp; &amp; Var &amp; (21,13) &amp; (22,13) &amp; Cov_1 \\\\ &amp; &amp; &amp; Var &amp; Cov_2 &amp; Cov_2 \\\\ &amp; &amp; &amp; &amp; Var &amp; Cov_2 \\\\ &amp; &amp; &amp; &amp; &amp; Var \\end{bmatrix} \\] Finally, the entries in cells with different treatment index pairs and different reader index pairs would be \\(Cov_3\\): \\[ \\Sigma= \\begin{bmatrix} Var &amp; Cov_2 &amp; Cov_2 &amp; Cov_1 &amp; Cov_3 &amp; Cov_3 \\\\ &amp; Var &amp; Cov_2 &amp; Cov_3 &amp; Cov_1 &amp; Cov_3 \\\\ &amp; &amp; Var &amp; Cov_3 &amp; Cov_3 &amp; Cov_1 \\\\ &amp; &amp; &amp; Var &amp; Cov_2 &amp; Cov_2 \\\\ &amp; &amp; &amp; &amp; Var &amp; Cov_2 \\\\ &amp; &amp; &amp; &amp; &amp; Var \\end{bmatrix} \\] To understand these terms consider how they might be estimated. Suppose one had the luxury of repeating the study with different case-sets, c = 1, 2, …, C. Then the variance term \\(Var\\) can be estimated as follows: \\[Var= \\left \\langle \\frac{1}{C-1}\\sum_{c=1}^{C} (\\theta_{ij\\{c\\}}-\\theta_{ij\\{\\bullet\\}})^2 (\\theta_{ij\\{c\\}}-\\theta_{ij\\{\\bullet\\}})^2 \\right \\rangle_{ij}\\] Of course, in practice one would use the bootstrap or the jackknife as a stand-in for the c-index, but for pedagogic purpose, one maintains the fiction that one has a large number of case-sets at one’s disposal (not to mention the time spent by the readers interpreting them). Notice that the left-hand-side of Eqn. (10.38) does not have treatment or reader indices. This is because implicit in the notation is averaging the observed variances over all treatments and readers, as implied by \\(\\left \\langle \\right \\rangle _{ij}\\). Likewise, the covariance terms are estimated as follows: \\[Cov_1= \\left \\langle \\frac{1}{C-1}\\sum_{c=1}^{C} (\\theta_{ij\\{c\\}}-\\theta_{ij\\{\\bullet\\}})^2 (\\theta_{i&#39;j\\{c\\}}-\\theta_{i&#39;j\\{\\bullet\\}})^2 \\right \\rangle_{ii&#39;,jj}\\] \\[Cov_2= \\left \\langle \\frac{1}{C-1}\\sum_{c=1}^{C} (\\theta_{ij\\{c\\}}-\\theta_{ij\\{\\bullet\\}})^2 (\\theta_{ij&#39;\\{c\\}}-\\theta_{ij&#39;\\{\\bullet\\}})^2 \\right \\rangle_{ii,jj&#39;}\\] \\[Cov_3= \\left \\langle \\frac{1}{C-1}\\sum_{c=1}^{C} (\\theta_{ij\\{c\\}}-\\theta_{ij\\{\\bullet\\}})^2 (\\theta_{i&#39;j&#39;\\{c\\}}-\\theta_{i&#39;j&#39;\\{\\bullet\\}})^2 \\right \\rangle_{ii&#39;,jj&#39;}\\] In Eqn. (10.40) the convention is that primed and unprimed variables are always different. Since there are no treatment and reader dependencies on the left-hand-sides of the above equations, one averages the estimates as follows: For \\(Cov_1\\) one averages over all combinations of different treatments and same readers, as denoted by \\(\\left \\langle \\right \\rangle_{ii&#39;,jj}\\). For \\(Cov_2\\) one averages over all combinations of same treatment and different readers, as denoted by \\(\\left \\langle \\right \\rangle_{ii,jj&#39;}\\). For \\(Cov_3\\) one averages over all combinations of different treatments and different readers, as denoted by \\(\\left \\langle \\right \\rangle_{ii&#39;,jj&#39;}\\). 8.4.2 Physical meanings of the covariance terms The meanings of the different terms follow a similar description to that given in 8.4.1. The diagonal term of the covariance matrix is the variance of the figure-of-merit values obtained when reader j interprets different case-sets in treatment i: each case-set yields a number and the variance of the C numbers, averaged over the I x J treatments and readers, is . It captures the total variability due to varying difficulty levels of the case-sets and within-reader variability. is the correlation of the figure-of-merit values obtained when the same reader j interprets a case-set in different treatment . Each case-set, starting with c = 1, yields two numbers and ; the process is repeated for C case-sets. The correlation of the two pairs of C-length arrays, averaged over all pairings of different treatments and same readers, is . Because of the common contribution due to the shared reader, will be non-zero. For large common variation, the two arrays become almost perfectly correlated, and approaches unity. For zero common variation, the two arrays become independent, and equals zero. Translating to covariances, one has . is the correlation of the figure-of-merit values obtained when different readers interpret the same case-set in the same treatment i. As before this yields two numbers and upon repeating over C case-sets one has two C-length arrays, whose correlation, upon averaging over all distinct treatment pairings and same readers, yields . If one assumes that common variation between different-reader same-treatment FOMs is smaller than the common variation between same-reader different-treatment FOMs, then will be smaller than . This is equivalent to stating that readers agree more with themselves on different treatments than they do with other readers on the same treatment. Translating to covariances, one has . is the correlation of the figure-of-merit values obtained when different readers interpret the same case set in different treatments , etc., yielding . This is expected to yield the least correlation. Summarizing, one expects the following ordering for the terms in the covariance matrix: . (10.40) 8.4.3 ORH random-reader random-case analysis 8.4.4 Decision rule, p-value and confidence interval The critical value of the F-statistic for rejection of the null hypothesis is , i.e., that value such that fraction of the area under the distribution lies to the left of the critical value. From the definition of , rejection of the NH is more likely if increases, meaning the treatment effect is larger, decreases (there is less contamination of the treatment effect by treatment-reader variability), the greater of Cov2 or Cov3 decreases (there is less contamination of the treatment effect by between-reader and treatment-reader variability), increases (allowing a greater probability of Type I errors), ndf increases (the more the number of treatment pairings, the greater the chance that at least one pair will reject the NH) or increases (this lowers the critical value of the F-statistic). The p-value of the test is the probability, under the NH, that an equal or larger value of the F-statistic than could be observed by chance. In other words, it is the area under the F-distribution that lies above the observed value : 8.4.5 Special cases 8.4.6 Single-treatment multiple-reader analysis Suppose one has data in a single treatment and multiple readers are involved. One wishes to determine if the performance of the readers as a group equals some specified value. In a §10.2 single-reader multiple-treatment analysis was described. Attention now turns to single-treatment multiple-reader analysis and they are not the same! After all, treatment is a fixed factor while reader is a random factor; so one cannot simply use the previous analysis with reader and treatment interchanged (my graduate student tried to do just that, and he is quite smart, hence the reason for this warning; one can use the previous analysis if reader is regarded as a fixed factor, and a function in RJafroc called StSignificanceTestingSingleFixedFactor() does just that). In the analysis described in this section reader is regarded as a random effect. The average performance of the readers is estimated and compared to a specified value. Hillis1,2,7 has described the appropriate modification of the OR model when all readers read cases in a single treatment. Two approaches are described, one using the DBM pseudovalue based model and the other based on the OR model with appropriate modification. The second approach is summarized below. For single-treatment multiple-reader ORH analysis, the figure of merit model is (contrast the following equation to Eqn. (10.1) noting the absence of an i index; if multiple modalities are present the current analysis is applicable to data in each treatment analyzed one at a time): . (10.62) One wishes to test the NH: where is some pre-specified value. It follows from the previous equation that (since c = 1, in the interest of brevity, one can suppress the c index): . (10.63) The variance of the reader-averaged FOM can be shown4 to be given by (the reference is to the original OR publication, specifically Eqn. 2.3): . . (10.64) Connection to existing literature: Rather than attempt to derive the preceding equation, it is shown how it follows from the existing literature4. For convenience Eqn. 2.3 ibid is reproduced below. . (10.65) In the OR notation, the FOM has three indices, . One deletes the i index as one is dealing with a single treatment and one can drop the average over the k index, as one is dealing with a single dataset; in the OR notation is what we are calling ; for single treatment the treatment-reader interaction term is absent; and for single “replication” the term (in OR notation K is the number of replications) is absent, or, more accurately, the within-reader variance is absorbed into the case sampling variance as the two are inseparable); the term is what we are calling ; and in OR paper is what we are calling . An alternative first principles derivation, due to Mr. Xuetong Zhai, is given in Online Appendix 10.E. One needs to replace in Eqn. (10.64) with an expected value. Again, rather than attempt to derive the following equation, it is shown how it follows from the existing literature7. We start with Table I ibid: this is a table of expected means squares for the OR model, analogous to Table 9.1 in Chapter 09, for the DBM model. For a single treatment (in the notation of the cited reference, t = 1 and the treatment-reader variance component goes away and the term is what we are calling ), it follows that: . (10.66) Substituting Eqn. (10.66) in Eqn. (10.64) yields, . . (10.67) An estimate of MSR is given by (from here on it is understood that MSR is an estimate, i.e., the circumflex notation is suppressed; the same is true for ): . (10.68) Replacing the expected mean-square value with the estimate and avoiding negative covariance, which could lead to a negative variance estimate, one has : . . (10.69) The observed value of the t-statistic (the subscript emphasizes that this statistic applies to the single treatment analysis) for testing the NH is: . (10.70) This is distributed as a t-statistic with degrees of freedom: . . (10.71) In the above equation, Hillis single-treatment degree of freedom is defined by7: . (10.72) The p-value of the test is the probability that the a random sample from the specified t-distribution exceeds the magnitude of the observed value: . (10.73) Therefore, a confidence interval for is: . (10.74) The single treatment method is implemented in mainSingleTreatment.R. The relevant code is listed in Online Appendix 10.F. Source the code to get the following output. 8.5 Discussion/Summary This chapter described the Obuchowski-Rockette method as modified by Hillis. As noted earlier, it has the same number of parameters as the DBMH method described in the preceding chapter, but the model Eqn. (10.26)appears simpler as some terms are “hidden” in the structure of the error term. In this chapter the NH condition was considered. Extension to the alternative hypothesis, i.e., estimating statistical power, is deferred to online appendices to Chapter 11. The extension is a little simpler with the DBMH model, as it is a standard ANOVA model. For example the expressions for the DBMH non-centrality parameter was readily defined in Chapter 09, e.g., §9.7.4. Hillis has derived expressions allowing transformation between quantities in the two methods, and this is the approach adopted in this book and implemented in the cited online appendix. Online Appendix 10.A describes R implementation of the DeLong method for estimating the covariance matrix for empirical AUC. Since the main difficulty understanding the original OR method is conceptualizing the covariance matrix, the author has explained this at an elementary level, using a case-set index which is implicit in the original OR paper4. This was the reason for the gentle introduction analyzing performance of a single reader in multiple treatments. The jackknife, bootstrap and the DeLong methods, all implemented in Online Appendix 10.B, should reinforce understanding of the covariance matrix. The DBM and ORH methods are compared for this special case in Online Appendix 10.C. A minimal implementation of the ORH method for MRMC data is given in Online Appendix 10.D, which is a literal implementation of the relevant formulae. The special case of multiple readers in a single treatment is coded in Online Appendix 10.F. This will be used in Chapter 22 where standalone CAD performance is compared to a group of radiologists interpreting the same cases. The original publication by Dorfman Berbaum and Metz5 and the subsequent one by Obuchowski and Rockette4 were major advances. Hillis’ work showing their equivalence unified the two apparently disparate analyses, and this was a major advance. The Hillis papers, while difficult reads, are ones the author goes to repeatedly. This concludes two methods used to analyze ROC MRMC datasets. A third method, restricted to the empirical AUC, is also available 17-20. As noted earlier, the author prefers methods that are applicable to other estimates of AUC, not just the empirical area, and to other data collection paradigms, and for which software is readily available. The next chapter takes on the subject of sample size estimation using either DBMH or the ORH method. 8.6 References REFERENCES "]
]
