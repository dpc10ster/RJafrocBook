[
["index.html", "The RJafroc Book Preface", " The RJafroc Book Dev P. Chakraborty, PhD 2020-06-14 Preface This book, an extended documentation of the RJafroc package, is currently (as of April 2020) in preperation. It is intended to bypass the file size limits of CRAN, which severely limits the extent of the documentation that can be included with the CRAN package. "],
["a-note-on-the-online-distribution-mechanism-of-the-book.html", "A note on the online distribution mechanism of the book", " A note on the online distribution mechanism of the book In the hard-copy version of my book (Chakraborty 2017) the online distribution mechanisms was BitBucket. BitBucket allows code sharing within a closed group of a few users (e.g., myself and a student). Since the purpose of open-source code is to encourage collaborations, this was, in hindsight, an unfortunate choice. Moreover, as my experience with R-packages grew, it became apparent that the vast majority of R-packages are shared on GitHub, not BitBucket. For these reasons I have switched to GitHub. Any previous instructions pertaining to BitBucket are obsolete. In order to access GitHub material one needs to create a (free) account. Go to this link and click on Sign Up. REFERENCES "],
["contributing-to-this-book.html", "Contributing to this book", " Contributing to this book I appreciate any feedback on this document, e.g., corrections, comments, etc. To do this raise an Issue on the GitHub interface. Click on the Issues tab under dpc10ster/RJafrocBook, then click on New issue. Contributions from users automatically become part of the GitHub documentation/history of the book. "],
["example-running-external-scripts.html", "Chapter 1 Example running external scripts", " Chapter 1 Example running external scripts source method source(here(&quot;R/example.R&quot;)) # source(here(&quot;R/example2.R&quot;)) print(head(data.frame(x,y))) #&gt; x y #&gt; 1 1 2.073785 #&gt; 2 2 4.244257 #&gt; 3 3 3.560274 #&gt; 4 4 4.626446 #&gt; 5 5 4.579850 #&gt; 6 6 3.164821 "],
["HypothesisTesting.html", "Chapter 2 Hypothesis Testing 2.1 Introduction 2.2 Hypothesis testing for a single-modality single-reader ROC study 2.3 Type-I errors 2.4 One sided vs. two sided tests 2.5 Statistical power 2.6 Comments 2.7 Why alpha is chosen to be 5% 2.8 Discussion 2.9 References", " Chapter 2 Hypothesis Testing 2.1 Introduction The problem addressed in this chapter is how to decide whether an estimate of AUC is consistent with a pre-specified value. One example of this is when a single-reader rates a set of cases in a single-modality, from which one estimates AUC, and the question is whether the estimate is statistically consistent with a pre-specified value. From a clinical point of view, this is generally not a useful exercise, but its simplicity is conducive to illustrating the broader concepts involved in this and later chapters. The clinically more useful analysis is when multiple readers interpret the same cases in two or more modalities. With two modalities, for example, one obtains an estimate AUC for each reader in each modality, averages the AUC values over all readers within each modality, and computes the inter-modality difference in reader-averaged AUC values. The question forming the main subject of this book is whether the observed difference is consistent with zero. Each situation outlined above admits a binary (yes/no) answer, which is different from the estimation problem that was dealt with in connection with the maximum likelihood method in Chapter 06, where one computed numerical estimates (and confidence intervals) of the parameters of the fitting model. Hypothesis testing is the process of dichotomizing the possible outcomes of a statistical study and then using probabilistic arguments to choose one option over the other. The two competing options are termed the null hypothesis (NH) and the alternative hypothesis (AH). The hypothesis testing procedure is analogous1 to the jury trial system in the US, with 20 instead of 12 jurors, with the null hypothesis being the presumption of innocence and the alternative hypothesis being the defendant is guilty, and the decision rule is to assume the defendant is innocent unless all 20 jurors agree the defendant is guilty. If even one juror disagrees, the defendant is deemed innocent (equivalent to choosing an \\(\\alpha\\) – defined below - of 0.05, or 1/20). 2.2 Hypothesis testing for a single-modality single-reader ROC study The binormal model described in Chapter 06 can be used to generate sets of ratings to illustrate the methods being described in this chapter. To recapitulate, the model is described by: \\[\\begin{equation*} Z_{k_11} \\sim N\\left ( 0,1 \\right )\\\\ Z_{k_22} \\sim N\\left ( \\mu,\\sigma^2 \\right ) \\end{equation*}\\] The following code chunk encodes the Wilcoxon function: Wilcoxon &lt;- function (zk1, zk2) { K1 = length(zk1) K2 = length(zk2) W &lt;- 0 for (k1 in 1:K1) { W &lt;- W + sum(zk1[k1] &lt; zk2) W &lt;- W + 0.5 * sum(zk1[k1] == zk2) } W &lt;- W/K1/K2 return (W) } In the next code chunk we set \\(\\mu = 1.5\\) and \\(\\sigma = 1.3\\) and simulate \\(K_1 = 50\\) non-diseased cases and \\(K_2 = 52\\) diseased cases. For clarity I like to keep the sizes of the two arrays slightly different; this allows one to quickly check, with a glance at the Environment panel, that array dimensions are as expected. The for loop draws 50 samples from the \\(N(0,1)\\) distribution and 52 samples from the \\(N(\\mu,\\sigma^2)\\) distribution, calculates the empirical AUC using the Wilcoxon, and the process is repeated 10,000 times, the AUC values are saved to a huge array AUC. After exit from the for-loop we calculate the mean and standard deviation of the AUC values. seed &lt;- 1;set.seed(seed) mu &lt;- 1.5;sigma &lt;- 1.3;K1 &lt;- 50;K2 &lt;- 52 # cheat to find the population mean and std. dev. AUC &lt;- array(dim = 10000) for (i in 1:length(AUC)) { zk1 &lt;- rnorm(K1);zk2 &lt;- rnorm(K2, mean = mu, sd = sigma) AUC[i] &lt;- Wilcoxon(zk1, zk2) } meanAUC &lt;- mean(AUC);sigmaAUC &lt;- sd(AUC) cat(&quot;pop mean AUC = &quot;, meanAUC, &quot;, pop sigma AUC = &quot;, sigmaAUC, &quot;\\n&quot;) #&gt; pop mean AUC = 0.819178 , pop sigma AUC = 0.04176683 By the simple (if unimaginative) approach of sampling 10,000 times, one estimates the population mean and standard deviation of empirical AUC, denoted below by \\(AUC_{pop}\\) and \\(\\sigma_{AUC}\\), respectively. Based on the 10,000 simulations, \\(AUC_{pop}\\) = 0.819178 and \\(\\sigma_{AUC}\\) =0.0417668. The next chunk simulates one more independent ROC study with the same numbers of cases, and the resulting area under the empirical curve is denoted \\(AUC\\), AUC in the code. # one more trial, this is the one we want to compare to meanAUC, i.e., get P-value zk1 &lt;- rnorm(K1);zk2 &lt;- rnorm(K2, mean = mu, sd = sigma) AUC &lt;- Wilcoxon(zk1, zk2) cat(&quot;New AUC = &quot;, AUC, &quot;\\n&quot;) #&gt; New AUC = 0.8626923 z &lt;- (AUC - meanAUC)/sigmaAUC cat(&quot;z-statistic = &quot;, z, &quot;\\n&quot;) #&gt; z-statistic = 1.04184 Is the new value, 0.8626923, sufficiently different from the population mean (0.819178) to reject the null hypothesis \\(NH: AUC = AUC_{pop}\\)? Note that the answer to this question can be either yes or no: equivocation is not allowed. The new value is “somewhat close” to the population mean, but how does one decide if “somewhat close” is close enough? Needed is the statistical distribution of the random variable \\(AUC\\) under the hypothesis that the true mean is \\(AUC_{pop}\\). In the asymptotic limit of a large number of cases (this is an approximation), one can assume that the pdf of \\(AUC\\) under the null hypothesis is the normal distribution \\(N\\left ( AUC_{pop}, \\sigma_{AUC}^{2} \\right )\\): \\[\\begin{equation*} pdf_{AUC}\\left ( AUC\\mid AUC_{pop}, \\sigma_{AUC} \\right )=\\frac{1}{\\sqrt{2\\pi}\\sigma_{AUC}}exp\\left ( -\\frac{1}{2} \\left ( \\frac{AUC-AUC_{pop}}{\\sigma_{AUC}} \\right )^2\\right ) \\end{equation*}\\] The translated and scaled value is distributed as a unit normal distribution, i.e., \\[\\begin{equation*} Z=\\frac{AUC-AUC_{pop}}{\\sigma_{AUC}}\\sim N\\left ( 0,1 \\right ) \\end{equation*}\\] [The \\(Z\\) notation here should not be confused with z-sample, decision variable or rating of a case in an ROC study; the latter, when sampled over a set of non-diseased and diseased cases, yield a realization of \\(AUC\\). The author trusts the distinction will be clear from the context.] The observed magnitude of \\(z\\) is 1.0418397. The ubiquitous p-value is the probability that the observed magnitude of \\(z\\), or larger, occurs under the null hypothesis (NH), that the true mean of \\(Z\\) is zero. The p-value corresponding to an observed \\(z\\) of 1.0418397 is given by (as always, uppercase \\(Z\\) is the random variable, while lower case \\(z\\) is a realized value): \\[\\begin{equation*} \\Pr\\left ( \\lvert Z \\rvert \\geq \\lvert z \\rvert \\mid Z\\sim N\\left ( 0,1 \\right )\\right )=\\\\ \\Pr\\left ( \\lvert Z \\rvert \\geq 1.042 \\mid Z\\sim N\\left ( 0,1 \\right )\\right )=\\\\ 2\\Phi\\left ( -1.042 \\right )=0.2975 \\end{equation*}\\] To recapitulate statistical notation, \\(\\Pr\\left ( \\lvert Z \\rvert \\geq \\lvert z \\rvert \\mid Z\\sim N\\left ( 0,1 \\right )\\right )\\) is to be parsed as \\(\\Pr\\left ( A\\mid B \\right )\\), that is, the probability \\(\\lvert Z \\rvert \\geq \\lvert z \\rvert\\) given that \\(Z\\sim N\\left ( 0,1 \\right )\\). The last line in Eqn. (8.4) follows from the symmetry of the unit normal distribution, i.e., the area above 1.042 must equal the area below -1.042. Since \\(z\\) is a continuous variable, the probability that a sampled value will exactly equal the observed value is zero. Therefore, one must pose the question as stated above, namely what is the probability that \\(Z\\) is at least as extreme as the observed value (by “extreme” I mean further from zero, in either positive or negative directions). If the observed was \\(z\\) = 2.5 then the corresponding p-value would be \\(2\\Phi(-2.5)\\)=0.01242, which is smaller than 0.2975 (2*pnorm(-2.5) = 0.01241933). This is cited below as the “second example”. Under the zero-mean null hypothesis, the larger the magnitude of the observed value of \\(Z\\), the smaller the p-value, and the more unlikely that the data supports the NH. The p-value can be interpreted as the degree of unlikelihood that the data supports the NH. By convention one adopts a fixed value of the probability, denoted and usually \\(\\alpha\\) = 0.05, which is termed the size of the test or the significance level of the test, and the decision rule is to reject the null hypothesis if the observed p-value &lt; \\(\\alpha\\). \\[\\begin{equation*} p &lt; \\alpha \\Rightarrow \\text{Reject NH} \\end{equation*}\\] p2tailed &lt;- pnorm(-abs(z)) + (1-pnorm(abs(z))) # p value for two-sided AH p1tailedGT &lt;- 1-pnorm(z) # p value for one-sided AH &gt; 0 p1tailedLT &lt;- pnorm(z)# p value for one-sided AH &lt; 0 alpha &lt;- 0.05 In the first example, with observed p-value equal to 0.2975, one would not reject the null hypothesis, but in the second example, with observed p-value equal to 0.01242, one would. If the p-value is exactly 0.05 (unlikely with ROC analysis, but one needs to account for it) then one does not reject the NH. In the 20-juror analogy, of one juror insists the defendant is not guilty, then observed \\(\\Pr\\) is 0.05, and one does not reject the NH that the defendant is innocent (the double negatives, very common in statistics, can be confusing; in plain English, the defendant goes home). According to the previous discussion, the critical magnitude of \\(z\\) that determines whether to reject the null hypothesis is given by: \\[\\begin{equation*} z_\\frac{\\alpha}{2}=-\\Phi^{-1}\\left ( \\frac{\\alpha}{2} \\right ) \\end{equation*}\\] For \\(\\alpha\\) = 0.05 this evaluates to 1.95996 (which is sometimes rounded up to two, good enough for “government work” as the saying goes) and the decision rule is to reject the null hypothesis only if the observed magnitude of \\(z\\) is larger than \\(z_\\frac{\\alpha}{2}\\). The decision rule based on comparing the observed z to a critical value is equivalent to a decision rule based on comparing the observed p-value to \\(\\alpha\\). It is also equivalent, as will be shown later, to a decision rule based on a \\(\\left ( 1-\\alpha \\right )\\) confidence interval for the observed statistic. One rejects the NH if the closed confidence interval does not include zero. 2.3 Type-I errors Just because one rejects the null hypothesis, as in the second example, does not mean that the null hypothesis is false. Following the decision rule “caps”, or puts an upper limit on, the probability of incorrectly rejecting the null hypothesis at \\(\\alpha\\). In other words, by agreeing to reject the NH only if \\(p \\leq \\alpha\\), one has set an upper limit, namely \\(\\alpha\\), on errors of this type, termed Type-I errors. These could be termed false positives in the hypothesis testing sense, not to be confused with false positive occurring on individual case-level decisions. According to the definition of \\(\\alpha\\): \\[\\begin{equation*} \\Pr( \\text{Type I error} \\mid {NH} )=\\alpha \\end{equation*}\\] To demonstrate the ideas one needs to have a very cooperative reader interpreting new sets of independent cases not just one more time, but 2000 more times (the reason for the 2000 trials will be explained below). The simulation code for this follows: seed &lt;- 1;set.seed(seed) mu &lt;- 1.5;sigma &lt;- 1.3;K1 &lt;- 50;K2 &lt;- 52 nTrials &lt;- 2000 alpha &lt;- 0.05 # size of test reject = array(0, dim = nTrials) for (trial in 1:length(reject)) { zk1 &lt;- rnorm(K1);zk2 &lt;- rnorm(K2, mean = mu, sd = sigma) AUC &lt;- Wilcoxon(zk1, zk2) z &lt;- (AUC - meanAUC)/sigmaAUC p &lt;- 2*pnorm(-abs(z)) # p value for individual trial if (p &lt; alpha) reject[trial] = 1 } CI &lt;- c(0,0); width &lt;- -qnorm(alpha/2) ObsvdTypeIErrRate &lt;- sum(reject)/length(reject) CI[1] &lt;- ObsvdTypeIErrRate - width*sqrt(ObsvdTypeIErrRate*(1-ObsvdTypeIErrRate)/nTrials) CI[2] &lt;- ObsvdTypeIErrRate + width*sqrt(ObsvdTypeIErrRate*(1-ObsvdTypeIErrRate)/nTrials) cat(&quot;alpha = &quot;, alpha, &quot;\\n&quot;) #&gt; alpha = 0.05 cat(&quot;ObsvdTypeIErrRate = &quot;, ObsvdTypeIErrRate, &quot;, 95% confidence interval = &quot;, CI, &quot;\\n&quot;) #&gt; ObsvdTypeIErrRate = 0.0535 , 95% confidence interval = 0.04363788 0.06336212 exact &lt;- binom.test(sum(reject),n = 2000,p = alpha) cat(&quot;exact 95% CI = &quot;, as.numeric(exact$conf.int), &quot;\\n&quot;) #&gt; exact 95% CI = 0.04404871 0.06428544 The population means were calculated in an earlier code chunk. One initializes NTrials to 2000 and \\(\\alpha\\) to 0.05. The for-loop describes our captive reader interpreting independent sets of cases 2000 times. Each completed interpretation of 102 cases is termed a trial. For each trial one calculates the observed value of AUC, the observed z statistic and the the observed p-value. The observed p-value is compared against the fixed value \\(\\alpha\\) and one sets the corresponding reject[trial] flag to unity if \\(p &lt; \\alpha\\). In other words, if the trial-specific p-value is less than \\(\\alpha\\) one counts an instance of rejection of the null hypothesis. The process is repeated 2000 times. Upon exit from the for-loop, one calculates the observed Type-I error rate, denoted ObsvdTypeIErrRate by summing the reject array and dividing by 2000. One calculates a 95% confidence interval for ObsvdTypeIErrRate based on the binomial distribution, as in Chapter 03. The observed Type-I error rate is a realization of a random variable, as is the estimated 95% confidence interval. The fact that the confidence interval includes \\(\\alpha\\) = 0.05 is no coincidence - it shows that the hypothesis testing procedure is working as expected. To distinguish between the selected \\(\\alpha\\) (a fixed value) and that observed in a simulation study (a realization of a random variable), the term empirical \\(\\alpha\\) is used to denote the observed value rejection rate. It is a mistake to state that one wishes to minimize the Type-I error probability (the author has seen this comment from a senior researcher, which is the reason for bringing it up). The minimum value of \\(\\alpha\\) (a probability) is zero. Run the software with this value of \\(\\alpha\\): the NH will never be rejected. The downside of minimizing the expected Type-I error rate is that the NH will never be rejected, even when the NH is patently false. The aim of a valid method of analyzing the data is not minimizing the Type-I error rate, rather, the observed Type-I error rate should equal the specified value of \\(\\alpha\\) (0.05 in our example), allowance being made for the inherent variability in it’s estimate, i.e., its confidence interval. This is the reason 2000 trials were chosen for testing the validity of the NH testing procedure. With this choice, the 95% confidence interval, assuming that observed value is close to 0.05, is roughly ±0.01 as explained next. Following analogous reasoning to Chapter 03, Eqn. (3.10.10), and defining \\(f\\) as the observed rejection fraction over \\(T\\) trials, and as usual, \\(F\\) is a random variable and \\(f\\) a realized value, \\[\\begin{equation*} \\sigma_f = \\sqrt{f(1-f)/T}\\\\ F \\sim N\\left ( f,\\sigma_{f}^{2} \\right ) \\end{equation*}\\] An approximate \\((1-\\alpha)100\\) percent CI for \\(f\\) is: \\[\\begin{equation*} {CI}_f = \\left [ f-z_{\\frac{\\alpha}{2}}\\sigma_f, f+z_{\\frac{\\alpha}{2}}\\sigma_f \\right ] \\end{equation*}\\] If \\(f\\) is close to 0.05, then for 2000 trials, the 0.95 or 95% CI for \\(f\\) is \\(f \\pm 0.01\\), i.e., qnorm(alpha/2) * sqrt(.05*(.95)/2000) = 0.009551683 ~ 0.01. The only way to reduce the width of the CI, and thereby run a more stringent test of the validity of the analysis, is to increase the number of trials \\(T\\). Since the width of the CI depends on the inverse square root of the number of trials, one soon reaches a point of diminishing returns. Usually \\(T = 2000\\) trials are enough for most statisticians and the author, but examples using more simulations have been published. 2.4 One sided vs. two sided tests In the preceding example, the null hypothesis was rejected anytime the magnitude of the observed value of \\(z\\) exceeded the critical value \\(-\\Phi^{-1}\\left ( \\frac{\\alpha}{2} \\right)\\). This is a statement of the alternative hypothesis (AH) \\(AUC\\neq AUC_{pop}\\), in other words too high or too low values of \\(z\\) both result in rejection of the null hypothesis. This is referred to as a two-sided AH and the resulting p-value is termed a two-sided p-value. This is the most common one used in the literature. Now suppose that the additional trial performed by the radiologist was performed after an intervention following which the radiologist’s performance is expected to increase. To make matters clearer, assume the interpretations in the 10,000 trials used to estimate \\(AUC_{pop}\\) were performed with the radiologist wearing an old pair of eye-glasses, possibly out of proper strength, and the additional trial is performed after the radiologist gets a new set of prescription eye-glasses. Because the radiologist’s eyesight has improved, the expectation is that performance should increase. In this situation, it is appropriate to use the one-sided alternative hypothesis \\(AUC &gt; AUC_{pop}\\). Now, large values of \\(z\\) result in rejection of the null hypothesis, but small values do not. The critical value of \\(z\\) is defined by \\(z_\\alpha = \\Phi\\left ( 1-\\alpha \\right )\\), which for \\(\\alpha\\) = 0.05 is 1.645 (i.e., qnorm(1-alpha) = 1.644854). Compare 1.64 to the critical value \\(-\\Phi^{-1}\\left ( \\frac{\\alpha}{2} \\right)\\) = 1.96 for a two-sided test. If the change is in the expected direction, it is more likely that one will reject the NH with a one-sided than with a two-sided test. The p-value for a one-sided test is given by: \\[\\begin{equation*} \\Pr\\left ( Z \\geq 1.042 \\mid NH \\right ) = \\Phi \\left (-1.042 \\right ) = 0.1487 \\end{equation*}\\] Notice that this is half the corresponding two-sided test p-value; this is because one is only interested in the area under the unit normal that is above the observed value of \\(z\\). If the intent is to obtain a significant finding, it is tempting to use one-sided tests. The down side of a one-sided test is that even with a large excursion of the observed \\(z\\) in the other direction one cannot reject the null hypothesis. So if the new eye-glasses are so bad as to render the radiologist practically blind (think of a botched cataract surgery) the observed \\(z\\) would be large and negative, but one cannot reject the null hypothesis \\(AUC=AUC_{pop}\\). The one-sided test could be run the other way, with the alternative hypothesis being stated as \\(AUC&lt;AUC_{pop}\\). Now, large negative excursions of the observed value of AUC cause rejection of the null hypothesis, but large positive excursions do not. The critical value is defined by \\(z_\\alpha = \\Phi^{-1}\\left (\\alpha \\right )\\), which for \\(\\alpha\\) = 0.05 is -1.645. The p-value is given by (note the reversed sign compared to the previous one-sided test: \\[\\begin{equation*} \\Pr\\left ( Z \\leq 1.042 \\mid NH \\right ) = \\Phi(1.042) = 1 - 0.1487 = 0.8513 \\end{equation*}\\] This is the complement of the value for a one-sided test with the alternative hypothesis going the other way: obviously the probability that \\(Z\\) is smaller than the observed value (1.042) plus the probability that \\(Z\\) is larger than the same value must equal one. 2.5 Statistical power So far, focus has been on the null hypothesis. The Type-I error probability was introduced, defined as the probability of incorrectly rejecting the null hypothesis, the control, or “cap” on which is \\(\\alpha\\), usually set to 0.05. What if the null hypothesis is actually false and the study fails to reject it? This is termed a Type-II error, the control on which is denoted \\(\\beta\\), the probability of a Type-II error. The complement of \\(\\beta\\) is called statistical power. The following table summarizes the two types of errors and the two correct decisions that can occur in hypothesis testing. In the context of hypothesis testing, a Type-II error could be termed a false negative, not to be confused with false negatives occurring on individual case-level decisions. Truth Fail to reject NH Reject NH NH is True 1 - \\(\\alpha\\) \\(\\alpha\\) (FPF) NH is False \\(\\beta\\) (FNF) Power = 1 - \\(\\beta\\) This resembles the 2 x 2 table encountered in Chapter 02, which led to the concepts of \\(FPF\\), \\(TPF\\) and the ROC curve. Indeed, it is possible think of an analogous plot of empirical (i.e., observed) power vs. empirical \\(\\alpha\\), which looks like an ROC plot, with empirical \\(\\alpha\\) playing the role of \\(FPF\\) and empirical power playing the role of \\(TPF\\), see below. If \\(\\alpha\\) = 0, then power = 0; i.e., if Type-I errors are minimized all the way to zero, then power is zero and one makes Type-II errors all the time. On the other hand, if \\(\\alpha\\) = 1 then Power = 1, and one makes Type-I errors all the time. A little history is due at this point. The author’s first FROC study, which led to his entry into this field (Chakraborty et al. 1986), was published in Radiology in 1986 after a lot of help from a reviewer, who we (correctly) guessed was the late Prof. Charles E. Metz. Prof. Gary T. Barnes (the author’s mentor at that time at the University of Alabama at Birmingham) and the author visited Prof. Charles Metz in Chicago for a day ca. 1986, to figuratively “pick Charlie’s brain”. Prof. Metz referred to the concept outlined in the previous paragraph, as an ROC within an ROC. This curve does not summarize the result of a single ROC study. Rather it summarizes the probabilistic behavior of the two types of errors that occur when one conducts thousands of such studies, under both NH true and NH false conditions, each time with different values of \\(\\alpha\\), with each trial ending in a decision to reject or not reject the null hypothesis. The long sentence is best explained with an example. seed &lt;- 1;set.seed(seed) muNH &lt;- 1.5;muAH &lt;- 2.1;sigma &lt;- 1.3;K1 &lt;- 50;K2 &lt;- 52# Line 6 # cheat to find the population mean and std. dev. AUC &lt;- array(dim = 10000) # line 8 for (i in 1:length(AUC)) { zk1 &lt;- rnorm(K1);zk2 &lt;- rnorm(K2, mean = muNH, sd = sigma) AUC[i] &lt;- Wilcoxon(zk1, zk2) } sigmaAUC &lt;- sqrt(var(AUC));meanAUC &lt;- mean(AUC) # Line 14 T &lt;- 2000 # Line 16 mu &lt;- c(muNH,muAH) # Line 17 alphaArr &lt;- seq(0.05, 0.95, length.out = 10) EmpAlpha &lt;- array(dim = length(alphaArr));EmpPower &lt;- array(dim = length(alphaArr)) for (a in 1:length(alphaArr)) { # Line 20 alpha &lt;- alphaArr[a] reject &lt;- array(0, dim = c(2, T)) for (h in 1:2) { for (t in 1:length(reject[h,])) { zk1 &lt;- rnorm(K1);zk2 &lt;- rnorm(K2, mean = mu[h], sd = sigma) AUC &lt;- Wilcoxon(zk1, zk2) obsvdZ &lt;- (AUC - meanAUC)/sigmaAUC p &lt;- 2*pnorm(-abs(obsvdZ)) # p value for individual t if (p &lt; alpha) reject[h,t] = 1 } } EmpAlpha[a] &lt;- sum(reject[1,])/length(reject[1,]) EmpPower[a] &lt;- sum(reject[2,])/length(reject[2,]) } EmpAlpha &lt;- c(0,EmpAlpha,1); EmpPower &lt;- c(0,EmpPower,1) # Line 19 pointData &lt;- data.frame(EmpAlpha = EmpAlpha, EmpPower = EmpPower) zetas &lt;- seq(-5, 5, by = 0.01) muRoc &lt;- 1.8 curveData &lt;- data.frame(EmpAlpha = pnorm(-zetas), EmpPower = pnorm(muRoc - zetas)) alphaPowerPlot &lt;- ggplot(mapping = aes(x = EmpAlpha, y = EmpPower)) + geom_point(data = pointData, shape = 1, size = 3) + geom_line(data = curveData) print(alphaPowerPlot) Line 6 creates two variables, muNH = 1.5 (the binormal model separation parameter under the NH) and muAH = 2.1 (the separation parameter under the AH). Under either hypotheses, the same diseased case standard deviation sigma = 1.3 and 50 non-diseased and 52 diseased cases are assumed. As before, lines 8 – 14 use the “brute force” technique to determine population AUC and standard deviation of AUC under the NH condition. Line 16 defines the number of trials T = 2000. Line 17 creates a vector mu containing the NH and AH values defined at line 6. Line 18 creates alphaArr, a sequence of 10 equally spaced values in the range 0.05 to 0.95, which represent 10 values for \\(\\alpha\\). Line 19 creates two arrays of length 10 each, named EmpAlpha and EmpPower, to hold the values of the observed Type-I error rate, i.e., empirical \\(\\alpha\\), and the empirical power, respectively. The program will run T = 2000 NH and T = 2000 AH trials using as \\(\\alpha\\) each successive value in alphaArr and save the observed Type-I error rates and observed powers to the arrays EmpAlpha and EmpPower, respectively. The action begins in line 20, which begins a for-loop in a, an index into alphaArr. Line 21 selects the appropriate value for alpha (0.05 on the first pass, 0.15 on the next pass, etc.). Line 22 initializes reject[2,2000] with zeroes, to hold the result of each trial; the first index corresponds to hypothesis h and the second to trial t. Line 23 begins a for-loop in h, with h = 1 corresponding to the NH and h = 2 to the AH. Line 24 begins a for-loop in t, the trial index. The code within this block is similar to previous examples. It simulates ratings, computes AUC, calculates the p-value, and saves a rejection of the NH as a one at the appropriate array location reject[h,t]. Lines 32 – 33 calculate the empirical \\(\\alpha\\) and empirical power for each value of \\(\\alpha\\) in alphaArr. After padding the ends with zero and ones (the trivial points), the remaining lines plot the “ROC within an ROC”. Each of the circles in the figure corresponds to a specific value of \\(\\alpha\\). For example the lowest non-trivial corresponds to \\(\\alpha\\) = 0.05, for which the empirical \\(\\alpha\\) is 0.049 and the corresponding empirical Power is 0.4955. True \\(\\alpha\\) increases as the operating point moves up the plot, with empirical \\(\\alpha\\) and empirical power increasing correspondingly. The \\(AUC\\) under this curve is determined by the effect size, defined as the difference between the AH and NH values of the separation parameter. If the effect size is zero, then the circles will scatter around the chance diagonal; the scatter will be consistent with the 2000 trials used to generate each coordinate of a point. As the effect size increases, the plot approaches the perfect “ROC”, i.e., approaching the top-left corner. One could use AUC under this “ROC” as a measure of the incremental performance, the advantage being that it would be totally independent of \\(\\alpha\\), but this would not be practical as it requires replication of the study under NH and AH conditions about 2000 times each and the entire process has to be repeated for several values of \\(\\alpha\\). The purpose of this demonstration was to illustrate the concept behind Metz’s profound remark. It is time to move on to factors affecting statistical power in a single study. 2.5.1 Factors affecting statistical power Effect size: effect size is defined as the difference in \\(AUC_{pop}\\) values between the alternative hypothesis condition and the null hypothesis condition. Recall that \\(AUC_{pop}\\) is defined as the true or population value of the empirical ROC-AUC for the relevant hypothesis. One can use the “cheat method” to estimate it under the alternative hypothesis. The formalism is easier if one assumes it is equal to the asymptotic binormal model predicted value. The binormal model yields an estimate of the parameters, which only approach the population values in the asymptotic limit of a large number of cases. In the following, it is assumed that the parameters on the right hand side are the population values) It follows that effect size (ES) is given by (all quantities on the right hand side of Eqn. (8.13) are population values): \\[\\begin{equation*} AUC = \\Phi\\left ( \\frac{ \\mu }{\\sqrt{ 1 + \\sigma^2}} \\right ) \\end{equation*}\\] It follows that effect size (ES) is given by (all quantities on the right hand side of above equation are population values): \\[\\begin{equation*} ES = \\Phi\\left ( \\frac{\\mu_{AH}}{\\sqrt{1+\\sigma^2}} \\right ) - \\Phi\\left ( \\frac{\\mu_{NH}}{\\sqrt{1+\\sigma^2}} \\right ) \\end{equation*}\\] EffectSize &lt;- function (muNH, sigmaNH, muAH, sigmaAH) { ES &lt;- pnorm(muAH/sqrt(1+sigmaAH^2)) - pnorm(muNH/sqrt(1+sigmaNH^2)) return (ES) } seed &lt;- 1;set.seed(seed) muAH &lt;- 2.1 # NH value, defined previously, was mu = 1.5 T &lt;- 2000 alpha &lt;- 0.05 # size of test reject = array(0, dim = T) for (t in 1:length(reject)) { zk1 &lt;- rnorm(K1);zk2 &lt;- rnorm(K2, mean = muAH, sd = sigma) AUC &lt;- Wilcoxon(zk1, zk2) obsvdZ &lt;- (AUC - meanAUC)/sigmaAUC p &lt;- 2*pnorm(-abs(obsvdZ)) # p value for individual t if (p &lt; alpha) reject[t] = 1 } ObsvdTypeIErrRate &lt;- sum(reject)/length(reject) CI &lt;- c(0,0);width &lt;- -qnorm(alpha/2) CI[1] &lt;- ObsvdTypeIErrRate - width*sqrt(ObsvdTypeIErrRate*(1-ObsvdTypeIErrRate)/T) CI[2] &lt;- ObsvdTypeIErrRate + width*sqrt(ObsvdTypeIErrRate*(1-ObsvdTypeIErrRate)/T) cat(&quot;obsvdPower = &quot;, ObsvdTypeIErrRate, &quot;\\n&quot;) #&gt; obsvdPower = 0.489 cat(&quot;95% confidence interval = &quot;, CI, &quot;\\n&quot;) #&gt; 95% confidence interval = 0.4670922 0.5109078 cat(&quot;Effect Size = &quot;, EffectSize(mu, sigma, muAH, sigma), &quot;\\n&quot;) #&gt; Effect Size = 0.08000617 0 The ES for the code above is 0.08 (in AUC units). It should be obvious that if effect size is zero, then power equals \\(\\alpha\\). This is because then there is no distinction between the null and alternative hypotheses conditions. Conversely, as effect size increases, statistical power increases, the limiting value being unity, when every trial results in rejection of the null hypothesis. The reader should experiment with different values of muAH to be convinced of the truth of these statements. Sample size: increase the number of cases by a factor of two, and run the above code chunk. #&gt; pop NH mean AUC = 0.8594882 , pop NH sigma AUC = 0.02568252 #&gt; num. non-diseased images = 100 num. diseased images = 104 #&gt; obsvdPower = 0.313 #&gt; 95% confidence interval = 0.2926772 0.3333228 #&gt; Effect Size = 0.08000617 0 So doubling the numbers of cases (both non-diseased and diseased) results in statistical power increasing from 0.509 to 0.844. Increasing the numbers of cases decreases \\(\\sigma_{AUC}\\), the standard deviation of the empirical AUC. The new value of \\(\\sigma_{AUC}\\) is 0.02947, which should be compared to the value 0.04177 for K1 = 50, K2 = 52. Recall that \\(\\sigma_{AUC}\\) enters the denominator of the Z-statistic, so decreasing it will increase the probability of rejecting the null hypothesis. Alpha: Statistical power depends on \\(alpha\\). return the sample size to the original values . The results below are for two runs of the code, the first with the original value , set at line 16, the second with : #&gt; alpha = 0.05 obsvdPower = 0.1545 #&gt; alpha = 0.01 obsvdPower = 0.0265 Decreasing \\(\\alpha\\) results in decreased statistical power. 2.6 Comments The Wilcoxon statistic was used to estimate the area under the ROC curve. One could have used the binormal model, introduced in Chapter 06, to obtain maximum likelihood estimates of the area under the binormal model fitted ROC curve. The reasons for choosing the simpler empirical area are as follows. (1) With continuous ratings and 102 operating points, the area under the empirical ROC curve is expected to be a close approximation to the fitted area. (2) With maximum likelihood estimation, the code would be more complex – in addition to the fitting routine one would require a binning routine and that would introduce yet another variable in the analysis, namely the number of bins and how the bin boundaries were chosen. (3) The maximum likelihood fitting code can sometimes fail to converge, while the Wilcoxon method is always guaranteed to yield a result. The non-convergence issue is overcome by modern methods of curve fitting described in later chapters. (4) The aim was to provide an understanding of null hypothesis testing and statistical power without being bogged down in the details of curve fitting. 2.7 Why alpha is chosen to be 5% One might ask why \\(\\alpha\\) is traditionally chosen to be 5%. It is not a magical number, rather a cost benefit tradeoff. Choosing too small a value of \\(\\alpha\\) would result in greater probability \\((1-\\alpha)\\) of the NH not being rejected, even when it is false, i.e., decreased power. Sometimes it is important to detect a true difference between the measured AUC and the postulated value. For example, a new eye-laser surgery procedure is invented and the number of patients is necessarily small as one does not wish to subject a large number of patients to an untried procedure. One seeks some leeway on the Type-I error probability, possibly increasing it to \\(\\alpha\\) = 0.1, in order to have a reasonable chance of success in detecting an improvement in performance due to better eyesight after the surgery. If the NH is rejected and the change is in the right direction, then that is good news for the researcher. One might then consider a larger clinical trial and set \\(\\alpha\\) at the traditional 0.05, making up the lost statistical power by increasing the number of patients on which the surgery is tried. If a whole branch of science hinges on the results of a study, such as discovering the Higg’s Boson in particle physics, statistical significance is often expressed in multiples of the standard deviation (\\(\\sigma\\)) of the normal distribution, with the significance threshold set at a much stricter level (e.g. \\(5\\sigma\\)). This corresponds to \\(\\alpha\\) ~ 1 in 3.5 million (1/pnorm(-5) = 3.5 x 10^6, a one-sided test of significance). There is an article in Scientific American (https://blogs.scientificamerican.com/observations/five-sigmawhats-that/) on the use of \\(n\\sigma\\), where n is an integer, e.g. 5, to denote the significance level of a study, and some interesting anecdotes on why such high significance levels (low alpha) are used in some fields of research. Similar concerns apply to manufacturing where the cost of a mistake could be the very expensive recall of an entire product line. For background on Six Sigma Performance, see http://www.six-sigma-material.com/Six-Sigma.html. An article downloaded 3/30/17 from https://en.wikipedia.org/wiki/Six_Sigma is included as supplemental material to this chapter (Six Sigma.pdf). It has an explanation of why \\(6\\sigma\\) translates to one defect per 3.4 million opportunities (it has to do with short-term and long-term drifts in a process). In the author’s opinion, looking at other fields offers a deeper understanding of this material than simply stating that by tradition one adopts alpha = 5%. Most observer performance studies, while important in the search for better imaging methods, are not of such “earth-shattering” importance, and it is somewhat important to detect true differences (AH is true) at a reasonable alpha, so alpha = 5% and beta = 20% represent a good compromise. If one adopted a \\(5\\sigma\\) criterion, the NH would never be rejected, and progress in image quality optimization would come to a grinding halt. That is not to say that a \\(5\\sigma\\) criterion cannot be used; rather if used, the number of patients needed to detect a reasonable difference (effect size) with 80% probability would be astronomically large. Truth-proven cases are a precious commodity in observer performance studies. Particle physicists working on discovering the Higg’s Boson can get away with \\(5\\sigma\\) criterion because the number of independent observations and/or effect size is much larger than corresponding numbers in observer performance research. 2.8 Discussion In most statistics books, the subject of hypothesis testing is demonstrated in different (i.e., non-ROC) contexts. That is to be expected since the ROC-analysis field is a very small subspecialty of statistics (Prof. Howard E. Rockette, private communication, ca. 2002). Since this book is about ROC analysis, the author decided to use a demonstration using ROC analysis. Using a data simulator, one is allowed to “cheat” by conducting a very large number of simulations to estimate the population \\(AUC\\) under the null hypothesis. This permitted us to explore the related concepts of Type-I and Type-II errors within the context of ROC analysis. Ideally, both errors should be zero, but the nature of statistics leads one to two compromises. Usually one accepts a Type-I error capped at 5% and a Type-II error capped at 20%. These translate to \\(\\alpha\\) = 0.05 and desired statistical power = 80%. The dependence of statistical power on \\(\\alpha\\), the numbers of cases and the effect size was explored. Statistical power increases with the effect size, it increases with \\(\\alpha\\) and it increases with the sample size (numbers of cases). In Chapter 11 sample-size calculations are described that allow one to estimate the numbers of readers and cases needed to detect a specified difference in inter-modality AUCs with an expected statistical power \\(1-\\beta\\) . The word “detect” in the preceding sentence is shorthand for “reject the NH with probability capped at \\(\\alpha\\) while also rejecting the alternative hypothesis with probability capped at \\(\\beta\\)”. This chapter also gives the first example of validation of a hypothesis testing method. Statisticians sometimes refer to this as showing a proposed test is a “5% test”. What is meant is that one needs to be assured that when the NH is true the probability of NH rejection equals the expected value, namely \\(\\alpha\\), typically chosen to be 5%. Since the observed NH rejection rate over 2000 simulations is a random variable, one does not expect the NH rejection rate to exactly equal 5%, rather the constructed 95% confidence interval (also a random interval variable) should include the NH value with probability \\(\\alpha\\). As noted in the introduction, comparing a single reader’s performance to a specified value is not a clinically interesting problem. The next two chapters describe methods for significance testing of multiple-reader multiple-case (MRMC) ROC datasets, consisting of interpretations by a group of readers of a common set of cases in typically two modalities. It turns out that the analyses yield variability estimates that permit sample size calculation. After all, sample size calculation is all about estimation of variability, the denominator of the z-statistic, i.e., Eqn. (8.3), in the context of this chapter. The formulae will look more complex, as interest is not in determining the standard deviation of AUC, but in the standard deviation of the inter-modality reader-averaged AUC difference. However, the basic concepts remain the same. 2.9 References REFERENCES "],
["DBMHnalysis.html", "Chapter 3 Dorfman Berbaum Metz Hillis (DBMH) Analysis 3.1 Introduction 3.2 Random and fixed factors 3.3 Reader and case populations and data correlations 3.4 Three types of analyses 3.5 General approach 3.6 The Dorfman-Berbaum-Metz (DBM) method 3.7 Expected values of mean squares 3.8 Random-reader random-case (RRRC) analysis 3.9 Fixed-reader random-case (FRRC) analysis 3.10 Random-reader fixed-case (RRFC) analysis 3.11 DBMH analysis: Example 1, Van Dyke Data 3.12 DBMH analysis: Example 2, VolumeRad data 3.13 Validation of DBMH analysis 3.14 The meaning of pseudovalues 3.15 Summary 3.16 Things for me to think about 3.17 References", " Chapter 3 Dorfman Berbaum Metz Hillis (DBMH) Analysis 3.1 Introduction In this chapter the term “treatment” is used as a generic for “imaging system”, “modality” or “image processing” and “reader” is used as a generic for “radiologist” or algorithmic observer, e.g., a computer aided detection (CAD) algorithm. In the context of illustrating hypothesis-testing methods the previous chapter described analysis of a single ROC dataset and comparing the observed area \\(AUC\\) under the ROC plot to a specified value. Clinically this is not the most interesting problem; rather, interest is usually in comparing performance of a group of readers interpreting a common set of cases in two or more treatments. Such data is termed multiple reader multiple case (MRMC). [An argument could be made in favor of the term “multiple-treatment multiple-reader”, since “multiple-case” is implicit in any ROC analysis that takes into account correct and incorrect decisions on cases. However, the author will stick with existing terminology.] The basic idea is that by sampling a sufficiently large number of readers and a sufficiently large number of cases one might be able to draw conclusions that apply broadly to other readers of similar skill levels interpreting other similar case sets in the selected treatments. How one accomplishes this, termed MRMC analysis, is the subject of this chapter. This chapter describes the first truly successful method of analyzing MRMC ROC data, namely the Dorfman-Berbaum-Metz (DBM) method (Dorfman, Berbaum, and Metz 1992). The other method, due to Obuchowski and Rockette (Obuchowski and Rockette 1995), is the subject of Chapter 10. Both methods have been substantially improved by Hillis (Hillis, Berbaum, and Metz 2008; Hillis 2007, 2014). Hence the title of this chapter: “Dorfman Berbaum Metz Hillis (DBMH) Analysis”. It is not an overstatement that ROC analysis came of age with the methods described in this chapter. Prior to the techniques described here, one knew of the existence of sources of variability affecting a measured \\(AUC\\) value, as discussed in (book) Chapter 07, but then-known techniques (Swets and Pickett 1982) for estimating the corresponding variances and correlations were impractical. 3.1.1 Historical background The author was thrown (unprepared) into the methodology field ca. 1985 when, as a junior faculty member, he undertook comparing a prototype digital chest-imaging device (Picker International, ca. 1983) vs. an optimized analog chest-imaging device at the University of Alabama at Birmingham. At the outset a decision was made to use free-response ROC methodology instead of ROC, as the former accounted for lesion localization, and the author and his mentor, Prof. Gary T. Barnes, were influenced in that decision by a publication (Bunch et al. 1978) to be described in (book) Chapter 12. Therefore, instead of ROC-AUC one had lesion-level sensitivity at a fixed number of location level false positives per case as the figure-of-merit (FOM). Details of the FOM are not relevant at this time. Suffice to state that methods described in this chapter, which had not been developed in 1983, while developed for analyzing reader-averaged inter-treatment ROC-AUC differences, apply to any scalar FOM. While the author was successful at calculating confidence intervals (this is the heart of what is loosely termed “statistical analysis”) and publishing the work (Chakraborty et al. 1986) using techniques described in a book (Swets and Pickett 1982) titled “Evaluation of Diagnostic Systems: Methods from Signal Detection Theory”, subsequent attempts at applying these methods in a follow-up paper (Niklason et al. 1986) led to negative variance estimates (private communication, Dr. Loren Niklason, ca. 1985). With the benefit of hindsight, negative variance estimates are not that uncommon and the method to be described in this chapter has to deal with that possibility. The methods (Swets and Pickett 1982) described in the cited book involved estimating the different variability components – case sampling, between-reader and within-reader variability. Between-reader and within-reader variability (the two cannot be separated as discussed in (book) Chapter 07) could be estimated from the variance of the \\(AUC\\) values corresponding to the readers interpreting the cases within a treatment and then averaging the variances over all treatments. Estimating case-sampling and within-reader variability required splitting the dataset into a few smaller subsets (e.g., a case set with 60 cases might be split into 3 sub-sets of 20 cases each), analyzing each subset to get an \\(AUC\\) estimate and calculating the variance of the resulting \\(AUC\\) values (Swets and Pickett 1982) and scaling the result to the original case size. Because it was based on few values, the estimate was inaccurate, and the already case-starved original dataset made it difficult to estimate AUCs for the subsets; moreover, the division into subsets was at the discretion of the researcher, and therefore unlikely to be reproduced by others. Estimating within-reader variability required re-reading the entire case set, or at least a part of it. ROC studies have earned a deserved reputation for taking much time to complete, and having to re-read a case set was not a viable option. [Historical note: the author recalls a barroom conversation with Dr. Thomas Mertelmeir after the conclusion of an SPIE meeting ca. 2004, where Dr. Mertelmeir commiserated mightily, over several beers, about the impracticality of some of the ROC studies required of imaging device manufacturers by the FDA.] 3.1.2 The Wagner analogy An important objective of modality comparison studies is to estimate the variance of the difference in reader-averaged AUCs between the treatments. For two treatments one sums the reader-averaged variance in each treatment and subtracts twice the covariance (a scaled version of the correlation). Therefore, in addition to estimating variances, one needs to estimate correlations. Correlations are present due to the common case set interpreted by the readers in the different treatments. If the correlation is large, i.e., close to unity, then the individual treatment variances tend to cancel, making the constant treatment-induced difference easier to detect. The author recalls a vivid analogy used by the late Dr. Robert F. Wagner to illustrate this point at an SPIE meeting ca. 2008. To paraphrase him, consider measuring from shore the heights of the masts on two adjacent boats in a turbulent ocean. Because of the waves, the heights, as measured from shore, are fluctuating wildly, so the variance of the individual height measurements is large. However, the difference between the two heights is likely to be relatively constant, i.e., have small variance. This is because the wave that causes one mast’s height to increase also increases the height of the other mast. 3.1.3 The shortage of numbers to analyze and a pivotal breakthrough The basic issue was that the calculation of \\(AUC\\) reduces the relatively large number of ratings of a set of non-diseased and diseased cases to a single number. For example, after completion of an ROC study with 5 readers and 100 non-diseased and 100 diseased cases interpreted in two treatments, the data is reduced to just 10 numbers, i.e., five readers times two treatments. It is difficult to perform statistics with so few numbers. The author recalls a conversation with Prof. Kevin Berbaum at a Medical Image Perception Society meeting in Tucson, Arizona, ca. 1997, in which he described the basic idea that forms the subject of this chapter. Namely, using the jackknife pseudovalues, Eqn. (7.6), as individual case-level figures of merit. This, of course, greatly increases the amount of data that one can work with; instead of just 10 numbers one now has 2,000 pseudovalues (2 x 5 x 200). If one assumes the pseudovalues behave essentially as case-level data, then by assumption they are independent and identically distributed , and therefore they satisfy the conditions for application of standard analysis of variance (ANOVA) techniques10. The relevant paper1 had already been published in 1992 but other distractions and lack of formal statistical training kept the author from fully appreciating this work until later. Although methods are available for more complex study designs including partially paired data (Metz, Herman, and Roe 1998; Obuchowski 2009), I will restrict to fully paired data (i.e., each case is interpreted by all readers in all treatments). There is a long history of how this field has evolved and the author cannot do justice to all methods that are currently available. Some of the methods (Toledano 2003; Ishwaran and Gatsonis 2000; Toledano and Gatsonis 1996) have the advantage that they can handle explanatory variables (termed covariates) that could influence performance, e.g., years of experience, types of cases, etc. Other methods are restricted to specific choices of FOM. Specifically, the probabilistic approach (Clarkson, Kupinski, and Barrett 2006; Kupinski, Clarkson, and Barrett 2006; Gallas, Pennello, and Myers 2007; Gallas 2006) is restricted to the empirical \\(AUC\\) under the ROC curve, and therefore are not applicable to other FOMs, e.g., parametrically fitted ROC AUCs or, more importantly, to location specific paradigm FOMs. Instead, the author will focus on methods for which software is readily available (i.e., freely on websites), which have been widely used (the method that the author is about to describe has been used in several hundred publications) and validated via simulations, and which apply to any scalar figure of merit, and therefore widely applicable, even to location specific paradigms. 3.1.4 Organization of the chapter The organization of the chapter is as follows. The concepts of reader and case populations, introduced in (book) Chapter 07, are recapitulated. A distinction is made between fixed and random factors – statistical terms with which one must become familiar. Described next are three types of analysis that are possible with MRMC data, depending on which factors are regarded as random and which as fixed. The general approach to the analysis is described. Two methods of analysis are possible: the jackknife pseudovalue-based approach detailed in this chapter and an alternative approach is detailed in Chapter 10. The Dorfman-Berbaum-Metz (DBM) model for the jackknife pseudovalues is described that incorporates different sources of variability and correlations possible with MRMC data. Calculation of ANOVA-related quantities, termed mean squares, from the pseudovalues, are described followed by the significance testing procedure for testing the null hypothesis of no treatment effect. A relevant distribution used in the analysis, namely the F-distribution, is illustrated with R examples. The decision rule, i.e., whether to reject the NH, calculation of the ubiquitous p-value, confidence intervals and how to handle multiple treatments is illustrated with two datasets, one an older ROC dataset that has been widely used to demonstrate advances in ROC analysis, and the other a recent dataset involving evaluation of digital chest tomosynthesis vs. conventional chest imaging. The approach to validation of DBMH analysis is illustrated with an R example. The chapter concludes with a section on the meaning of the pseudovalues. The intent is to explain, at an intuitive level, why the DBM method “works”, even though use of pseudovalues has been questioned3 at the conceptual level. For organizational reasons and space limitations, details of the software are relegated to Online Appendices, but they are essential reading, preferably in front of a computer running the online software that is part of this book. The author has included material here that may be obvious to statisticians, e.g., an explanation of the Satterthwaite approximation, but are expected to be helpful to others from non-statistical backgrounds. 3.2 Random and fixed factors This paragraph introduces some analysis of variance (ANOVA) terminology. Treatment, reader and case are factors with different numbers of levels corresponding to each factor. For an ROC study with two treatments, five readers and 200 cases, there are two levels of the treatment factor, five levels of the reader factor and 200 levels of the case factor. If a factor is regarded as fixed, then the conclusions of the analysis apply only to the specific levels of the factor used in the study. If a factor is regarded as random, the levels of the factor are regarded as random samples from a parent population of the corresponding factor and conclusions regarding specific levels are not allowed; rather, conclusions apply to the distribution from which the levels are, by assumption, sampled. ROC MRMC studies require a sample of cases and interpretations by one or more readers in one or more treatments (in this book the term “multiple” includes as a special case “one”). A study is never conducted on a sample of treatments. It would be nonsensical to image patients using a “sample” of all possible treatments known to exist. Every variation of an imaging technique (e.g., different kilovoltage or kVp) or display method (e.g., window-level setting) or image processing techniques qualifies as a distinct treatment. The number of possible treatments is very large, and, from a practical point of view, most of them are uninteresting. Rather, interest is in comparing two or more (a few at most) treatments that, based on preliminary studies, are clinically interesting. One treatment may be computed tomography, the other magnetic resonance imaging, or one may be interested in comparing a standard image processing method to a newly proposed one, or one may be interested in comparing CAD to a group of readers. This brings out an essential difference between how cases, readers and treatments have to be regarded in the variability estimation procedure. Cases and readers are usually regarded as random factors (there has to be at least one random factor – if not, there are no sources of variability and nothing to apply statistics to!), while treatments are regarded as fixed factors. The random factors contribute stochastic (i.e., random) variability, but the fixed factors do not, rather they contribute constant shifts in performance. The terms fixed and random factors are used in this specific sense, and are derived, in turn, from ANOVA methods in statistics10,25. With two or more treatments, there are shifts in performance of treatments relative to each other, that one seeks to assess the significance of against a background of noise contributed by the random factors. If the shifts are sufficiently large compared to the noise, then one can state, with some certainty, that they are real. Quantifying the last statement uses the methods of hypothesis testing introduced in Chapter 2 or Chapter Hypothesis Testing. 3.3 Reader and case populations and data correlations As discussed in (book) §7.2, conceptually there is a reader-population, generally modeled as a normal distribution \\(\\theta_j \\sim N\\left ( \\theta_{\\bullet\\{1\\}}, \\sigma_{br+wr}^{2} \\right )\\), describing the variation of skill-level of readers. The notation closely follows that in the cited section, the only change being that the binormal model estimate \\(A_z\\) has been replaced by a generic FOM, denoted \\(\\theta\\). Each reader \\(j\\) is characterized by a different value of \\(\\theta_j\\), \\(j=1,2,...J\\) and one can conceptually think of a bell-shaped curve with variance \\(\\sigma_{br+wr}^{2}\\) describing between-reader variability of the readers. A large variance implies large spread in reader skill levels. Likewise, there is a case-population, also modeled as a normal distribution, describing the variations in difficulty levels of the patients. One actually has two unit-variance distributions, one per diseased state, characterized by a separation parameter and conceptually an easy case set has a larger than usual separation parameter while a difficult case set has a smaller than usual separation parameter. The distribution of the separation parameter can be modeled as a bell-shaped curve \\(\\theta_{\\{c\\}} \\sim N\\left ( \\theta_{\\{\\bullet\\}}, \\sigma_{cs+wr}^{2} \\right )\\) with variance \\(\\sigma_{cs+wr}^{2}\\) describing the variations in difficulty levels of different case samples. Note the need for the case-set index, introduced in Chapter 07, to specify the separation parameter for a specific case-set (in principle a \\(j\\)-index is also needed as one cannot have an interpretation without a reader; for now it is suppressed; one can think of the stated equation as applying to the average reader). A small variance \\(\\sigma_{cs}^{2}\\) implies the different case sets have similar difficulty levels while a larger variance would imply a larger spread in difficulty levels. Anytime one has a common random component to two measurements, the measurements are correlated. In the Wagner analogy, the common component is the random height, as a function of time, of a wave, which contributes the same amount to both height measurements (since the boats are adjacent). Since the readers interpret a common case set in all treatments one needs to account for various types of correlations that are potentially present. These occur due to the various types of pairings that can occur with MRMC data, where each pairing implies the presence of a common component to the measurements: (a) the same reader interpreting the same cases in different treatments, (b) different readers interpreting the same cases in the same treatment and (c) different readers interpreting the same cases in different treatments. These pairings are more clearly elucidated in (book) Chapter 10. The current chapter uses jackknife pseudovalue based analysis to model the variances and the correlations. Hillis has shown that the two approaches are essentially equivalent (Hillis, Berbaum, and Metz 2008). 3.4 Three types of analyses MRMC analysis attempts to draw conclusions regarding the significances of inter-treatment shifts in performance. Ideally a conclusion (i.e., a difference is significant: yes/no; the “yes” applies if the p-value is less than alpha) should generalize to the respective populations from which the random samples were obtained. In other words, the idea is to generalize from the observed samples to the underlying populations. Three types of analyses are possible depending on which factor(s) one regards as random and which as fixed: random-reader random-case (RRRC), fixed-reader random-case (FRRC) and random-reader fixed-case (RRFC). If a factor is regarded as random, then the conclusion of the study applies to the population from which the levels of the factor were sampled. If a factor is regarded as fixed, then the conclusion applies only to the specific levels of the sampled factor. For example, if reader is regarded as a random factor, the conclusion generalizes to the reader population from which the readers used in the study were obtained. If reader is regarded as a fixed factor, then the conclusion applies to the specific readers that participated in the study. Regarding a factor as fixed effectively “freezes out” the sampling variability of the population and interest then centers only on the specific levels of the factor used in the study. For fixed reader analysis, conclusions about the significances of differences between pairs of readers are allowed; these are not allowed if reader is treated as a random factor. Likewise, treating case as a fixed factor means the conclusion of the study is specific to the case-set used in the study. 3.5 General approach This section provides an overview of the steps involved in analysis of MRMC data. Two approaches are described in parallel: a figure of merit (FOM) derived jackknife pseudovalue based approach, detailed in this chapter and an FOM based approach, detailed in the next chapter. The analysis proceeds as follows: A FOM is selected: the selection of FOM is the single-most critical aspect of analyzing an observer performance study. The selected FOM is denoted \\(\\theta\\). To keep the notation reasonably compact the usual circumflex “hat” symbol used previously to denote an estimate is suppressed. The FOM has to be an objective scalar measure of performance with larger values characterizing better performance. [The qualifier “larger” is trivially satisfied; if the figure of merit has the opposite characteristic, a sign change is all that is needed to bring it back to compliance with this requirement.] Examples are empirical \\(AUC\\), the binormal model-based estimate \\(A_z\\) , other advance method based estimates of \\(AUC\\), sensitivity at a predefined value of specificity, etc. An example of a FOM requiring a sign-change is \\(FPF\\) at a specified \\(TPF\\), where smaller values signify better performance. For each treatment \\(i\\) and reader \\(j\\) the figure of merit \\(\\theta_{ij}\\) is estimated from the ratings data. Repeating this over all treatments and readers yields a matrix of observed values \\(\\theta_{ij}\\). This is averaged over all readers in each treatment yielding \\(\\theta_{i\\bullet}\\). The observed effect-size \\(ES_{obs}\\) is defined as the difference between the reader-averaged FOMs in the two treatments, i.e., \\(ES_{obs}\\) = \\(\\theta_{2\\bullet}\\) - \\(\\theta_{1\\bullet}\\). While extensible to more than two treatments, the explanation is more transparent by restricting to two modalities. If the magnitude of \\(ES_{obs}\\) is “large” one has reason to suspect that there might indeed be a significant difference in AUCs between the two treatments, where significant is used in the sense of (book) Chapter 08. Quantification of this statement, specifically how large is “large”, requires the conceptually more complex steps described next. In the DBMH approach, the subject of this chapter, jackknife pseudovalues are calculated as described in Chapter 08. A standard ANOVA model with uncorrelated errors is used to model the pseudovalues. In the ORH approach, the subject of the next chapter, the FOM is modeled directly using a custom ANOVA model with correlated errors. Depending on the selected method of modeling the data (pseudovalue vs. FOM) a statistical model is used which includes parameters modeling the true values in each treatment, and expected variations due to different variability components in the model, e.g., between-reader variability, case-sampling variability, interactions (e.g., modeling the possibility that the random effect of a given reader could be treatment dependent) and the presence of correlations (between pseudovalues or FOMs) because of the pairings inherent in the interpretations. In RRRC analysis one accounts for randomness in readers and cases. In FRRC analysis one regards reader as a fixed factor. In RRFC analysis one regards case as a fixed factor. The statistical model depends on the type of analysis. The parameters of the statistical model are estimated from the observed data. The estimates are used to infer the statistical distribution of the observed effect size, \\(ES_{obs}\\), regarded as a realization of a random variable, under the null hypothesis (NH) that the true effect size is zero. Based on this statistical distribution, and assuming a two-sided test, the probability (this is the oft-quoted p-value) of obtaining an effect size at least as extreme as that actually observed, is calculated, as in Chapter 08. If the p-value is smaller than a preselected value, denoted \\(\\alpha\\), one declares the treatments different at the \\(\\alpha\\) - significance level. The quantity \\(\\alpha\\) is the control (or cap) on the probability of making a Type I error, defined as rejecting the NH when it is true. It is common to set \\(\\alpha\\) = 0.05 but depending on the severity of the consequences of a Type I error, as discussed in (book) Chapter 08, one might consider choosing a different value. Notice that \\(\\alpha\\) is a pre-selected number while the p-value is a realization of a random variable. For a valid statistical analysis, the empirical probability \\(\\alpha_{emp}\\) over many (typically 2000) independent NH datasets, that the p-value is smaller than \\(\\alpha\\), should equal \\(\\alpha\\) to within statistical uncertainty. 3.6 The Dorfman-Berbaum-Metz (DBM) method The figure-of-merit has three indices: 1. A treatment index \\(i\\), where \\(i\\) runs from 1 to \\(I\\), where \\(I\\) is the total number of treatments. 1. A reader index \\(j\\), where \\(j\\) runs from 1 to \\(J\\), where \\(J\\) is the total number of readers. 1. The often-suppressed case-sample index \\(\\{c\\}\\), where \\(\\{1\\}\\) i.e., \\(c\\) = 1, denotes a set of cases, \\(K_1\\) non-diseased and \\(K_2\\) diseased, interpreted by all readers in all treatments, and other integer values of \\(c\\) correspond to other independent sets of cases that, although not in fact interpreted by the readers, could potentially be “interpreted” using resampling methods such as the bootstrap or the jackknife. The approach (Dorfman, Berbaum, and Metz 1992) taken by Dorfman-Berbaum-Metz (DBM) was to use the jackknife resampling method described in (book) Chapter 7 to calculate FOM pseudovalues \\({Y&#39;}_{ijk}\\) defined by (the reason for the prime will become clear shortly): \\[\\begin{equation} Y&#39;_{ijk}=K\\theta_{ij}-(K-1)\\theta_{ij\\{k\\}} \\tag{3.1} \\end{equation}\\] Here \\(\\theta_{ij}\\) is the estimate of the figure-of-merit for reader \\(j\\) interpreting all cases in treatment \\(i\\) and \\(\\theta_{ij\\{k\\}}\\) is the corresponding figure of merit with case \\(k\\) deleted from the analysis. To adhere to convention and to keep the notation simple the \\(\\{1\\}\\) index on every figure of merit symbol is suppressed (unless it is absolutely necessary for clarity). Recall from book Chapter 07 that the jackknife is a way of teasing out the case-dependence: the left hand side of Eqn. (9.1) literally has a case index \\(k\\), with \\(k\\) running from 1 to \\(K\\), where \\(K\\) is the total number of cases: \\(K=K_1+K_2\\). Hillis has proposed a centering transformation on the pseudovalues (Hillis calls them “normalized” pseudovalues but to the author “centering” is a more accurate and descriptive term - Normalize: (In mathematics) multiply (a series, function, or item of data) by a factor that makes the norm or some associated quantity such as an integral equal to a desired value (usually 1). New Oxford American Dictionary, 2016): \\[\\begin{equation} Y_{ijk}=Y&#39;_{ijk}+\\left (\\theta_{ij} - Y&#39;_{ij\\bullet} \\right ) \\tag{3.2} \\end{equation}\\] Note: the bullet symbol denotes an average over the corresponding index. The effect of this transformation is that the average of the centered pseudovalues over the case index is identical to the corresponding estimate of the figure of merit: \\[\\begin{equation} Y_{ij\\bullet}=Y&#39;_{ij\\bullet}+\\left (\\theta_{ij} - Y&#39;_{ij\\bullet} \\right )=\\theta_{ij} \\tag{3.3} \\end{equation}\\] This has the advantage that all confidence intervals are properly centered. The transformation is unnecessary if one uses the Wilcoxon as the figure-of-merit, as the pseudovalues calculated using the Wilcoxon as the figure of merit are automatically centered (it is left as an exercise for the reader to show that this statement is true). It is understood that, unless explicitly stated otherwise, all calculations from now on will use centered pseudovalues. Consider \\(N\\) replications of a MRMC study, where a replication means repetition of the study with the same treatments, readers and case-set \\(\\{1\\}\\). For \\(N\\) replications per treatment-reader-case combination, the DBM model for the pseudovalues is (\\(n\\) is the replication index, usually \\(n\\) = 1, but kept here for now): \\[\\begin{equation} Y_{n(ijk)} = \\mu + \\tau_i+ R_j + C_k + (\\tau R)_{ij}+ (\\tau C)_{ik}+ (R C)_{jk} + (\\tau RC)_{ijk}+ \\epsilon_{n(ijk)} \\tag{3.4} \\end{equation}\\] The term \\(\\mu\\) is a constant. By definition, the treatment effect \\(\\tau_i\\) is subject to the constraint: \\[\\begin{equation} \\sum_{i=1}^{I}\\tau_i=0\\Rightarrow \\tau_\\bullet=0 \\tag{3.5} \\end{equation}\\] It is shown below, Eqn. (9.9), that this constraint ensures that \\(\\mu\\) has the interpretation as the average of the pseudovalues over treatments, readers, cases and replications, if any. The notation for the replication index, i.e., \\(n(ijk)\\), implies \\(n\\) observations for treatment-reader-case combination \\(ijk\\). With no replications (\\(N\\) = 1) it is convenient to omit the n-symbol. As an example, the parameter \\(\\tau_i\\) is readily estimated as follows: \\[\\begin{equation} Y_{ijk} \\equiv Y_{1(ijk)}\\\\ \\tau_i = Y_{i \\bullet \\bullet} -Y_{\\bullet \\bullet \\bullet} \\tag{3.6} \\end{equation}\\] The basic assumption of the DBM model, Eqn. (9.4), is that the pseudovalues can be regarded as independent and identically distributed observations. That being the case, the pseudovalues can be analyzed by standard ANOVA techniques. 3.6.1 Explanation of terms in the model The right hand side of Eqn. (9.4) consists of one fixed and 7 random effects. The current analysis assumes readers and cases as random factors (RRRC), so by definition \\(R_j\\) and \\(C_k\\) are random effects, and moreover, any term that includes a random factor is a random effect; for example, \\((\\tau R)_{ij}\\) is a random effect because it includes the \\(R\\) factor. Here is a list of the random terms: \\[\\begin{equation} R_j, C_k, (\\tau R)_{ij}, (\\tau C)_{ik}, (RC)_{jk}, (\\tau RC)_{ijk}, \\epsilon_{ijk} \\tag{3.7} \\end{equation}\\] Assumption: Each of the random effects is modeled as a random sample from mutually independent zero-mean normal distributions with variances as specified below: \\[\\begin{equation} R_j \\sim N\\left ( 0,\\sigma_{R}^{2} \\right ) \\\\ C_k \\sim N\\left ( 0,\\sigma_{C}^{2} \\right ) \\\\ (\\tau R)_{ij} \\sim N\\left ( 0,\\sigma_{\\tau R}^{2} \\right ) \\\\ (\\tau C)_{ik} \\sim N\\left ( 0,\\sigma_{\\tau C}^{2} \\right ) \\\\ (RC)_{jk} \\sim N\\left ( 0,\\sigma_{RC}^{2} \\right ) \\\\ (\\tau RC)_{ijk} \\sim N\\left ( 0,\\sigma_{\\tau RC}^{2} \\right ) \\\\ \\epsilon_{ijk} \\sim N\\left ( 0,\\sigma_{\\epsilon}^{2} \\right ) \\tag{3.8} \\end{equation}\\] One could have placed a \\(Y\\) subscript (or superscript) on each of the variances, as they describe fluctuations of the pseudovalues, not FOM values – the latter are the subject of the next chapter. However, this tends to make the notation cumbersome. So here is the convention: Unless explicitly stated otherwise, all variance symbols in this chapter refer to pseudovalues. Another convention: \\((\\tau R)_{ij}\\) is not the product of the treatment and reader factors, rather it is a single factor, namely the treatment-reader factor with \\(IJ\\) levels, subscripted by the index \\(ij\\) and similarly for the other product-like terms in Eqn. (9.7). 3.6.2 Meanings of variance components in the DBM model (TBA this section can be improved) The variances defined in Eqn. (9.7) are collectively termed variance components. Specifically, they are jackknife pseudovalue variance components, to be distinguished from figure of merit (FOM) variance components to be introduced in Chapter 10. They are in order: \\(\\sigma_{R}^{2} ,\\sigma_{C}^{2} \\sigma_{\\tau R}^{2},\\sigma_{\\tau C}^{2},\\sigma_{RC}^{2}, \\sigma_{\\tau RC}^{2},\\sigma_{\\epsilon}^{2}\\). They have the following meanings (all references to “variance” mean “variance of pseudovalues”). The term \\(\\sigma_{R}^{2}\\) is the variance of readers that is independent of treatment or case, which are modeled separately. It is not to be confused with the terms \\(\\sigma_{br+wr}^{2}\\) and \\(\\sigma_{cs+wr}^{2}\\) used in §9.3, which describe the variability of \\(\\theta\\) measured under specified conditions. [A jackknife pseudovalue is a weighted difference of FOM like quantities, Eqn. (9.1). Its meaning will be explored later. For now, a pseudovalue variance is distinct from a FOM variance.] The term \\(\\sigma_{C}^{2}\\) is the variance of cases that is independent of treatment or reader. The term \\(\\sigma_{\\tau R}^{2}\\) is the treatment-dependent variance of readers that was excluded in the definition of \\(\\sigma_{R}^{2}\\). If one were to sample readers and treatments for the same case-set, the net variance would be \\(\\sigma_{R}^{2}+\\sigma_{\\tau R}^{2}+\\sigma_{\\epsilon}^{2}\\). The term \\(\\sigma_{\\tau C}^{2}\\) is the treatment-dependent variance of cases that was excluded in the definition of \\(\\sigma_{C}^{2}\\). So, if one were to sample cases and treatments for the same readers, the net variance would be \\(\\sigma_{C}^{2}+\\sigma_{\\tau C}^{2}+\\sigma_{\\epsilon}^{2}\\). The term \\(\\sigma_{RC}^{2}\\) is the treatment-independent variance of readers and cases that were excluded in the definitions of \\(\\sigma_{R}^{2}\\) and \\(\\sigma_{C}^{2}\\). So, if one were to sample readers and cases for the same treatment, the net variance would be \\(\\sigma_{R}^{2}+\\sigma_{C}^{2}+\\sigma_{RC}^{2}+\\sigma_{\\epsilon}^{2}\\). The term \\(\\sigma_{\\tau RC}^{2}\\) is the variance of treatments, readers and cases that were excluded in the definitions of all the preceding terms in Eqn. (9.7). So, if one were to sample treatments, readers and cases the net variance would be \\(\\sigma_{R}^{2}+\\sigma_{C}^{2}+\\sigma_{\\tau C}^{2}+\\sigma_{RC}^{2}+\\sigma_{\\tau RC}^{2}+\\sigma_{\\epsilon}^{2}\\). The last term, \\(\\sigma_{\\epsilon}^{2}\\) describes the variance arising from different replications of the study using the same treatments, readers and cases. Measuring this variance requires repeating the study several (\\(N\\)) times with the same treatments, readers and cases, and computing the variance of \\(Y_{n(ijk)}\\) , where the additional \\(n\\)-index refers to true replications, \\(n\\) = 1, 2, …, \\(N\\). \\[\\begin{equation} \\sigma_{\\epsilon}^{2}=\\frac{1}{IJK}\\sum_{i=1}^{I}\\sum_{j=1}^{J}\\sum_{k=1}^{k}\\frac{1}{N-1}\\sum_{n=1}^{N}\\left ( Y_{n(ijk)} - Y_{\\bullet (ijk)} \\right )^2 \\tag{3.9} \\end{equation}\\] The right hand side of Eqn. (9.8) is the variance of \\(Y_{n(ijk)}\\), for specific \\(ijk\\), with respect to the replication index \\(n\\), averaged over all \\(ijk\\). In practice \\(N\\) = 1 (i.e., there are no replications) and this variance cannot be estimated (it would imply dividing by zero). It has the meaning of reader inconsistency, usually termed within-reader variability. As will be shown later, the presence of this inestimable term does not limit ones ability to perform significance testing on the treatment effect without having to replicate the whole study, as implied in earlier work (Obuchowski and Rockette 1995). An equation like Eqn. (9.7) is termed a linear model with the left hand side, the pseudovalue “observations”, modeled by a sum of fixed and random terms. Specifically it is a mixed model, because the right hand side has both fixed and random effects. Statistical methods have been developed for analysis of such linear models. One estimates the terms on the right hand side of Eqn. (9.7), it being understood that for the random effects, one estimates the variances of the zero-mean normal distributions, Eqn. (9.7), from which the samples are obtained (by assumption). Estimating the fixed effects is trivial. The term \\(\\mu\\) is estimated by averaging the left hand side of Eqn. (9.4) over all three indices (since \\(N\\) = 1): \\(\\mu=Y_{\\bullet \\bullet \\bullet}\\) Because of the way the treatment effect is defined, Eqn. (9.5), averaging, which involves summing, over the treatment-index \\(i\\), yields zero, and all of the remaining random terms yield zero upon averaging, because they are individually sampled from zero-mean normal distributions. To estimate the treatment effect one takes the difference \\(\\tau_i=Y_{\\bullet \\bullet \\bullet}-\\mu\\). It can be easily seen that the reader and case averaged difference between two different treatments \\(i\\) and \\(i&#39;\\) is estimated by \\(\\tau_i-\\tau_{i&#39;} = Y_{i \\bullet \\bullet} - Y_{i&#39; \\bullet \\bullet}\\). Estimating the strengths of the random terms is a little more complicated. It involves methods adapted from least squares, or maximum likelihood, and more esoteric ways. I do not feel comfortable going into these methods. Instead, results are presented and arguments are made to make them plausible. The starting point is definitions of quantities called mean squares and their expected values. 3.6.3 Definitions of mean-squares Again, to be clear, one should put a \\(Y\\) subscript (or superscript) on each of the following definitions, but that would make the notation unnecessarily cumbersome. In this chapter, all mean-square quantities are calculated using pseudovalues, not figure-of-merit values. The presence of three subscripts on Y should make this clear. Also the replication index and the nesting notation are suppressed. The notation is abbreviated so MST is the mean square corresponding to the treatment effect, etc. The definitions of the mean-squares below match those (when provided) in (Hillis and Berbaum 2004, 1261); the definition of \\(MS(R)\\) in (Hillis 2014, top of page 339) is incorrect. \\[\\begin{equation} \\text{MS(T)}=\\frac{JK\\sum_{i=1}^{I}\\left ( Y_{i \\bullet \\bullet} - Y_{ \\bullet \\bullet \\bullet} \\right )^2}{I-1} \\\\ \\text{MS(R)}=\\frac{IK\\sum_{j=1}^{J}\\left ( Y_{\\bullet j \\bullet} - Y_{ \\bullet \\bullet \\bullet} \\right )^2}{J-1} \\\\ \\text{MS(C)}=\\frac{IJ\\sum_{k=1}^{K}\\left ( Y_{\\bullet \\bullet k} - Y_{ \\bullet \\bullet \\bullet} \\right )^2}{K-1} \\\\ \\text{MS(TR)}=\\frac{K\\sum_{i=1}^{I}\\sum_{j=1}^{J}\\left ( Y_{i j \\bullet} - Y_{i \\bullet \\bullet} - Y_{\\bullet j \\bullet} + Y_{ \\bullet \\bullet \\bullet} \\right )^2}{(I-1)(J-1)} \\\\ \\text{MS(TC)}=\\frac{J\\sum_{i=1}^{I}\\sum_{k=1}^{K}\\left ( Y_{i \\bullet k} - Y_{i \\bullet \\bullet} - Y_{\\bullet \\bullet k} + Y_{ \\bullet \\bullet \\bullet} \\right )^2}{(I-1)(K-1)} \\\\ \\text{MS(RC)}=\\frac{I\\sum_{j=1}^{J}\\sum_{k=1}^{K}\\left ( Y_{\\bullet j k} - Y_{\\bullet j \\bullet} - Y_{\\bullet \\bullet k} + Y_{ \\bullet \\bullet \\bullet} \\right )^2}{(J-1)(K-1)}\\\\ \\text{MS(TRC)}=\\frac{\\sum_{i=1}^{I}\\sum_{j=1}^{J}\\sum_{k=1}^{K}\\left ( Y_{i j k} - Y_{i j \\bullet} - Y_{i \\bullet k} - Y_{\\bullet j k} + Y_{i \\bullet \\bullet} + Y_{\\bullet j \\bullet} + Y_{\\bullet \\bullet k} - Y_{ \\bullet \\bullet \\bullet} \\right )^2}{(I-1)(J-1)K-1)} \\tag{3.10} \\end{equation}\\] Note the absence of \\(MSE\\), corresponding to the \\(\\epsilon\\) term on the right hand side of Eqn. (9.4). With only one observation per treatment-reader-case combination, MSE cannot be estimated; it effectively gets folded into the \\(MS(TRC)\\) term. 3.7 Expected values of mean squares “In our original formulation [2], expected mean squares for the ANOVA were derived from a restricted parameterization in which mixed-factor interactions sum to zero over indexes of fixed effects. In the restricted parameterization, the mixed effects are correlated, parameters are sometimes awkward to define [17], and extension to unbalanced designs is dubious [17, 18]. In this article, we recommend the unrestricted parameterization. The restricted and unrestricted parameterizations are special cases of a general model by Scheffe [19] that allows an arbitrary covariance structure among experimental units within a level of a random factor. Tables 1 and 2 show the ANOVA tables with expected mean squares for the unrestricted formulation.” — (Dorfman, Berbaum, and Lenth 1995) The mean squares on the left hand side of Eqn. (9.12) can be calculated directly from the pseudovalues. The next step in the analysis is to obtain expressions for their expected values in terms of the variances defined in Eqn. (9.7). Assuming no replications, i.e., \\(N\\) = 1, the expected mean squares are as follows, Table 9.1; understanding how this table is derived, would lead the author well outside his expertise and the scope of this book; suffice to say that these are unconstrained estimates (as summarized in the quotation above) which are different from the constrained estimates appearing in the original DBM publication (Dorfman, Berbaum, and Metz 1992). Table 9.1 Unconstrained expected values of mean-squares, as in (Dorfman, Berbaum, and Lenth 1995) Source df E(MS) T (I-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(\\sigma_{\\tau RC}^{2}\\) + \\(K\\sigma_{\\tau R}^{2}\\) + \\(J\\sigma_{\\tau C}^{2}\\) + \\(JK\\sigma_{\\tau}^{2}\\) R (J-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(I\\sigma_{RC}^{2}\\) + \\(IK\\sigma_{R}^{2}\\) + \\(K\\sigma_{\\tau R}^{2}\\) C (K-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(I\\sigma_{RC}^{2}\\) + \\(IJ\\sigma_{C}^{2}\\) + \\(J\\sigma_{\\tau C}^{2}\\) TR (I-1)(J-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(\\sigma_{\\tau RC}^{2}\\) + \\(K\\sigma_{\\tau R}^{2}\\) TC (I-1)(K-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(\\sigma_{\\tau RC}^{2}\\) + \\(J\\sigma_{\\tau C}^{2}\\) RC (J-1)(K-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(I\\sigma_{RC}^{2}\\) TRC (I-1)(J-1)(K-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(\\sigma_{\\tau RC}^{2}\\) \\(\\epsilon\\) \\(N-1=0\\) \\(\\sigma_{\\epsilon}^{2}\\) In Tables 9.1 the following notation is used as a shorthand: \\[\\begin{equation} \\sigma_{\\tau}^{2}=\\frac{1}{I-1}\\sum_{i=1}^{I}\\left ( Y_{i \\bullet \\bullet} - Y_{\\bullet \\bullet \\bullet} \\right )^2 \\tag{3.11} \\end{equation}\\] Since treatment is a fixed effect, the variance symbol \\(\\sigma_{\\tau}^{2}\\), which is used for notational consistency in Tables 9.1 - 9.3, could cause confusion. The right hand side “looks like” a variance, indeed one that could be calculated for just two treatments but, of course, random sampling from a distribution of treatments is not the intent of the notation. With this explanation, I trust the reader will not be confused. 3.8 Random-reader random-case (RRRC) analysis Both readers and cases are regarded as random factors. The expected mean squares in Table 9.1 are variance-like quantities; specifically, they are weighted linear combinations of the variances appearing in Eqn. (9.7). For single factors the column headed “degrees of freedom” (\\(df\\)) is one less than the number of levels of the corresponding factor; estimating a variance requires first estimating the mean, which imposes a constraint, thereby decreasing \\(df\\) by one. For interaction terms, \\(df\\) is the product of the degrees of freedom for the individual factors. As an example, the term \\((\\tau RC)_{ijk}\\) contains three individual factors, and therefore \\(df = (I-1)(J-1)(K-1)\\). The number of degrees of freedom can be thought of as the amount of information available in estimating a mean square. As a special case, with no replications, the \\(\\epsilon\\) term has zero \\(df\\) as \\(N-1 = 0\\). With only one observation \\(Y_{1(ijk)}\\) there is no information to estimate the variance corresponding to the \\(\\epsilon\\) term. To estimate this term one needs to replicate the study several times – each time the same readers interpret the same cases in all treatments – a very boring task for the reader and totally unnecessary from the researcher’s point of view. 3.8.1 Example 1: Calculation of mean squares We choose dataset02 to illustrate calculation of mean squares for pseudovalues. This is referred to in the book as the “VD” dataset (Van Dyke et al. 1993). It consists of 114 cases, 45 of which are diseased, interpreted in two treatments (“0” = single spin echo MRI, “1” = cine-MRI) by five radiologists using the ROC paradigm. The first line below computes the pseudovalues and extracts the numbers of treatmenets, readers and cases, used in the subsequent calculations of mean squares. Y &lt;- UtilPseudoValues(dataset02, FOM = &quot;Wilcoxon&quot;)$jkPseudoValues I &lt;- dim(Y)[1];J &lt;- dim(Y)[2];K &lt;- dim(Y)[3] msT &lt;- 0 for (i in 1:I) { # OK msT &lt;- msT + (mean(Y[i, , ]) - mean(Y))^2 } msT &lt;- msT * K * J/(I - 1) msTC &lt;- 0 for (i in 1:I) { for (k in 1:K) { # OK msTC &lt;- msTC + (mean(Y[i, , k]) - mean(Y[i, , ]) - mean(Y[, , k]) + mean(Y))^2 } msTC &lt;- msTC * J/((I - 1) * (K - 1)) } msR &lt;- 0 for (j in 1:J) { # OK msR &lt;- msR + (mean(Y[, j, ]) - mean(Y))^2 } msR &lt;- msR * K * I/(J - 1) msC &lt;- 0 for (k in 1:K) { # Not used subsequently msC &lt;- msC + (mean(Y[, , k]) - mean(Y))^2 } msC &lt;- msC * I * J/(K - 1) msTR &lt;- 0 for (i in 1:I) { for (j in 1:J) { # OK msTR &lt;- msTR + (mean(Y[i, j, ]) - mean(Y[i, , ]) - mean(Y[, j, ]) + mean(Y))^2 } } msTR &lt;- msTR * K/((I - 1) * (J - 1)) msTC &lt;- 0 for (i in 1:I) { for (k in 1:K) { # OK msTC &lt;- msTC + (mean(Y[i, , k]) - mean(Y[i, , ]) - mean(Y[, , k]) + mean(Y))^2 } } msTC &lt;- msTC * J/((I - 1) * (K - 1)) msRC &lt;- 0 for (j in 1:J) { for (k in 1:K) { # ?? Not used subsequently msRC &lt;- msRC + (mean(Y[, j, k]) - mean(Y[, j, ]) - mean(Y[, , k]) + mean(Y))^2 } } msRC &lt;- msRC * I/((J - 1) * (K - 1)) msTRC &lt;- 0 for (i in 1:I) { for (j in 1:J) { for (k in 1:K) { # OK msTRC &lt;- msTRC + (Y[i, j, k] - mean(Y[i, j, ]) - mean(Y[i, , k]) - mean(Y[, j, k]) + mean(Y[i, , ]) + mean(Y[, j, ]) + mean(Y[, , k]) - mean(Y))^2 } } } msTRC &lt;- msTRC/((I - 1) * (J - 1) * (K - 1)) data.frame(&quot;msT&quot; = msT, &quot;msR&quot; = msR, &quot;msC&quot; = msC, &quot;msTR&quot; = msTR, &quot;msTC&quot; = msTC, &quot;msRC&quot; = msRC, &quot;msTRC&quot; = msTRC) #&gt; msT msR msC msTR msTC msRC msTRC #&gt; 1 0.5467634 0.4373268 0.3968699 0.06281749 0.09984808 0.06450106 0.0399716 as.data.frame(UtilMeanSquares(dataset02)[1:7]) #&gt; msT msR msC msTR msTC msRC msTRC #&gt; 1 0.5467634 0.4373268 0.3968699 0.06281749 0.09984808 0.06450106 0.0399716 After displaying the results of the calculation, the results are compared to those calculated by RJafroc function UtilMeanSquares(dataset02). 3.8.2 Significance testing If the NH of no treatment effect is true, i.e., if \\(\\sigma_{\\tau}^{2}\\) = 0, then according to Table 9.1 the following holds (the last term in the row labeled \\(T\\) in Table 9.1 drops out): \\[\\begin{equation} E\\left ( MST\\mid NH \\right ) = \\sigma_{\\epsilon}^{2} + \\sigma_{\\tau RC}^{2} + K\\sigma_{\\tau R}^{2} + J\\sigma_{\\tau C}^{2} \\tag{3.12} \\end{equation}\\] Also, the following linear combination is equal to \\(E\\left ( MST\\mid NH \\right )\\): \\[\\begin{equation} E\\left ( MS(TR) \\right ) + E\\left ( MS(TC) \\right ) - E\\left ( MS(TRC) \\right ) \\\\ = \\left (\\sigma_{\\epsilon}^{2} + \\sigma_{\\tau RC}^{2} + K\\sigma_{\\tau R}^{2} \\right ) + \\left (\\sigma_{\\epsilon}^{2} + \\sigma_{\\tau RC}^{2} + J\\sigma_{\\tau C}^{2} \\right ) -\\left (\\sigma_{\\epsilon}^{2} + \\sigma_{\\tau RC}^{2} \\right ) \\\\ = \\sigma_{\\epsilon}^{2} + \\sigma_{\\tau RC}^{2} + J \\sigma_{\\tau C}^{2} + K\\sigma_{\\tau R}^{2} \\\\ = E\\left ( MS(T)\\mid NH \\right ) \\tag{3.13} \\end{equation}\\] Therefore, under the NH, the ratio: \\[\\begin{equation} \\frac{E\\left ( MS(T)\\mid NH \\right )}{E\\left ( MS(TR) \\right ) + E\\left ( MS(TC) \\right ) - E\\left ( MS(TRC) \\right )} = 1 \\tag{3.14} \\end{equation}\\] In practice, one does not know the expected values – that would require averaging each of these quantities, regarded as random variables, over their respective distributions. Therefore, one defines the following statistic, denoted \\(F_{DBM}\\), using the observed values of the mean squares, calculated almost trivially using Eqn. (9.12): \\[\\begin{equation} F_{DBM} = \\frac{MS(T)}{MS(TR) + MS(TC) - MS(TRC)} \\tag{3.15} \\end{equation}\\] \\(F_{DBM}\\) is a realization of a random variable. A non-zero treatment effect, i.e., \\(\\sigma_{\\tau}^{2} &gt; 0\\), will cause the ratio to be larger than one, because \\(E\\left ( MS(T) \\right)\\) will be larger, see row labeled \\(T\\) in Table 9.1. Therefore values of \\(F_{DBM} &gt; 1\\) will tend to reject the NH. Drawing on a theorem from statistics (Larsen and Marx 2001), under the NH the ratio of two independent mean squares is distributed as a (central) F-statistic with degrees of freedom corresponding to those of the mean squares forming the numerator and denominator of the ratio (Theorem 12.2.5 in “An Introduction to Mathematical Statistics and Its Applications”). Knowing the distribution of the statistic defined by (9.18) under the NH enables hypothesis testing. This is completely analogous to Chapter 08 where knowledge of the distribution of AUC under the NH enabled testing the null hypothesis that the observed value of AUC equals a pre-specified value. Under the NH the left hand side of by (9.18), i.e., \\(F_{DBM}\\), is distributed according to the F-distribution characterized by two numbers: A numerator degrees of freedom (\\(ndf\\)) – determined by the degrees of freedom of the numerator \\(MST\\) of the ratio comprising the F-statistic, i.e., \\(I – 1\\), and A denominator degrees of freedom (\\(ddf\\)) - determined by the degrees of freedom of the denominator of the ratio comprising the F-statistic, to be described below. Summarizing, \\[\\begin{equation} F_{DBM} \\sim F_{ndf,ddf} \\\\ ndf=I-1 \\tag{3.16} \\end{equation}\\] The next topic is estimating \\(ddf\\). 3.8.3 The Satterthwaite approximation The denominator of the F-ratio is MS(TR)+MS(TC)-MS(TRC). This is not a simple mean square. Rather it is a linear combination of mean squares (with coefficients 1, 1 and 1), and the resulting value could even be negative, which is an illegal value for a sample from an F-distribution. In 1941 Satterthwaite (Satterthwaite 1941, 1946) proposed an approximate degree of freedom for a linear combination of simple mean square quantities. Online Appendix 9.A explains the approximation in more detail. The end result is that the mean square quantity described in Eqn. (9.21) has an approximate degree of freedom defined by (this is called the Satterthwaite’s approximation): \\[\\begin{equation} ddf_{Sat}=\\frac{\\left ( MS(TR) + MS(TC) - MS(TRC) \\right )^2}{\\left ( \\frac{MS(TR)^2}{(I-1)(J-1)} + \\frac{MS(TC)^2}{(I-1)(K-1)} + \\frac{MS(TRC)^2}{(I-1)(J-1)(K-1)} \\right )} \\tag{3.17} \\end{equation}\\] The subscript \\(Sat\\) is for Satterthwaite. From Eqn. (9.22) it should be fairly obvious that in general \\(ddf_{Sat}\\) is not an integer. To accommodate possible negative estimates of the denominator, Eqn. (9.21), the original DBM method (Dorfman, Berbaum, and Metz 1992) proposed four expressions for the F-statistic and corresponding expressions for \\(ddf\\). Rather than repeat them here, since they have been superseded by the method described below, the interested reader is referred to Eqn. 6 and Eqn. 7 in (Hillis, Berbaum, and Metz 2008). Hillis (Hillis 2007) proposes the following statistic for testing the null hypothesis (the subscript \\(DBMH\\) give credit to the original formulation by DBM and the subsequent improvements by Hillis): \\[\\begin{equation} F_{DBMH} = \\frac{MS(T)}{MS(TR) + \\max \\left (MS(TC) - MS(TRC), 0 \\right )} \\tag{3.18} \\end{equation}\\] Now the denominator cannot be negative. One can think of the F-statistic \\(F_{DBMH}\\) as a signal-to-noise ratio like quantity, with the difference that both numerator and denominator are variance like quantities. If the “variance” represented by the treatment effect is larger than the variance of the noise tending to mask the treatment effect, then \\(F_{DBMH}\\) tends to be large, which makes the observed treatment “variance” stand out more clearly compared to the noise. Hillis has shown that the left hand side of Eqn. (9.23) is distributed as an F-statistic with ndf defined by Eqn. (9.20), and denominator degrees of freedom defined by: \\[\\begin{equation} ddf_H =\\frac{\\left ( MS(TR) + \\max \\left (MS(TC) - MS(TRC),0 \\right ) \\right )^2}{\\left ( \\frac{MS(TR)^2}{(I-1)(J-1)} \\right )} \\tag{3.19} \\end{equation}\\] Summarizing, \\[\\begin{equation} F_{DBM} \\sim F_{ndf,ddf} \\\\ ndf=I-1 \\tag{3.20} \\end{equation}\\] Instead of 4 rules, as in the original DBM method, the Hillis modification involves just one rule, summarized by Eqns. (9.23) through Eqn. (9.25). Moreover, the F-statistic is constrained to non-negative values. Using simulation testing (Hillis, Berbaum, and Metz 2008) has shown that the DBMH method has better null hypothesis behavior than the original DBM method; the latter tended to be too conservative, typically yielding Type I error rates smaller than the optimal 5%. 3.8.4 Decision rules, p-value and confidence intervals The critical value of the F-statistic \\(F_{1-\\alpha,ndf,ddf_H}\\) is defined such that fraction of the distribution lies to the left of the critical value, in other words it is the quantile function for the F-distribution: \\[\\begin{equation} \\Pr\\left ( F\\leq F_{1-\\alpha,ndf,ddf_H} \\mid F\\sim F_{ndf,ddf_H}\\right ) = 1 - \\alpha \\tag{3.21} \\end{equation}\\] The critical value \\(F_{1-\\alpha,ndf,ddf_H}\\) increases as \\(\\alpha\\) decreases. The value of \\(\\alpha\\), generally chosen to be 0.05, termed the nominal \\(\\alpha\\), is fixed. The decision rule is that if \\(F_{DBMH} &gt; F_{1-\\alpha, ndf, ddf_H}\\) one rejects the NH and otherwise one does not. It follows, from the definition of \\(F_{DBMH}\\), Eqn. (9.23), that rejection of the NH is more likely if: * \\(F_{DBMH}\\) is large, Eqn. (9.18), which occurs if \\(MS(T)\\) is large, meaning the treatment effect is large, and / or \\(MS(TR) + \\max \\left (MS(TC) - MS(TRC),0 \\right )\\) is small, see comments following Eqn. (9.23). * \\(\\alpha\\) is large: for then \\(F_{1-\\alpha,ndf,ddf_H}\\) decreases and is more likely to be exceeded by \\(F_{DBMH}\\). * ndf is large: the more the number of treatment pairings, the greater the chance that at least one pairing will reject the NH. * \\(ddf_H\\) is large: this causes the critical value to decrease, see below, and is more likely to be exceeded by \\(F_{DBMH}\\). 3.8.4.1 Example 2: Code illustrating the F-distribution for different arguments See [BACKGROUND ON THE F-DISTRIBUTION]. 3.8.4.2 p-value and confidence interval **The p-value of the test is the probability, under the NH, that an equal or larger value of the F-statistic than \\(F_{DBMH}\\) could occur by chance. In other words, it is the area under the (central) F-distribution \\(F_{ndf,ddf}\\) that lies above the observed value \\(F_{DBMH}\\): \\[\\begin{equation} p=\\Pr\\left ( F &gt; F_{DBMH} \\mid F \\sim F_{ndf,ddf_H} \\right ) \\tag{3.22} \\end{equation}\\] If \\(p &lt; \\alpha\\) then the NH that all treatments are identical is rejected at significance level \\(\\alpha\\). That informs the researcher that there exists at least one treatment-pair that has a significant difference. To identify which pair(s) are different, one calculates confidence intervals for each paired difference. Hillis has shown that the \\((1-\\alpha)\\) percent confidence interval for \\(Y_{i \\bullet \\bullet} - Y_{i&#39; \\bullet \\bullet}\\) is given by: \\[\\begin{equation} CI_{1-\\alpha}=\\left ( Y_{i \\bullet \\bullet} - Y_{i&#39; \\bullet \\bullet} \\right ) \\pm t_{\\alpha/2;ddf_H} \\sqrt{\\frac{2}{JK}\\left ( MS(TR) + \\max\\left ( MS(TC)-MS(TRC),0 \\right ) \\right )} \\end{equation}\\] Here \\(t_{\\alpha/2;ddf_H}\\) is that value such that \\(\\alpha/2\\) of the central t-distribution with \\(ddf_H\\) degrees of freedom is contained in the upper tail of the distribution: \\[\\begin{equation} \\Pr\\left ( T&gt;t_{\\alpha/2;ddf_H} \\right )=\\alpha/2 \\tag{3.23} \\end{equation}\\] Since centered pseudovalues were used: \\[\\begin{equation} \\left ( Y_{i \\bullet \\bullet} - Y_{i&#39; \\bullet \\bullet} \\right )=\\left ( \\theta_{i \\bullet } - \\theta_{i&#39; \\bullet} \\right ) \\end{equation}\\] Eqn. (9.28) can be rewritten: \\[\\begin{equation} CI_{1-\\alpha}=\\left ( \\theta_{i \\bullet} - \\theta_{i&#39; \\bullet} \\right ) \\pm t_{\\alpha/2;ddf_H} \\sqrt{\\frac{2}{JK}\\left ( MS(TR) + \\max\\left ( MS(TC)-MS(TRC),0 \\right ) \\right )} \\tag{3.24} \\end{equation}\\] For two treatments the following equivalent rules could be adopted to reject the NH: \\(F_{DBMH} &gt; F_{1-\\alpha,ndf,ddf_H}\\) \\(p &lt; \\alpha\\) \\(CI_{1-alpha}\\) excludes zero For more than two treatments the first two rules are equivalent and if a significant difference is found using either of them, then one can use the confidence intervals to determine which treatment pair differences are significantly different from zero. In this book the first F-test is called the overall F-test and the subsequent tests the treatment-pair t-tests. One only conducts treatment pair t-tests if the overall F-test yields a significant result. 3.8.4.3 Example 3: Code illustrating the F-statistic, ddf and p-value for RRRC analysis, Van Dyke data alpha &lt;- 0.05 retMS &lt;- data.frame(&quot;msT&quot; = msT, &quot;msR&quot; = msR, &quot;msC&quot; = msC, &quot;msTR&quot; = msTR, &quot;msTC&quot; = msTC, &quot;msRC&quot; = msRC, &quot;msTRC&quot; = msTRC) F_DBMH_den &lt;- retMS$msTR+max(retMS$msTC - retMS$msTRC,0) # den of Eqn. (9.23) F_DBMH &lt;- retMS$msT / F_DBMH_den # Eqn. (9.23) ndf &lt;- (I-1) ddf_H &lt;- F_DBMH_den^2/(retMS$msTR^2/((I-1)*(J-1))) # Eqn. (9.22) FCrit &lt;- qf(1 - alpha, ndf, ddf_H) pValueH &lt;- 1 - pf(F_DBMH, ndf, ddf_H) retRJafroc &lt;- StSignificanceTesting(dataset = dataset02, FOM = &quot;Wilcoxon&quot;, method = &quot;DBMH&quot;) data.frame(&quot;F_DBMH&quot; = F_DBMH, &quot;ddf_H&quot;= ddf_H, &quot;pValueH&quot; = pValueH) #&gt; F_DBMH ddf_H pValueH #&gt; 1 4.4563187 15.259675 0.051665686 data.frame(&quot;F_DBMH&quot; = retRJafroc$RRRC$FTests$FStat[1], &quot;ddf_H&quot;= retRJafroc$RRRC$FTests$DF[2], &quot;pValueH&quot; = retRJafroc$RRRC$FTests$p[1]) #&gt; F_DBMH ddf_H pValueH #&gt; 1 4.4563187 15.259675 0.051665686 The first output shows the values (\\(F_{DBMH}\\), \\(ddf_H\\), \\(p\\)) calculated by the above code, which closely follows the formulae in this chapter. The next output are the correponding variables yielded by RJafroc. The FOM difference is not significant, whether viewed from the point of view of the F-statistic exceeding the critical value or the observed p-value being larger than alpha. 3.8.4.4 Example 4: Code illustrating the confidence interval calculation for RRRC analysis, Van Dyke data theta &lt;- as.matrix(UtilFigureOfMerit(dataset02, FOM = &quot;Wilcoxon&quot;)) theta_i_dot &lt;- array(dim = I) for (i in 1:I) theta_i_dot[i] &lt;- mean(theta[i,]) trtDiff &lt;- array(dim = c(I,I)) for (i1 in 1:(I-1)) { for (i2 in (i1+1):I) { trtDiff[i1,i2] &lt;- theta_i_dot[i1]- theta_i_dot[i2] } } trtDiff &lt;- trtDiff[!is.na(trtDiff)] nDiffs &lt;- I*(I-1)/2 CI_DIFF_FOM_RRRC &lt;- array(dim = c(nDiffs, 3)) for (i in 1 : nDiffs) { CI_DIFF_FOM_RRRC[i,1] &lt;- qt(alpha/2,df = ddf_H)*sqrt(2*F_DBMH_den/J/K) + trtDiff[i] CI_DIFF_FOM_RRRC[i,2] &lt;- trtDiff[i] CI_DIFF_FOM_RRRC[i,3] &lt;- qt(1-alpha/2,df = ddf_H)*sqrt(2*F_DBMH_den/J/K) + trtDiff[i] print(data.frame(&quot;Lower&quot; = CI_DIFF_FOM_RRRC[i,1], &quot;Mid&quot; = CI_DIFF_FOM_RRRC[i,2], &quot;Upper&quot; = CI_DIFF_FOM_RRRC[i,3])) } #&gt; Lower Mid Upper #&gt; 1 -0.087959499 -0.043800322 0.00035885444 data.frame(&quot;Lower&quot; = retRJafroc$RRRC$ciDiffTrt[1,&quot;CILower&quot;], &quot;Mid&quot; = retRJafroc$RRRC$ciDiffTrt[1,&quot;Estimate&quot;], &quot;Upper&quot; = retRJafroc$RRRC$ciDiffTrt[1,&quot;CIUpper&quot;]) #&gt; Lower Mid Upper #&gt; 1 -0.087959499 -0.043800322 0.00035885444 Again, the first row of output shows the Lower, the Mid-point and the Upper 95% confidence interval. The second row shows the corresponding RJafroc output. The FOM difference is not significant, whether viewed from the point of view of the F-statistic not exceeding the critical value, the observed p-value being larger than alpha or the 95% CI for the difference FOM including zero. 3.9 Fixed-reader random-case (FRRC) analysis The model is the same as in Eqn. (9.4) except one puts \\(\\sigma_{R}^{2}\\) = \\(\\sigma_{\\tau R}^{2}\\) = 0 in Table 9.1. The appropriate test statistic is: \\[\\begin{equation} \\frac{E\\left ( MS(T) \\right )}{E\\left ( MS(TC) \\right )} = \\frac{\\sigma_{\\epsilon}^{2}+\\sigma_{\\tau RC}^{2}+J\\sigma_{\\tau C}^{2}+JK\\sigma_{\\tau}^{2}}{\\sigma_{\\epsilon}^{2}+\\sigma_{\\tau RC}^{2}+J\\sigma_{\\tau C}^{2}} \\end{equation}\\] Under the null hypothesis \\(\\sigma_{\\tau}^{2} = 0\\): \\[\\begin{equation} \\frac{E\\left ( MS(T) \\right )}{E\\left ( MS(TC) \\right )} = 1 \\end{equation}\\] As before, one defines the F-statistic (by replacing expected with observed values) by \\[\\begin{equation} F_{DBM|R}=\\frac{MS(T)}{MS(TC)} \\tag{3.25} \\end{equation}\\] The observed value \\(F_{DBM|R}\\) (the Roe-Metz notation (Roe and Metz 1997) is used which indicates that the factor appearing to the right of the vertical bar is regarded as fixed) is distributed as an F-statistic with \\(\\text{ndf}\\) = \\(I – 1\\) and \\(ddf = (I-1)(K-1)\\); the degrees of freedom follow from the rows labeled \\(T\\) and \\(TC\\) in Table 9.1. Therefore, the distribution of the observed value is (no Satterthwaite approximation needed this time as both numerator and denominator are simple mean-squares): \\[\\begin{equation} F_{DBM|R} \\sim F_{I-1,(I-1)(K-1)} \\tag{3.26} \\end{equation}\\] The null hypothesis is rejected if the observed value of the F- statistic exceeds the critical value: \\[\\begin{equation} F_{DBM|R} &gt; F_{1-\\alpha,I-1,(I-1)(K-1)} \\tag{3.27} \\end{equation}\\] The p-value of the test is the probability that a random sample from the F-distribution Eqn. (9.39), exceeds the observed value: \\[\\begin{equation} p=\\Pr\\left ( F&gt; F_{DBM|R} \\mid F \\sim F_{I-1,(I-1)(K-1)} \\right ) \\tag{3.28} \\end{equation}\\] The \\((1-\\alpha)\\) confidence interval for the inter-treatment reader-averaged difference FOM is given by: \\[\\begin{equation} CI_{1-\\alpha}=\\left ( \\theta_{i \\bullet} - \\theta_{i&#39; \\bullet} \\right ) \\pm t_{\\alpha/2,(I-1)(K-1)}\\sqrt{2\\frac{MS(T)}{JK}} \\tag{3.29} \\end{equation}\\] 3.9.1 Single-reader multiple-treatment analysis With a single reader interpreting cases in two or more treatments, the reader factor must necessarily be regarded as fixed. The preceding analysis is applicable. One simply puts \\(J = 1\\) in the equations above. 3.9.1.1 Example 5: Code illustrating p-values for FRRC analysis, Van Dyke data FDbmFR &lt;- retMS$msT / retMS$msTC ndf &lt;- (I-1); ddf &lt;- (I-1)*(K-1) pValue &lt;- 1 - pf(FDbmFR, ndf, ddf) std_DIFF_FOM_FRRC &lt;- sqrt(2*retMS$msTC/J/K) nDiffs &lt;- I*(I-1)/2 CI_DIFF_FOM_FRRC &lt;- array(dim = c(nDiffs, 3)) for (i in 1 : nDiffs) { CI_DIFF_FOM_FRRC[i,1] &lt;- qt(alpha/2,df = ddf)*std_DIFF_FOM_FRRC + trtDiff[i] CI_DIFF_FOM_FRRC[i,2] &lt;- trtDiff[i] CI_DIFF_FOM_FRRC[i,3] &lt;- qt(1-alpha/2,df = ddf)*std_DIFF_FOM_FRRC + trtDiff[i] print(data.frame(&quot;pValue&quot; = pValue, &quot;Lower&quot; = CI_DIFF_FOM_FRRC[i,1], &quot;Mid&quot; = CI_DIFF_FOM_FRRC[i,2], &quot;Upper&quot; = CI_DIFF_FOM_FRRC[i,3])) } #&gt; pValue Lower Mid Upper #&gt; 1 0.021034969 -0.080883031 -0.043800322 -0.0067176131 data.frame(&quot;pValue&quot; = retRJafroc$FRRC$FTests$p[1], &quot;Lower&quot; = retRJafroc$FRRC$ciDiffTrt[1,&quot;CILower&quot;], &quot;Mid&quot; = retRJafroc$FRRC$ciDiffTrt[1,&quot;Estimate&quot;], &quot;Upper&quot; = retRJafroc$FRRC$ciDiffTrt[1,&quot;CIUpper&quot;]) #&gt; pValue Lower Mid Upper #&gt; 1 0.021034969 -0.080883031 -0.043800322 -0.0067176131 As one might expect, if one “freezes” reader variability, the FOM difference becomes significant, whether viewed from the point of view of the F-statistic exceeding the critical value, the observed p-value being smaller than alpha or the 95% CI for the difference FOM not including zero. 3.10 Random-reader fixed-case (RRFC) analysis The model is the same as in Eqn. Eqn. (9.4) except one puts \\(\\sigma_C^2 = \\sigma_{\\tau C}^2 =0\\) in Table 9.1. It follows that: \\[\\begin{equation} \\frac{E(MS(T))}{E(MS(TR))}=\\frac{\\sigma_\\epsilon^2+\\sigma_{\\tau RC}^2+K\\sigma_{\\tau R}^2+JK\\sigma_{\\tau}^2}{\\sigma_\\epsilon^2+\\sigma_{\\tau RC}^2+K\\sigma_{\\tau R}^2} \\end{equation}\\] Under the null hypothesis \\(\\sigma_\\tau^2 = 0\\): \\[\\begin{equation} \\frac{E(MS(T))}{E(MS(TR))}=1 \\end{equation}\\] Therefore, one defines the F-statistic (replacing expected values with observed values) by: \\[\\begin{equation} F_{DBM|C} \\sim \\frac{MS(T)}{MS(TR)} \\tag{3.30} \\end{equation}\\] The observed value \\(F_{DBM|C}\\) is distributed as an F-statistic with \\(ndf = I – 1\\) and \\(ddf = (I-1)(J-1)\\), see rows labeled \\(T\\) and \\(TR\\) in Table 9.1. \\[\\begin{equation} F_{DBM|C} \\sim F_{I-1,(I-1)(J-1))} \\tag{3.31} \\end{equation}\\] The null hypothesis is rejected if the observed value of the F statistic exceeds the critical value: \\[\\begin{equation} F_{DBM|C} &gt; F_{1-\\alpha,I-1,(I-1)(J-1))} \\tag{3.32} \\end{equation}\\] The p-value of the test is the probability that a random sample from the distribution exceeds the observed value: \\[\\begin{equation} p=\\Pr\\left ( F&gt;F_{DBM|C} \\mid F \\sim F_{I-1,(I-1)(J-1)} \\right ) \\tag{3.33} \\end{equation}\\] The confidence interval for inter-treatment differences is given by (TBA check this): \\[\\begin{equation} CI_{1-\\alpha}=\\left ( \\theta_{i \\bullet} - \\theta_{i&#39; \\bullet} \\right ) \\pm t_{\\alpha/2,(I-1)(J-1)}\\sqrt{2\\frac{MS(TR)}{JK}} \\tag{3.34} \\end{equation}\\] 3.10.0.1 Example 6: Code illustrating analysis for RRFC analysis, Van Dyke data FDbmFC &lt;- retMS$msT / retMS$msTR ndf &lt;- (I-1) ddf &lt;- (I-1)*(J-1) pValue &lt;- 1 - pf(FDbmFC, ndf, ddf) nDiffs &lt;- I*(I-1)/2 CI_DIFF_FOM_RRFC &lt;- array(dim = c(nDiffs, 3)) for (i in 1 : nDiffs) { CI_DIFF_FOM_RRFC[i,1] &lt;- qt(alpha/2,df = ddf)*sqrt(2*retMS$msTR/J/K) + trtDiff[i] CI_DIFF_FOM_RRFC[i,2] &lt;- trtDiff[i] CI_DIFF_FOM_RRFC[i,3] &lt;- qt(1-alpha/2,df = ddf)*sqrt(2*retMS$msTR/J/K) + trtDiff[i] print(data.frame(&quot;pValue&quot; = pValue, &quot;Lower&quot; = CI_DIFF_FOM_RRFC[i,1], &quot;Mid&quot; = CI_DIFF_FOM_RRFC[i,2], &quot;Upper&quot; = CI_DIFF_FOM_RRFC[i,3])) } #&gt; pValue Lower Mid Upper #&gt; 1 0.041958752 -0.085020224 -0.043800322 -0.0025804202 data.frame(&quot;pValue&quot; = retRJafroc$RRFC$FTests$p[1], &quot;Lower&quot; = retRJafroc$RRFC$ciDiffTrt[1,&quot;CILower&quot;], &quot;Mid&quot; = retRJafroc$RRFC$ciDiffTrt[1,&quot;Estimate&quot;], &quot;Upper&quot; = retRJafroc$RRFC$ciDiffTrt[1,&quot;CIUpper&quot;]) #&gt; pValue Lower Mid Upper #&gt; 1 0.041958752 -0.085020224 -0.043800322 -0.0025804202 3.11 DBMH analysis: Example 1, Van Dyke Data 3.12 DBMH analysis: Example 2, VolumeRad data 3.13 Validation of DBMH analysis 3.14 The meaning of pseudovalues 3.15 Summary This chapter has detailed analysis of MRMC ROC data using the DBMH method. A reason for the level of detail is that almost all of the material carries over to other data collection paradigms, and a thorough understanding of the relatively simple ROC paradigm data is helpful to understanding the more complex ones. DBMH has been used in several hundred ROC studies (Prof. Kevin Berbaum, private communication ca. 2010). While the method allows generalization of a study finding, e.g., rejection of the NH, to the population of readers and cases, the author believes this is sometimes taken too literally. If a study is done at a single hospital, then the radiologists tend to be more homogenous as compared to sampling radiologists from different hospitals. This is because close interactions between radiologists at a hospital tend to homogenize reading styles and performance. A similar issue applies to patient characteristics, which are also expected to vary more between different geographical locations than within a given location served by the hospital. This means is that single hospital study based p-values may tend to be biased downwards, declaring differences that may not be replicable if a wider sampling “net” were used using the same sample size. The price paid for a wider sampling net is that one must use more readers and cases to achieve the same sensitivity to genuine treatment effects, i.e., statistical power (i.e., there is no “free-lunch”). A third MRMC ROC method, due to Clarkson, Kupinski and Barrett19,20, implemented in open-source JAVA software by Gallas and colleagues22,44 (http://didsr.github.io/iMRMC/) is available on the web. Clarkson et al19,20 provide a probabilistic rationale for the DBM model, provided the figure of merit is the empirical \\(AUC\\). The method is elegant but it is only applicable as long as one is using the empirical AUC as the figure of merit (FOM) for quantifying observer performance. In contrast the DBMH approach outlined in this chapter, and the approach outlined in the following chapter, are applicable to any scalar FOM. Broader applicability ensures that significance-testing methods described in this, and the following chapter, apply to other ROC FOMs, such as binormal model or other fitted AUCs, and more importantly, to other observer performance paradigms, such as free-response ROC paradigm. An advantage of the Clarkson et al. approach is that it predicts truth-state dependence of the variance components. One knows from modeling ROC data that diseased cases tend to have greater variance than non-diseased ones, and there is no reason to suspect that similar differences do not exist between the variance components. Testing validity of an analysis method via simulation testing is only as good as the simulator used to generate the datasets, and this is where current research is at a bottleneck. The simulator plays a central role in ROC analysis. In the author’s opinion this is not widely appreciated. In contrast, simulators are taken very seriously in other disciplines, such as cosmology, high-energy physics and weather forecasting. The simulator used to validate3 DBMH is that proposed by Roe and Metz39 in 1997. This simulator has several shortcomings. (a) It assumes that the ratings are distributed like an equal-variance binormal model, which is not true for most clinical datasets (recall that the b-parameter of the binormal model is usually less than one). Work extending this simulator to unequal variance has been published3. (b) It does not take into account that some lesions are not visible, which is the basis of the contaminated binormal model (CBM). A CBM model based simulator would use equal variance distributions with the difference that the distribution for diseased cases would be a mixture distribution with two peaks. The radiological search model (RSM) of free-response data, Chapter 16 &amp;17 also implies a mixture distribution for diseased cases, and it goes farther, as it predicts some cases yield no z-samples, which means they will always be rated in the lowest bin no matter how low the reporting threshold. Both CBM and RSM account for truth dependence by accounting for the underlying perceptual process. (c) The Roe-Metz simulator is out dated; the parameter values are based on datasets then available (prior to 1997). Medical imaging technology has changed substantially in the intervening decades. (d) Finally, the methodology used to arrive at the proposed parameter values is not clearly described. Needed is a more realistic simulator, incorporating knowledge from alternative ROC models and paradigms that is calibrated, by a clearly defined method, to current datasets. Since ROC studies in medical imaging have serious health-care related consequences, no method should be used unless it has been thoroughly validated. Much work still remains to be done in proper simulator design, on which validation is dependent. 3.16 Things for me to think about 3.16.1 Expected values of mean squares Assuming no replications the expected mean squares are as follows, Table 9.1; understanding how this table is derived, would lead the author well outside his expertise and the scope of this book; suffice to say that these are unconstrained estimates (as summarized in the quotation above) which are different from the constrained estimates appearing in the original DBM publication (Dorfman, Berbaum, and Metz 1992), Table 9.2; the differences between these two types of estimates is summarized in (Dorfman, Berbaum, and Lenth 1995). For reference, Table 9.3 is the table published in the most recent paper that I am aware of (Hillis 2014). All three tables are different! In this chapter I will stick to Table 9.1 for the subsequent development. Table 9.1 Unconstrained expected values of mean-squares, as in (Dorfman, Berbaum, and Lenth 1995) Source df E(MS) T (I-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(\\sigma_{\\tau RC}^{2}\\) + \\(K\\sigma_{\\tau R}^{2}\\) + \\(J\\sigma_{\\tau C}^{2}\\) + \\(JK\\sigma_{\\tau}^{2}\\) R (J-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(I\\sigma_{RC}^{2}\\) + \\(IK\\sigma_{R}^{2}\\) + \\(K\\sigma_{\\tau R}^{2}\\) C (K-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(I\\sigma_{RC}^{2}\\) + \\(IJ\\sigma_{C}^{2}\\) + \\(J\\sigma_{\\tau C}^{2}\\) TR (I-1)(J-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(\\sigma_{\\tau RC}^{2}\\) + \\(K\\sigma_{\\tau R}^{2}\\) TC (I-1)(K-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(\\sigma_{\\tau RC}^{2}\\) + \\(J\\sigma_{\\tau C}^{2}\\) RC (J-1)(K-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(I\\sigma_{RC}^{2}\\) TRC (I-1)(J-1)(K-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(\\sigma_{\\tau RC}^{2}\\) \\(\\epsilon\\) \\(N-1=0\\) \\(\\sigma_{\\epsilon}^{2}\\) Table 9.2 Constrained expected values of mean-squares, as in (Dorfman, Berbaum, and Metz 1992) Source df E(MS) T (I-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(\\sigma_{\\tau RC}^{2}\\) + \\(K\\sigma_{\\tau R}^{2}\\) + \\(J\\sigma_{\\tau C}^{2}\\) + \\(JK\\sigma_{\\tau}^{2}\\) R (J-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(I\\sigma_{RC}^{2}\\) + \\(IK\\sigma_{R}^{2}\\) C (K-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(I\\sigma_{RC}^{2}\\) + \\(IJ\\sigma_{C}^{2}\\) TR (I-1)(J-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(\\sigma_{\\tau RC}^{2}\\) + \\(K\\sigma_{\\tau R}^{2}\\) TC (I-1)(K-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(\\sigma_{\\tau RC}^{2}\\) + \\(J\\sigma_{\\tau C}^{2}\\) RC (J-1)(K-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(I\\sigma_{RC}^{2}\\) TRC (I-1)(J-1)(K-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(\\sigma_{\\tau RC}^{2}\\) \\(\\epsilon\\) 0 \\(\\sigma_{\\epsilon}^{2}\\) Table 9.3 As in Hillis “marginal-means ANOVA paper” (Hillis 2014) Source df E(MS) T (I-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(\\sigma_{\\tau RC}^{2}\\) + \\(K\\sigma_{\\tau R}^{2}\\) + \\(J\\sigma_{\\tau C}^{2}\\) + \\(JK\\sigma_{\\tau}^{2}\\) R (J-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(\\sigma_{\\tau RC}^{2}\\) + \\(I\\sigma_{RC}^{2}\\) + \\(IK\\sigma_{R}^{2}\\) + \\(K\\sigma_{\\tau R}^{2}\\) C (K-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(\\sigma_{\\tau RC}^{2}\\) + \\(I\\sigma_{RC}^{2}\\) + \\(IJ\\sigma_{C}^{2}\\) + \\(J\\sigma_{\\tau C}^{2}\\) TR (I-1)(J-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(\\sigma_{\\tau RC}^{2}\\) + \\(K\\sigma_{\\tau R}^{2}\\) TC (I-1)(K-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(\\sigma_{\\tau RC}^{2}\\) + \\(J\\sigma_{\\tau C}^{2}\\) RC (J-1)(K-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(\\sigma_{\\tau RC}^{2}\\) + \\(I\\sigma_{RC}^{2}\\) TRC (I-1)(J-1)(K-1) \\(\\sigma_{\\epsilon}^{2}\\) + \\(\\sigma_{\\tau RC}^{2}\\) \\(\\epsilon\\) 0 \\(\\sigma_{\\epsilon}^{2}\\) 3.17 References REFERENCES "],
["ORHAnalysis.html", "Chapter 4 Obuchowski Rockette Hillis2 (ORH) Analysis 4.1 Introduction 4.2 Single-reader multiple-treatment model 4.3 Multiple-reader multiple-treatment ORH model 4.4 Discussion/Summary 4.5 References", " Chapter 4 Obuchowski Rockette Hillis2 (ORH) Analysis 4.1 Introduction The previous chapter described the DBM significance testing procedure (Dorfman, Berbaum, and Metz 1992) for analyzing MRMC ROC data, along with improvements suggested by Hillis. Because the method assumes that jackknife pseudovalues can be regarded as independent and identically distributed case-level figures of merit, it has been criticized by Hillis who states that the method “works” but lacks firm statistical foundations (Hillis et al. 2005; Hillis 2007; Hillis, Berbaum, and Metz 2008). If a method works there must be good reasons why it works and the last section of the previous chapter, §9.13, gave a justification for why the method works. Specifically, the empirical AUC pseudovalues qualify as case-level FOMs - this property was also noted by (Hajian-Tilaki et al. 1997). However, this property applies only to the empirical AUC, so an alternate approach that applies to any figure of merit is desirable. This chapter presents Hillis’ preferred alternative to the DBMH approach. He has argued that the DBMH method can be regarded as a “working model that gives the right results”, but a method based on an earlier publication (Obuchowski and Rockette 1995) by Obuchowski and Rockette, which does not depend on pseudovalues, and predicts more or less the same results, is preferable from a conceptual viewpoint. Since, besides showing the correspondence, Hillis has made significant improvements to the original methodology, this chapter is named “ORH Analysis”, where ORH stands for Obuchowski, Rockette and Hillis. The ORH method has advantages in being able to handle more complex study designs (Hillis 2014) that are outside the scope of this book (the author acknowledges a private communication from Dr. Obuchowski, ca. 2006, that demonstrated the flexibility afforded by the OR approach) and it is likely that applications to other FOMs (e.g., the FROC paradigm uses a rather different FOM from empirical ROC-AUC) are better performed with the ORH method. This chapter starts with a gentle introduction to the Obuchowski and Rockette method. The reason is that the method was rather opaque to me, an I suspect, most users. Part of the problem, in my opinion, is the notation, namely lack of usage of the case-set index \\(\\{c\\}\\). A key difference of the Obuchowski and Rockette method from DBMH is in how the error term is modeled by a non-diagonal covariance matrix. The structure of the covariance matrix is examined in some detail as it is key to understanding the ORH method. In the first step of the introduction a single reader interpreting a case-set in multiple treatments is modeled and the results compared to those obtained using DBMH fixed-reader analysis described in the previous chapter. In the second step multiple readers interpreting a case-set in multiple treatments is modeled. The two analyses, DBMH and ORH, are compared for the same dataset. The special cases of fixed-reader and fixed-case analyses are described. Single treatment analysis, where interest is in comparing average performance of readers to a fixed value, is described. Three methods of estimating the covariance matrix are described. 4.2 Single-reader multiple-treatment model Consider a single-reader providing ROC interpretations of a common case-set \\(\\{c\\}\\) in multiple-treatments \\(i\\) (\\(i\\) = 1, 2, …, \\(I\\)). Before proceeding, we note that this is not homologous (i.e., formally equivalent) to multiple-readers providing ROC interpretations in a single treatment, §10.7; this is because reader is a random factor while treatment is not. The figure of merit \\(\\theta\\) is modeled as: \\[\\begin{equation} \\theta_{i\\{c\\}}=\\mu+\\tau_i+\\epsilon_{i\\{c\\}} \\tag{4.1} \\end{equation}\\] In the (Obuchowski and Rockette 1995) one models the figure-of-merit, not the pseudovalues, indeed this is one of the key differences from the DBMH method. Recall that \\(\\{c\\}\\) denotes a set of cases. (4.1) models the observed figure-of-merit \\(\\theta_{i\\{c\\}}\\) as a constant term \\(\\mu\\) plus a treatment dependent term \\(\\tau_i\\) (the treatment-effect) with the constraint: \\[\\begin{equation} \\sum_{i=1}^{I}\\tau_i=0 \\tag{4.2} \\end{equation}\\] The c-index was introduced in (book) Chapter 07. The left hand side of (4.1) is the figure-of-merit \\(\\theta_i\\{c\\}\\) for treatment \\(i\\) and case-set index \\(\\{c\\}\\), where \\(c\\) = 1, 2, …, \\(C\\) denotes different independent case-sets sampled from the population, i.e., different collections of \\(K_1\\) non-diseased and \\(K_2\\) diseased cases, not individual cases. This is one place the case-set index is essential for clarity; without it \\(\\theta_i\\) is a fixed quantity - the figure of merit estimate for treatment \\(i\\) - lacking any index allowing for sampling related variability. Obuchowski and Rockette use a k-index, defined as the “kth repetition of the study involving the same diagnostic test, reader and patient (sic)”. In the author’s opinion, what is meant is a case-set index instead of a repetition index. Repeating a study with the same treatment, reader and cases yields within-reader variability, which is different from sampling the population of cases with new case-sets, which yields case-sampling plus within-reader variability. As noted earlier, within-reader variability cannot be “turned off” and affects the interpretations of all case-sets. Interest is in extrapolating to the population of cases and the only way to do this is to sample different case-sets. It is shown below that usage of the case-set index interpretation yields the same results using the DBMH or the ORH methods. Finally, and this is where I had some difficulty understanding what is going on, there is an additive random error term \\(\\epsilon_{i\\{c\\}}\\) whose sampling behavior is described by a multivariate normal distribution with an I-dimensional zero mean vector and an \\(I \\times I\\) dimensional covariance matrix \\(\\Sigma\\): \\[\\begin{equation} \\epsilon_{i\\{c\\}} \\sim N_I\\left ( \\vec{0} , \\Sigma\\right ) \\tag{4.3} \\end{equation}\\] Here \\(N_I\\) is the I-variate normal distribution (i.e., each sample yields \\(I\\) random numbers). Obuchowski and Rockette assumed the following structure for the covariance matrix (they describe a more general multi-reader model, but here one restricts to the simpler single-reader case): \\[\\begin{equation} \\Sigma_{ii&#39;}=Cov\\left ( \\epsilon_{i\\{c\\}}, \\epsilon_{i&#39;\\{c\\}} \\right )=\\left\\{\\begin{matrix} Var \\qquad (i=i&#39;)\\\\ Cov_1 \\qquad (i\\neq i&#39;) \\end{matrix}\\right. \\tag{4.4} \\end{equation}\\] The reason for the subscript “1” in \\(Cov_1\\) will become clear when one extends this model to multiple readers. The \\(I \\times I\\) covariance matrix \\(\\Sigma\\) is: \\[\\begin{equation} \\Sigma= \\begin{pmatrix} Var &amp; Cov_1 &amp; \\ldots &amp; Cov_1 &amp; Cov_1 \\\\ Cov_1 &amp; Var &amp; \\ldots &amp;Cov_1 &amp; Cov_1 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ Cov_1 &amp; Cov_1 &amp; \\ldots &amp; Var &amp; Cov_1 \\\\ Cov_1 &amp; Cov_1 &amp; \\ldots &amp; Cov_1 &amp; Var \\end{pmatrix} \\tag{4.5} \\end{equation}\\] If \\(I\\) = 2 then \\(\\Sigma\\) is a symmetric 2 x 2 matrix, whose diagonal terms are the common variances in the two treatments (each assumed equal to \\(Var\\)) and whose off-diagonal terms (each assumed equal to \\(Cov_1\\)) are the co-variances. With \\(I\\) = 3 one has a 3 x 3 symmetric matrix with all diagonal elements equal to \\(Var\\) and all off-diagonal terms are equal to \\(Cov_1\\), etc. An important aspect of the Obuchowski and Rockette model is that the variances and co-variances are assumed to be treatment independent. This implies that \\(Var\\) estimates need to be averaged over all treatments. Likewise, \\(Cov_1\\) estimates need to be averaged over all distinct treatment-treatment pairings. A more complex model, with more parameters and therefore more difficult to work with, would allow the variances to be treatment dependent, and the covariances to depend on the specific treatment pairings. For obvious reasons (“Occam’s Razor” or the law of parsimony ) one wishes to start with the simplest model that, one hopes, captures essential characteristics of the data. Some elementary statistical results are presented next. 4.2.1 Definitions of covariance and correlation The covariance of two scalar random variables X and Y is defined by: \\[\\begin{equation} Cov(X,Y) =\\frac{\\sum_{i=1}^{N}(x_{i}-x_{\\bullet})(y_{i}-y_{\\bullet})}{N-1}=E(XY)-E(X)-E(Y) \\tag{4.6} \\end{equation}\\] Here \\(E(X)\\) is the expectation value of the random variable \\(X\\), i.e., the integral of x multiplied by its \\(pdf\\) over the range of \\(x\\): \\[E(X)=\\int pdf(x) x dx\\] The covariance can be thought of as the common part of the variance of two random variables. The variance, a special case of covariance, of \\(X\\) is defined by: \\[Var(X,Y) = Cov(X,X)=E(X^2)-(E(X))^2=\\sigma_x^2\\] It can be shown, using the Cauchy–Schwarz inequality, that: \\[\\mid Cov(X,Y) \\mid^2 \\le Var(X)Var(Y)\\] A related quantity, the correlation \\(\\rho\\) is defined by (the \\(\\sigma\\)s are standard deviations): \\[\\rho_{XY} \\equiv Cor(X,Y)=\\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}\\] It has the property: \\[\\mid \\rho_{XY} \\mid \\le 1\\] 4.2.2 Special case when variables have equal variances Assuming \\(X\\) and \\(Y\\) have the same variance: \\[Var(X)=Var(Y)\\equiv Var\\equiv \\sigma^2\\] A useful theorem applicable to the OR single-reader multiple-treatment model is: \\[\\begin{equation} Var(X-Y)=Var(X)+Var(Y)-2Cov(X,Y)=2(Var-Cov) \\tag{4.7} \\end{equation}\\] The left part of the above equation is general, the right part specializes to the OR single-reader multiple-treatment model where the variances are equal and likewise all covariances in (4.5) are equal) The correlation \\(\\rho_1\\) is defined by (the reason for the subscript 1 on \\(\\rho\\) is the same as the reason for the subscript 1 on \\(Cov_1\\), which will be explained later): \\[\\rho_1=\\frac{Cov_1}{Var}\\] The I x I covariance matrix \\(\\Sigma\\) can be written alternatively as (shown below is the matrix for I = 5; as the matrix is symmetric elements at and above the diagonal are shown): \\[\\begin{equation} \\Sigma = \\begin{bmatrix} \\sigma^2 &amp; \\rho_1\\sigma^2 &amp; \\rho_1\\sigma^2 &amp; \\rho_1\\sigma^2 &amp; \\rho_1\\sigma^2\\\\ &amp; \\sigma^2 &amp; \\rho_1\\sigma^2 &amp; \\rho_1\\sigma^2 &amp; \\rho_1\\sigma^2\\\\ &amp; &amp; \\sigma^2 &amp; \\rho_1\\sigma^2 &amp; \\rho_1\\sigma^2\\\\ &amp; &amp; &amp; \\sigma^2 &amp; \\rho_1\\sigma^2\\\\ &amp; &amp; &amp; &amp; \\sigma^2 \\end{bmatrix} \\tag{4.8} \\end{equation}\\] 4.2.3 Estimation of the covariance matrix An unbiased estimate of the covariance (4.4) follows from: \\[\\begin{equation} \\Sigma_{ii&#39;}=\\frac{1}{C-1}\\sum_{c=1}^{C} \\left ( \\theta_{i\\{c\\}} - \\theta_{i\\{\\bullet\\}} \\right) \\left ( \\theta_{i&#39;\\{c\\}} - \\theta_{i&#39;\\{\\bullet\\}} \\right) \\tag{4.9} \\end{equation}\\] Sampling different case-sets, as required (4.9), is unrealistic and in reality one is stuck with \\(C\\) = 1, i.e., a single dataset. Therefore, direct application of this formula is impossible. However, as seen when this situation was encountered before in (book) Chapter 07, one can use resampling methods to realize, for example, different bootstrap samples, which are resampling-based “stand-ins” for actual case-sets. If \\(B\\) is the total number of bootstraps, then the estimation formula is: \\[\\begin{equation} \\Sigma_{ii&#39;}\\mid_{bs} =\\frac{1}{B-1}\\sum_{b=1}^{B} \\left ( \\theta_{i\\{b\\}} - \\theta_{i\\{\\bullet\\}} \\right) \\left ( \\theta_{i&#39;\\{b\\}} - \\theta_{i&#39;\\{\\bullet\\}} \\right) \\tag{4.10} \\end{equation}\\] (4.10), the bootstrap method of estimating the covariance matrix, is a direct translation of (4.9). Alternatively, one could have used the jackknife FOM values \\(\\theta_{i(k)}\\), i.e., the figure of merit with a particular case removed, for all cases, to estimate the covariance matrix: \\[\\begin{equation} \\Sigma_{ii&#39;}\\mid_{jk} =\\frac{(K-1)^2}{K} \\left [ \\frac{1}{K-1}\\sum_{k=1}^{K} \\left ( \\theta_{i(k)} - \\theta_{i(\\bullet)} \\right) \\left ( \\theta_{i&#39;(k)} - \\theta_{i&#39;(\\bullet)} \\right) \\right ] \\tag{4.11} \\end{equation}\\] Note the subtle difference in notation between (4.9) and EstimateSigmaJackknife and. In the former, the subscript \\(\\{c\\}\\) denotes a set of \\(K\\) cases while in the latter, \\((k)\\) denotes the original case set with a particular case \\(k\\) removed, leaving \\(K-1\\) cases. For simplicity, in this section we depart from the usual two-subscript convention to index each case. So \\(k\\) ranges from 1 to \\(K\\), where the first \\(K_1\\) values represent non-diseased and the following \\(K_2\\) values represent diseased cases. Jackknife figure of merit values are not to be confused with jackknife pseudovalues. The jackknife FOM value corresponding to a particular case is the FOM with the particular case removed. Unlike pseudovalues, jackknife FOM values cannot be regarded as independent and identically distributed. Notice the use of the subscript enclosed in parenthesis \\((k)\\) to denote the FOM with case \\(k\\) removed, i.e., a single case, while in the bootstrap equation one uses the curly brackets \\(\\{b\\}\\) to denote the bth bootstrap case-set, i.e., a whole set of \\(K_1\\) non-diseased and \\(K_2\\) diseased cases, sampled with replacement from the original dataset. Furthermore, the expression for the jackknife covariance contains a variance inflation factor: \\[\\begin{equation} \\frac{(K-1)^2}{K} \\tag{4.12} \\end{equation}\\] This factor multiplies the traditional expression for the covariance, shown in square brackets in (4.11). A third method of estimating the covariance, namely the DeLong et al. method (DeLong, DeLong, and Clarke-Pearson 1988), applicable only to the empirical AUC, is described later. 4.2.4 Meaning of the covariance matrix in (4.5) Suppose one has the luxury of repeatedly sampling case-sets, each consisting of \\(K\\) cases from the population. A single radiologist interprets these cases in \\(I\\) treatments. Therefore, each case-set \\(\\{c\\}\\) yields \\(I\\) figures of merit. The final numbers at ones disposal are \\(\\theta_{i\\{c\\}}\\), where \\(i\\) = 1,2,…,\\(I\\) and \\(c\\) = 1,2,…,\\(C\\). Considering treatment \\(i\\), the variance of the FOM-values for the different case-sets \\(c\\) = 1,2,…,\\(C\\), is an estimate of \\(Var_i\\) for this treatment: \\[\\begin{equation} \\sigma_i^2 \\equiv Var_i = \\frac{1}{C-1}\\sum_{c=1}^{C}\\left ( \\theta_{i\\{c\\}} - \\theta_{i\\{\\bullet\\}} \\right) \\left ( \\theta_{i\\{c\\}} - \\theta_{i\\{\\bullet\\}} \\right) \\tag{4.13} \\end{equation}\\] The process is repeated for all treatments and the \\(I\\)-variance values are averaged. This is the final estimate of \\(Var\\) appearing in (4.3). To estimate the covariance matrix one considers pairs of FOM values for the same case-set \\(\\{c\\}\\) but different treatments, i.e., \\(\\theta_{i\\{c\\}}\\) and \\(\\theta_{i&#39;\\{c\\}}\\); by definition primed and un-primed indices are different. Since they are derived from the same case-set, one expects the values to be correlated. For a particularly easy case-set one expects all I-estimates to be collectively higher than usual. The process is repeated for different case-sets and one calculates the correlation \\(\\rho_{1;ii&#39;}\\) between the two \\(C\\)-length arrays \\(\\theta_{i\\{c\\}}\\) and \\(\\theta_{i&#39;\\{c\\}}\\): \\[\\begin{equation} \\rho_{1;ii&#39;} = \\frac{1}{C-1}\\sum_{c=1}^{C} \\frac {\\left ( \\theta_{i\\{c\\}} - \\theta_{i\\{\\bullet\\}} \\right) \\left ( \\theta_{i&#39;\\{c\\}} - \\theta_{i&#39;\\{\\bullet\\}} \\right)}{\\sigma_i \\sigma_{i&#39;} } \\tag{4.14} \\end{equation}\\] The entire process is repeated for different treatment pairings and the resulting \\(I(I-1)/2\\) distinct values are averaged yielding the final estimate of \\(\\rho_1\\) in (4.8). According to @ref(eq: EstimateRho) one expects the covariance to be smaller than the variance determined as in the previous paragraph. In most situations one expects \\(\\rho_1\\) (for ROC studies) to be positive. There is, perhaps unlikely, a scenario that could lead to anti-correlation and negative. This could occur, with “complementary” treatments, e.g., CT vs. MRI, where one treatment is good for bone imaging and the other for soft-tissue imaging. In this situation what constitutes an easy case-set in one treatment could be a difficult case-set in the other treatment. 4.2.5 Code illustrating the covariance matrix (TBA) As indicated above, the covariance matrix can be estimated using the jackknife or the bootstrap. If the figure of merit is the Wilcoxon statistic, then one can also use the DeLong et al method (DeLong, DeLong, and Clarke-Pearson 1988). In (book) Chapter 07, these methods were described in the context of estimating the variance of AUC. (4.10) and (4.11) extend the jackknife and the bootstrap methods, respectively, to estimating the covariance of AUC (whose diagonal elements are the variances estimated in the earlier chapter). The extension of the DeLong method to covariances is described in Online Appendix 10.A (TBA) and implemented in file VarCovMtrxDLStr.R. The file name stands for “variance covariance matrix according to the DeLong structural components method” described in five unnumbered equations following Eqn. 4 in the cited reference. To minimize clutter, the R functions (for estimating Var and Cov1 using bootstrap, jackknife, and the DeLong methods) are not shown, but they are compiled. To display them clone the book repository and look at the Rmd file corresponding to this output and the sourced R files listed below: source(here(&quot;R/CH10-ORH/Wilcoxon.R&quot;)) source(here(&quot;R/CH10-ORH/VarCov1Bs.R&quot;)) source(here(&quot;R/CH10-ORH/VarCov1Bs.R&quot;)) source(here(&quot;R/CH10-ORH/VarCov1Jk.R&quot;)) source(here(&quot;R/CH10-ORH/VarCovMtrxDLStr.R&quot;)) source(here(&quot;R/CH10-ORH/VarCovs.R&quot;)) The following code chunk extracts (using the DfExtractDataset function) a single-reader multiple-treatment ROC dataset corresponding to the first reader from dataset02, i.e., the Van Dyke or VD dataset. rocData1R &lt;- DfExtractDataset(dataset02, rdrs = 1) #select the 1st reader to be analyzed zik1 &lt;- rocData1R$ratings$NL[,1,,1];K &lt;- dim(zik1)[2];I &lt;- dim(zik1)[1] zik2 &lt;- rocData1R$ratings$LL[,1,,1];K2 &lt;- dim(zik2)[2];K1 &lt;- K-K2;zik1 &lt;- zik1[,1:K1] The following notation is used in the code below: jk = jackknife method bs = boostrap method, with B = number of bootstraps and seed = value. dl = DeLong method rj_jk = RJafroc, covEstMethod = “jackknife” rj_bs = RJafroc, covEstMethod = “bootstrap” For exammple, Cov1_jk is the jackknife estimate of Cov1. Shown below are the results of the jackknife method, first using the code in this repository and next, as a cross-check, using RJafroc function UtilVarComponentsOR: ret1 &lt;- VarCov1_Jk(zik1, zik2) Var &lt;- ret1$Var Cov1 &lt;- ret1$Cov1 # use these (i.e., jackknife) as default values in subsequent code data.frame (&quot;Cov1_jk&quot; = Cov1, &quot;Var_jk&quot; = Var) #&gt; Cov1_jk Var_jk #&gt; 1 0.0003734661 0.0006989006 ret4 &lt;- UtilVarComponentsOR(rocData1R, FOM = &quot;Wilcoxon&quot;)$varComp # default `covEstMethod` is jackknife data.frame (&quot;Cov1_rj_jk&quot; = ret4$VarCom[&quot;Cov1&quot;, &quot;Estimates&quot;], &quot;Var_rj_jk&quot; = ret4$VarCom[&quot;Var&quot;, &quot;Estimates&quot;]) #&gt; data frame with 0 columns and 0 rows Note that the estimates are identical and that the \\(Cov_1\\) estimate is smaller than the \\(Var\\) estimate (their ratio is the correlation \\(\\rho_1 = Cov_1/Var\\) = 0.5343623). Shown next are bootstrap method estimates with increasing number of bootstraps (200, 2000 and 20,000): ret2 &lt;- VarCov1_Bs(zik1, zik2, 200, seed = 100) data.frame (&quot;Cov_bs&quot; = ret2$Cov1, &quot;Var_bs&quot; = ret2$Var) #&gt; Cov_bs Var_bs #&gt; 1 0.000283905 0.0005845354 ret2 &lt;- VarCov1_Bs(zik1, zik2, 2000, seed = 100) data.frame (&quot;Cov_bs&quot; = ret2$Cov1, &quot;Var_bs&quot; = ret2$Var) #&gt; Cov_bs Var_bs #&gt; 1 0.0003466804 0.0006738506 ret2 &lt;- VarCov1_Bs(zik1, zik2, 20000, seed = 100) data.frame (&quot;Cov_bs&quot; = ret2$Cov1, &quot;Var_bs&quot; = ret2$Var) #&gt; Cov_bs Var_bs #&gt; 1 0.0003680714 0.0006862668 With increasing number of bootstraps the values approach the jackknife estimates. Following, as a cross check, are results of bootstrap method as calculated by the RJafroc function UtilVarComponentsOR: ret5 &lt;- UtilVarComponentsOR(rocData1R, FOM = &quot;Wilcoxon&quot;, covEstMethod = &quot;bootstrap&quot;, nBoots = 2000, seed = 100)$varComp data.frame (&quot;Cov_rj_bs&quot; = ret5$cov1, &quot;Var_rj_bs&quot; = ret5$var) #&gt; data frame with 0 columns and 0 rows Note that the two estimates are identical provided the seeds are identical. Following are results of the DeLong covariance estimation method, the first output using this repository code and the second using the RJafroc function UtilVarComponentsOR with appropriate arguments: mtrxDLStr &lt;- VarCovMtrxDLStr(rocData1R) ret3 &lt;- VarCovs(mtrxDLStr) data.frame (&quot;Cov_dl&quot; = ret3$cov1, &quot;Var_dl&quot; = ret3$var) #&gt; Cov_dl Var_dl #&gt; 1 0.0003684357 0.0006900766 ret5 &lt;- UtilVarComponentsOR(rocData1R, FOM = &quot;Wilcoxon&quot;, covEstMethod = &quot;DeLong&quot;)$varComp data.frame (&quot;Cov_rj_dl&quot; = ret5$cov1, &quot;Var_rj_dl&quot; = ret5$var) #&gt; data frame with 0 columns and 0 rows Note that the two estimates are identical and that the DeLong estimate are close to the bootstrap estimates using 20,000 bootstraps. The close correspondence is only expected when using the Wilcoxon figure of merit. 4.2.6 Significance testing The covariance matrix is needed for significance testing. Define the mean square corresponding to the treatment effect, denoted \\(MS(T)\\), by: \\[\\begin{equation} MS(T)=\\frac{1}{I-1}\\sum_{i=1}^{I}(\\theta_i-\\theta_\\bullet)^2 \\tag{4.15} \\end{equation}\\] Unlike the previous chapter, all mean square quantities defined in this chapter are based on FOMs, not pseudovalues. It can be shown that under the null hypothesis (that all treatments have identical performances) the test statistic \\(\\chi_{1R}\\) defined below (the \\(1R\\) subscript denotes single-reader analysis) is distributed approximately as a \\(\\chi^2\\) distribution with \\(I-1\\) degrees of freedom, i.e., \\[\\begin{equation} \\chi_{1R} \\equiv \\frac{(I-1)MS(T)}{Var-Cov_1} \\sim \\chi_{I-1}^{2} \\tag{4.16} \\end{equation}\\] (4.16) is from §5.4 (Hillis 2007) with two covariance terms “zeroed out” because they are multiplied by \\(J-1 = 0\\) (since, in this example, we are restricting to \\(J=1\\)). Or equivalently, in terms of the F-distribution (Hillis et al. 2005): \\[\\begin{equation} F_{1R} \\equiv \\frac{MS(T)}{Var-Cov_1} \\sim F_{I-1, \\infty} \\tag{4.17} \\end{equation}\\] 4.2.6.1 An aside on the relation between the chisquare and the F-distribution with infinite ddf Define \\(D_{1-\\alpha}\\), the \\((1-\\alpha)\\) quantile of distribution \\(D\\), as that “cutoff” value such that the probability of observing a random sample \\(d\\) less than or equal to \\(D_{1-\\alpha}\\) is \\((1-\\alpha)\\). In other words, \\[\\begin{equation} \\Pr(d\\leq D_{1-alpha} \\mid d \\sim D)=1-\\alpha \\tag{4.18} \\end{equation}\\] With definition (4.18), the \\((1-\\alpha)\\) quantile of the \\(\\chi_{I-1}^2\\) distribution, i.e., \\(\\chi_{1-\\alpha,I-1}^2\\), is related to the \\((1-\\alpha)\\) quantile of the \\(F_{I-1,\\infty}\\) distribution, i.e., \\(F_{1-\\alpha,I-1,\\infty}\\), as follows (see Hillis et al. 2005, Eq. 22): \\[\\begin{equation} \\frac{\\chi_{1-\\alpha,I-1}^{2}}{I-1} = F_{1-\\alpha,I-1,\\infty} \\tag{4.19} \\end{equation}\\] (4.19) implies that the \\((1-\\alpha)\\) quantile of the F-distribution with \\(ndf=(I-1)\\) and \\(ddf=\\infty\\) equals the \\((1-\\alpha)\\) quantile of the \\(\\chi_{I-1}^2\\) distribution divided by \\((I-1)\\). Here is an R illustration of this theorem for \\(I-1 = 4\\) and \\(\\alpha = 0.05\\): qf(0.05, 4, Inf) #&gt; [1] 0.1776808 qchisq(0.05,4)/4 #&gt; [1] 0.1776808 4.2.7 p-value and confidence interval The p-value is the probability that a sample from the \\(F_{I-1,\\infty}\\) distribution is greater than the observed value of the test statistic, namely: \\[\\begin{equation} p\\equiv \\Pr(f&gt;F_{1R} \\mid f \\sim F_{I-1,\\infty}) \\tag{4.20} \\end{equation}\\] The \\((1-\\alpha)\\) confidence interval for the inter-treatment FOM difference is given by: \\[\\begin{equation} CI_{1-\\alpha,1RMT} = (\\theta_{i\\bullet} - \\theta_{i&#39;\\bullet}) \\pm t_{\\alpha/2,\\infty} \\sqrt{2(Var-Cov_1)} \\tag{4.21} \\end{equation}\\] Comparing (4.21) to (4.7) shows that the term \\(\\sqrt{2(Var-Cov_1)}\\) is the standard error of the inter-treatment FOM difference, whose square root is the standard deviation. The term \\(t_{\\alpha/2,\\infty}\\) is -1.96. Therefore, the confidence interval is constructed by adding and subtracting 1.96 times the standard deviation of the difference from the central value. [One has probably encountered the rule that a 95% confidence interval is plus or minus two standard deviations from the central value. The “2” comes from rounding up 1.96.] 4.2.8 Comparing DBM to Obuchowski and Rockette for single-reader multiple-treatments We have shown two methods for analyzing a single reader in multiple treatments: the DBMH method, involving jackknife derived pseudovalues and the Obuchowski and Rockette method that does not have to use the jackknife, since it could use the bootstrap to get the covariance matrix, or some other methods such as the DeLong method, if one restricts to the Wilcoxon statistic for the figure of merit (empirical ROC-AUC). Since one is dealing with a single reader in multiple treatments, for DBMH one needs the fixed-reader random-case analysis described in §9.8 of the previous chapter (with one reader the conclusions obviousl apply to the specific reader, so reader must be regarded as a fixed factor). Shown below are results obtained using RJafroc function StSignificanceTesting with analysisOption = \"FRRC\" for DBMH (which uses the jackknife), and for ORH using 3 different ways of estimating the covarince matrix (i.e., \\(Cov_1\\) and \\(Var\\)). ret1 &lt;- StSignificanceTesting(rocData1R,FOM = &quot;Wilcoxon&quot;, method = &quot;DBMH&quot;, analysisOption = &quot;FRRC&quot;) data.frame(&quot;DBMH:F&quot; = ret1$FTestStatsFRRC$fFRRC, &quot;DBMH:ddf&quot; = ret1$FTestStatsFRRC$ddfFRRC, &quot;DBMH:P-val&quot; = ret1$FTestStatsFRRC$pFRRC) #&gt; data frame with 0 columns and 0 rows ret2 &lt;- StSignificanceTesting(rocData1R,FOM = &quot;Wilcoxon&quot;, method = &quot;ORH&quot;, analysisOption = &quot;FRRC&quot;) data.frame(&quot;ORHJack:F&quot; = ret2$FTestStatsFRRC$fFRRC, &quot;ORHJack:ddf&quot; = ret2$FTestStatsFRRC$ddfFRRC, &quot;ORHJack:P-val&quot; = ret2$FTestStatsFRRC$pFRRC) #&gt; data frame with 0 columns and 0 rows ret3 &lt;- StSignificanceTesting(rocData1R,FOM = &quot;Wilcoxon&quot;, method = &quot;ORH&quot;, analysisOption = &quot;FRRC&quot;, covEstMethod = &quot;DeLong&quot;) data.frame(&quot;ORHDeLong:F&quot; = ret3$FTestStatsFRRC$fFRRC, &quot;ORHDeLong:ddf&quot; = ret3$FTestStatsFRRC$ddfFRRC, &quot;ORHDeLong:P-val&quot; = ret3$FTestStatsFRRC$pFRRC) #&gt; data frame with 0 columns and 0 rows ret4 &lt;- StSignificanceTesting(rocData1R,FOM = &quot;Wilcoxon&quot;, method = &quot;ORH&quot;, analysisOption = &quot;FRRC&quot;, covEstMethod = &quot;bootstrap&quot;) data.frame(&quot;ORHBoot:F&quot; = ret4$FTestStatsFRRC$fFRRC, &quot;ORHBoot:ddf&quot; = ret4$FTestStatsFRRC$ddfFRRC, &quot;ORHBoot:P-val&quot; = ret4$FTestStatsFRRC$pFRRC) #&gt; data frame with 0 columns and 0 rows The DBMH and ORH-jackknife methods yield identical F-statistics, but the denominator degrees of freedom are different, \\((I-1)(K-1)\\) = 113 for DBMH and \\(\\infty\\) for ORH. The F-statistics for ORH-bootstrap and ORH-DeLong are different. Shown below is a first-principles implementation of signficance testing for the one-reader case. alpha &lt;- 0.05 theta_i &lt;- c(0,0);for (i in 1:I) theta_i[i] &lt;- Wilcoxon(zik1[i,], zik2[i,]) MS_T &lt;- 0 for (i in 1:I) { MS_T &lt;- MS_T + (theta_i[i]-mean(theta_i))^2 } MS_T &lt;- MS_T/(I-1) F_1R &lt;- MS_T/(Var - Cov1) pValue &lt;- 1 - pf(F_1R, I-1, Inf) trtDiff &lt;- array(dim = c(I,I)) for (i1 in 1:(I-1)) { for (i2 in (i1+1):I) { trtDiff[i1,i2] &lt;- theta_i[i1]- theta_i[i2] } } trtDiff &lt;- trtDiff[!is.na(trtDiff)] nDiffs &lt;- I*(I-1)/2 CI_DIFF_FOM_1RMT &lt;- array(dim = c(nDiffs, 3)) for (i in 1 : nDiffs) { CI_DIFF_FOM_1RMT[i,1] &lt;- trtDiff[i] + qt(alpha/2, df = Inf)*sqrt(2*(Var - Cov1)) CI_DIFF_FOM_1RMT[i,2] &lt;- trtDiff[i] CI_DIFF_FOM_1RMT[i,3] &lt;- trtDiff[i] + qt(1-alpha/2,df = Inf)*sqrt(2*(Var - Cov1)) print(data.frame(&quot;theta_1&quot; = theta_i[1], &quot;theta_2&quot; = theta_i[2], &quot;Var&quot; = Var, &quot;Cov1&quot; = Cov1, &quot;MS_T&quot; = MS_T, &quot;F_1R&quot; = F_1R, &quot;pValue&quot; = pValue, &quot;Lower&quot; = CI_DIFF_FOM_1RMT[i,1], &quot;Mid&quot; = CI_DIFF_FOM_1RMT[i,2], &quot;Upper&quot; = CI_DIFF_FOM_1RMT[i,3])) } #&gt; theta_1 theta_2 Var Cov1 MS_T F_1R #&gt; 1 0.91964573 0.94782609 0.00069890056 0.0003734661 0.00039706618 1.2201111 #&gt; pValue Lower Mid Upper #&gt; 1 0.26933885 -0.078183215 -0.028180354 0.021822507 How does this compare to RJafroc FRRC analysis using the StSignificanceTesting function? TBA: the code has chisq not F; need to reconcile ret_rj &lt;- StSignificanceTesting(rocData1R, FOM = &quot;Wilcoxon&quot;, method = &quot;ORH&quot;, analysisOption = &quot;FRRC&quot;) print(data.frame(&quot;theta_1&quot; = ret_rj$FOMs$foms[1,1], &quot;theta_2&quot; = ret_rj$FOMs$foms[2,1], &quot;Var&quot; = ret_rj$ANOVA$VarCom[&quot;Var&quot;, &quot;Estimates&quot;], &quot;Cov1&quot; = ret_rj$ANOVA$VarCom[&quot;Cov1&quot;, &quot;Estimates&quot;], &quot;MS_T&quot; = ret_rj$ANOVA$TRanova[1,3], &quot;Chisq_1R&quot; = ret_rj$FRRC$FTests[&quot;Treatment&quot;,&quot;Chisq&quot;], &quot;pValue&quot; = ret_rj$FRRC$FTests[&quot;Treatment&quot;,&quot;p&quot;], &quot;Lower&quot; = ret_rj$FRRC$ciDiffTrt[1,&quot;CILower&quot;], &quot;Mid&quot; = ret_rj$FRRC$ciDiffTrt[1,&quot;Estimate&quot;], &quot;Upper&quot; = ret_rj$FRRC$ciDiffTrt[1,&quot;CIUpper&quot;])) #&gt; theta_1 theta_2 Var Cov1 MS_T Chisq_1R #&gt; 1 0.91964573 0.94782609 0.00069890056 0.0003734661 0.00039706618 1.2201111 #&gt; pValue Lower Mid Upper #&gt; 1 0.26933885 -0.078183215 -0.028180354 0.021822507 The first-principles and the RJafroc values agree exactly with each other. This above code also shows how to extract the different estimates (\\(Var\\), \\(Cov_1\\), etc.) from the object ret_rj returned by RJafroc. Var: ret_rj\\(ANOVA\\)VarCom[“Var”, “Estimates”] Cov1: ret_rj\\(ANOVA\\)VarCom[“Cov1”, “Estimates”] Chisquare-statistic: ret_rj\\(FRRC\\)FTests[“Treatment”,“Chisq”] df: ret_rj\\(FRRC\\)FTests[1,“DF”] p-value: ret_rj\\(FRRC\\)FTests[“Treatment”,“p”] CI Lower: ret_rj\\(FRRC\\)ciDiffTrt[1,“CILower”] Mid Value: ret_rj\\(FRRC\\)ciDiffTrt[1,“Estimate”] CI Upper: ret_rj\\(FRRC\\)ciDiffTrt[1,“CIUpper”] 4.2.8.1 Jumping ahead If RRRC analysis were conducted, the values would be: msR: ret_rj$meanSquares$msR msT: ret_rj$meanSquares$msT msTR: ret_rj$meanSquares$msTR Var: ret_rj$varComp$var Cov1: ret_rj$varComp$cov Cov2: ret_rj$varComp$cov2 Cov3: ret_rj$varComp$cov3 varR: ret_rj$varComp$varR varTR: ret_rj$varComp$varTR F-statistic: ret_rj$FTestStatsRRRC$fRRRC ddf: ret_rj$FTestStatsRRRC$ddfRRRC p-value: ret_rj$FTestStatsRRRC$pRRRC CI Lower: ret_rj$ciDiffTrtRRRC$CILower Mid Value: ret_rj$ciDiffTrtRRRC$Estimate CI Upper: ret_rj$ciDiffTrtRRRC$CIUpper And similarly, for RRFC analysis, one replaces RRRC with RRFC.] 4.3 Multiple-reader multiple-treatment ORH model The previous sections served as a gentle introduction to the single-reader multiple-treatment Obuchowski and Rockette method. This section extends it to multiple-readers interpreting a common case-set in multiple-treatments (MRMC). The extension is, in principle, fairly straightforward. Compared to (4.1), one needs an additional \\(j\\) index to index readers, and additional random terms to model reader and treatment-reader variability, and the error term needs to be modified to account for the additional random reader factor. The general Obuchowski and Rockette model for fully paired multiple-reader multiple-treatment interpretations is: \\[\\begin{equation} \\theta_{ij\\{c\\}}=\\mu+\\tau_i+R_j+(\\tau R)_{ij}+\\epsilon_{ij\\{c\\}} \\tag{4.22} \\end{equation}\\] The fixed treatment effect \\(\\tau_i\\) is subject to the usual constraint, (4.2). The first two terms on the right hand side of (4.22) have their usual meanings: a constant term \\(\\mu\\) representing performance averaged over treatments and readers, and a treatment effect \\(\\tau_i\\) (\\(i\\) = 1,2, …, \\(I\\)). The following two terms are, by assumption, mutually independent random samples specified as follows: \\(R_j\\) denotes the random treatment-independent contribution to the figure-of-merit of reader \\(j\\) (\\(j\\) = 1,2, …, \\(J\\)), modeled as a sample from a zero-mean normal distribution with variance \\(\\sigma_R^2\\); \\((\\tau R)_{ij}\\) denotes the treatment-dependent random contribution of reader \\(j\\) in treatment \\(i\\), modeled as a sample from a zero-mean normal distribution with variance \\(\\sigma_{\\tau R}^2\\). There could be a perceived notational clash with similar variance component terms defined for the DBMH model – except in that case they applied to pseudovalues. The meaning should be clear from the context. Summarizing: \\[\\begin{equation} \\left\\{\\begin{matrix} R_j \\sim N(0,\\sigma_R^2)\\\\ {\\tau R} \\sim N(0,\\sigma_{\\tau R}^2) \\end{matrix}\\right. \\tag{4.23} \\end{equation}\\] For a single dataset \\(c\\) = 1. An estimate of \\(\\mu\\) follows from averaging over the \\(i\\) and \\(j\\) indices (the averages over the random terms are zeroes): \\[\\begin{equation} \\mu = \\theta_{\\bullet \\bullet \\{1\\}} \\tag{4.24} \\end{equation}\\] As before the dot subscript denotes an average over the replaced index. Averaging over the j index and performing a subtraction yields an estimate of : \\[\\begin{equation} \\tau_i = \\theta_{i \\bullet \\{1\\}} - \\theta_{\\bullet \\bullet \\{1\\}} \\tag{4.25} \\end{equation}\\] The \\(\\tau_i\\) estimates obey the constraint (4.2). For example, with two treatments, the values of \\(\\tau_i\\) must be the negatives of each other: \\(\\tau_1=-\\tau_2\\). The error term on the right hand side of (4.22) is more complex than the corresponding DBM model error term. Obuchowski and Rockette model this term with a multivariate normal distribution with a length \\((IJ)\\) zero-mean vector and a \\((IJ \\times IJ)\\) dimensional covariance matrix \\(\\Sigma\\). In other words, \\[\\begin{equation} \\epsilon_{ij\\{c\\}} \\sim N_{IJ}(\\vec{0},\\Sigma) \\tag{4.26} \\end{equation}\\] Here \\(N_{IJ}\\) is the \\(N_{IJ}\\) variate normal distribution. The covariance matrix \\(\\Sigma\\) is defined by 4 parameters, \\(Var, Cov_1, Cov_2, Cov_3\\), defined as follows: \\[\\begin{equation} Cov(\\epsilon_{ij\\{c\\}},\\epsilon_{i&#39;j&#39;\\{c\\}}) = \\left\\{\\begin{matrix} Var \\; (i=i&#39;,j=j&#39;) \\\\ Cov1 \\; (i\\ne i&#39;,j=j&#39;)\\\\ Cov2 \\; (i = i&#39;,j \\ne j&#39;)\\\\ Cov3 \\; (i\\ne i&#39;,j \\ne j&#39;) \\end{matrix}\\right\\} \\tag{4.27} \\end{equation}\\] Apart from fixed effects, the model implied by (4.22) and (4.27) contains 6 parameters: \\[\\sigma_R^2,\\sigma_{\\tau R}^2,Var,Cov_1,Cov_2,Cov_3\\] This is the same number of variance component parameters as in the DBMH model, which should not be a surprise since one is modeling the data with equivalent models. The Obuchowski and Rockette model (4.22) “looks” simpler because four covariance terms are encapsulated in the \\(\\epsilon\\) term. As with the singe-reader multiple-treatment model, the covariance matrix is assumed to be independent of treatment or reader, as allowing treatment and reader dependencies would greatly increase the number of parameters that would need to be estimated. It is implicit in the Obuchowski-Rockette model that the \\(Var\\), \\(Cov_1\\), Cov_2$, and \\(Cov_3\\), estimates need to be averaged over all applicable treatment-reader combinations. 4.3.1 Structure of the covariance matrix To understand the structure of this matrix, recall that the diagonal elements of a (square) covariance matrix are variances and the off-diagonal elements are covariances. With two indices \\(ij\\) one can still imagine a square matrix where each dimension is labeled by a pair of indices \\(ij\\). One \\(ij\\) pair corresponds to the horizontal direction, and the other \\(ij\\) pair corresponds to the vertical direction. To visualize this let consider the simpler situation of two treatments (\\(I = 2\\)) and three readers (\\(J = 3\\)). The resulting 6x6 covariance matrix would look like this: \\[ \\Sigma= \\begin{bmatrix} (11,11) &amp; (12,11) &amp; (13,11) &amp; (21,11) &amp; (22,11) &amp; (23,11) \\\\ &amp; (12,12) &amp; (13,12) &amp; (21,12) &amp; (22,12) &amp; (23,12) \\\\ &amp; &amp; (13,13) &amp; (21,13) &amp; (22,13) &amp; (23,13) \\\\ &amp; &amp; &amp; (21,21) &amp; (22,21) &amp; (23,21) \\\\ &amp; &amp; &amp; &amp; (22,22) &amp; (23,22) \\\\ &amp; &amp; &amp; &amp; &amp; (23,23) \\end{bmatrix} \\] Shown in each cell of the matrix is a pair of ij-values, serving as column indices, followed by a pair of ij-values serving as row indices, and a comma separates the pairs. For example, the first column is labeled by (11,xx), where xx depends on the row. The second column is labeled (12,xx), the third column is labeled (13,xx), and the remaining columns are successively labeled (21,xx), (22,xx) and (23,xx). Likewise, the first row is labeled by (yy,11), where yy depends on the column. The following rows are labeled (yy,12), (yy,13), (yy,21), (yy,22)and (yy,23). Note that the reader index increments faster than the treatment index. The diagonal elements are evidently those cells where the row and column index-pairs are equal. These are (11,11), (12,12), (13,13), (21,21), (22,22) and (23,23). According to (4.27) the entries in these cells would be \\(Var\\). \\[ \\Sigma= \\begin{bmatrix} Var &amp; (12,11) &amp; (13,11) &amp; (21,11) &amp; (22,11) &amp; (23,11) \\\\ &amp; Var &amp; (13,12) &amp; (21,12) &amp; (22,12) &amp; (23,12) \\\\ &amp; &amp; Var &amp; (21,13) &amp; (22,13) &amp; (23,13) \\\\ &amp; &amp; &amp; Var &amp; (22,21) &amp; (23,21) \\\\ &amp; &amp; &amp; &amp; Var &amp; (23,22) \\\\ &amp; &amp; &amp; &amp; &amp; Var \\end{bmatrix} \\] According to (4.27) the entries in cells with different treatment index pairs but identical reader index pairs would be \\(Cov_1\\) (as an example, the cell (21,11) has the same reader index, namely reader 1, but different treatment indices, namely 2 and 1, so it is replaced by \\(Cov_1\\)): \\[ \\Sigma= \\begin{bmatrix} Var &amp; (12,11) &amp; (13,11) &amp; Cov_1 &amp; (22,11) &amp; (23,11) \\\\ &amp; Var &amp; (13,12) &amp; (21,12) &amp; Cov_1 &amp; (23,12) \\\\ &amp; &amp; Var &amp; (21,13) &amp; (22,13) &amp; Cov_1 \\\\ &amp; &amp; &amp; Var &amp; (22,21) &amp; (23,21) \\\\ &amp; &amp; &amp; &amp; Var &amp; (23,22) \\\\ &amp; &amp; &amp; &amp; &amp; Var \\end{bmatrix} \\] Similarly, the entries in cells with identical treatment index pairs but different reader index pairs would be \\(Cov_2\\): \\[ \\Sigma= \\begin{bmatrix} Var &amp; Cov_2 &amp; Cov_2 &amp; Cov_1 &amp; (22,11) &amp; (23,11) \\\\ &amp; Var &amp; Cov_2 &amp; (21,12) &amp; Cov_1 &amp; (23,12) \\\\ &amp; &amp; Var &amp; (21,13) &amp; (22,13) &amp; Cov_1 \\\\ &amp; &amp; &amp; Var &amp; Cov_2 &amp; Cov_2 \\\\ &amp; &amp; &amp; &amp; Var &amp; Cov_2 \\\\ &amp; &amp; &amp; &amp; &amp; Var \\end{bmatrix} \\] Finally, the entries in cells with different treatment index pairs and different reader index pairs would be \\(Cov_3\\): \\[ \\Sigma= \\begin{bmatrix} Var &amp; Cov_2 &amp; Cov_2 &amp; Cov_1 &amp; Cov_3 &amp; Cov_3 \\\\ &amp; Var &amp; Cov_2 &amp; Cov_3 &amp; Cov_1 &amp; Cov_3 \\\\ &amp; &amp; Var &amp; Cov_3 &amp; Cov_3 &amp; Cov_1 \\\\ &amp; &amp; &amp; Var &amp; Cov_2 &amp; Cov_2 \\\\ &amp; &amp; &amp; &amp; Var &amp; Cov_2 \\\\ &amp; &amp; &amp; &amp; &amp; Var \\end{bmatrix} \\] To understand these terms consider how they might be estimated. Suppose one had the luxury of repeating the study with different case-sets, c = 1, 2, …, C. Then the variance term \\(Var\\) can be estimated as follows: \\[\\begin{equation} Var= \\left \\langle \\frac{1}{C-1}\\sum_{c=1}^{C} (\\theta_{ij\\{c\\}}-\\theta_{ij\\{\\bullet\\}})^2 (\\theta_{ij\\{c\\}}-\\theta_{ij\\{\\bullet\\}})^2 \\right \\rangle_{ij} \\epsilon_{ij\\{c\\}} \\sim N_{IJ}(\\vec{0},\\Sigma) \\tag{4.28} \\end{equation}\\] Of course, in practice one would use the bootstrap or the jackknife as a stand-in for the c-index, but for pedagogic purpose, one maintains the fiction that one has a large number of case-sets at one’s disposal (not to mention the time spent by the readers interpreting them). Notice that the left-hand-side of (4.28) lacks treatment or reader indices. This is because implicit in the notation is averaging the observed variances over all treatments and readers, as implied by \\(\\left \\langle \\right \\rangle _{ij}\\). Likewise, the covariance terms are estimated as follows: \\[\\begin{equation} Cov=\\left\\{\\begin{matrix} Cov_1=\\left \\langle \\frac{1}{C-1}\\sum_{c=1}^{C} (\\theta_{ij\\{c\\}}-\\theta_{ij\\{\\bullet\\}})^2 (\\theta_{i&#39;j\\{c\\}}-\\theta_{i&#39;j\\{\\bullet\\}})^2 \\right \\rangle_{ii&#39;,jj}\\\\ Cov_2=\\left \\langle \\frac{1}{C-1}\\sum_{c=1}^{C} (\\theta_{ij\\{c\\}}-\\theta_{ij\\{\\bullet\\}})^2 (\\theta_{ij&#39;\\{c\\}}-\\theta_{ij&#39;\\{\\bullet\\}})^2 \\right \\rangle_{ii,jj&#39;}\\\\ Cov_3=\\left \\langle \\frac{1}{C-1}\\sum_{c=1}^{C} (\\theta_{ij\\{c\\}}-\\theta_{ij\\{\\bullet\\}})^2 (\\theta_{i&#39;j&#39;\\{c\\}}-\\theta_{i&#39;j&#39;\\{\\bullet\\}})^2 \\right \\rangle_{ii&#39;,jj&#39;} \\end{matrix}\\right. \\tag{4.29} \\end{equation}\\] In (4.29) the convention is that primed and unprimed variables are always different. Since there are no treatment and reader dependencies on the left-hand-sides of the above equations, one averages the estimates as follows: For \\(Cov_1\\) one averages over all combinations of different treatments and same readers, as denoted by \\(\\left \\langle \\right \\rangle_{ii&#39;,jj}\\). For \\(Cov_2\\) one averages over all combinations of same treatment and different readers, as denoted by \\(\\left \\langle \\right \\rangle_{ii,jj&#39;}\\). For \\(Cov_3\\) one averages over all combinations of different treatments and different readers, as denoted by \\(\\left \\langle \\right \\rangle_{ii&#39;,jj&#39;}\\). 4.3.2 Physical meanings of the covariance terms The meanings of the different terms follow a similar description to that given in 4.3.1. The diagonal term \\(Var\\) of the covariance matrix \\(\\Sigma\\) is the variance of the figure-of-merit values obtained when reader \\(j\\) interprets different case-sets in treatment \\(i\\): each case-set yields a number \\(\\theta_{ij\\{c\\}}\\) and the variance of the \\(C\\) numbers, averaged over the \\(I \\times J\\) treatments and readers, is \\(Var\\). It captures the total variability due to varying difficulty levels of the case-sets and within-reader variability. \\(\\rho_{1;ii&#39;jj}\\) is the correlation of the figure-of-merit values obtained when the same reader \\(j\\) interprets a case-set in different treatment \\(i,i&#39;\\). Each case-set, starting with \\(c = 1\\), yields two numbers \\(\\theta_{ij\\{1\\}}\\) and \\(\\theta_{i&#39;j\\{1\\}}\\); the process is repeated for \\(C\\) case-sets. The correlation of the two pairs of C-length arrays, averaged over all pairings of different treatments and same readers, is \\(\\rho_1\\). Because of the common contribution due to the shared reader, \\(\\rho_1\\) will be non-zero. For large common variation, the two arrays become almost perfectly correlated, and \\(\\rho_1\\) approaches unity. For zero common variation, the two arrays become independent, and \\(\\rho_1\\) equals zero. Translating to covariances, one has \\(Cov_1 &lt; Var\\). \\(\\rho_{2;iijj&#39;}\\) is the correlation of the figure-of-merit values obtained when different readers \\(j,j&#39;\\) interpret the same case-set in the same treatment \\(i\\). As before this yields two numbers and upon repeating over \\(C\\) case-sets one has two C-length arrays, whose correlation, upon averaging over all distinct treatment pairings and same readers, yields \\(\\rho_2\\). If one assumes that common variation between different-reader same-treatment FOMs is smaller than the common variation between same-reader different-treatment FOMs, then \\(\\rho_2\\) will be smaller than \\(\\rho_1\\). This is equivalent to stating that readers agree more with themselves on different treatments than they do with other readers on the same treatment. Translating to covariances, one has \\(Cov_2 &lt; Cov_1 &lt; Var\\). \\(\\rho_{3;ii&#39;jj&#39;}\\) is the correlation of the figure-of-merit values obtained when different readers \\(j,j&#39;\\) interpret the same case set in different treatments \\(i,i&#39;\\), etc., yielding \\(\\rho_3\\). This is expected to yield the least correlation. Summarizing, one expects the following ordering for the terms in the covariance matrix: \\[\\begin{equation} Cov_3 &lt; Cov_2 &lt; Cov_1 &lt; Var \\tag{4.30} \\end{equation}\\] 4.3.3 ORH random-reader random-case analysis A model such as (4.22) cannot be analyzed by standard analysis of variance (ANOVA) techniques. Because of the correlated structure of the error term a customized ANOVA is needed (in standard ANOVA models, such as used in DBMH, the covariance matrix of the error term is diagonal with all diagonal elements equal to a common variance, represented by the epsilon term in the DBM model). One starts with the null hypothesis (NH) that the true figures-of-merit of all treatments are identical, i.e., \\[\\begin{equation} NH:\\tau_i=0\\;\\; (i=1,2,...,I) \\tag{4.31} \\end{equation}\\] The analysis described next considers both readers and cases as random effects. Because of the special nature of the covariance matrix, a modified F-statistic is needed 1-4,7, denoted \\(F_{ORH}\\), defined by: \\[\\begin{equation} F_{ORH}=\\frac{MS(T)}{MS(TR)+J\\max(Cov_2-Cov_3,0)} \\tag{4.32} \\end{equation}\\] (4.32) incorporates Hillis’ modification, which ensures that the constraint (4.30) is always obeyed and avoids a possibly negative (hence illegal) F-statistic. The mean square (MS) terms are defined by (these are calculated directly using FOM values, not pseudovalues): \\[\\begin{equation} \\left.\\begin{matrix} MS(T)=\\frac{J}{I-1}\\sum_{i=1}^{I}(\\theta_{i\\bullet}-\\theta_{\\bullet\\bullet})^2\\\\ \\\\ MS(TR)=\\frac{1}{(I-1)(J-1)}\\sum_{i=1}^{I}\\sum_{j=1}^{J}(\\theta_{ij}-\\theta_{i\\bullet}-\\theta_{\\bullet j}+\\theta_{\\bullet\\bullet}) \\end{matrix}\\right\\} \\tag{4.33} \\end{equation}\\] In their original paper (Obuchowski and Rockette 1995) Obuchowski and Rockette state that their proposed test statistic F (basically (4.32) without the constraint implied by the \\(\\max\\) function) is distributed as an F-statistic with numerator degree of freedom \\(ndf=I-1\\) and denominator degree of freedom \\(ddf=(I-1)(J-1)\\). It turns out that then the test is unduly conservative, meaning it is unusually reluctant to reject the null hypothesis. In this connection the author has two historical anecdotes. The late Dr. Robert F. Wagner once stated to the author (ca. 2001) that the sample-size tables published by Obuchowski (Obuchowski 1998, 2000), using the unmodified version of (4.32), predicted such high number of readers and cases that he was doubtful about the chances of anyone conducting a practical ROC study. The second story is that the author once conducted NH simulations using the Roe-Metz simulator described in the preceding chapter and the significance testing as described in the Obuchowski-Rockette paper: the method did not reject the null hypothesis even once in 2000 trials! Recall that with \\(\\alpha = 0.05\\) a valid test should reject the null hypothesis about \\(100\\pm20\\) times in 2000 trials. The author recalls (ca. 2004) telling Dr. Steve Hillis about this issue, and he suggested a different value for the denominator degrees of freedom (ddf), substitution of which magically solved the problem, i.e., the simulations rejected the null hypothesis about 5% of the time; the new \\(ddf\\) is defined below (\\(ndf\\) is unchanged), with the subscript H denoting the Hillis modification: \\[\\begin{equation} ndf=I-1 \\tag{4.34} \\end{equation}\\] \\[\\begin{equation} ddf_H=\\frac{\\left [ MS(TR) + J \\max(Cov_2-Cov_3)\\right ]^2}{\\frac{\\left [ MS(TR) \\right ]^2}{(I-1)(J-1)}} \\tag{3.19} \\end{equation}\\] If \\(Cov_2&lt;Cov_3\\) this reduces to the expression originally suggested by Obuchowski and Rockette. With these changes, under the null hypothesis, the observed statistic \\(F_{ORH}\\), defined in (4.32), is distributed as an F-statistic with \\(I-1\\) and \\(ddf_H\\) degrees of freedom (Hillis et al. 2005; Hillis 2007; Hillis, Berbaum, and Metz 2008): \\[\\begin{equation} F_{ORH}\\sim F_{ndf,ddf_H} \\tag{4.35} \\end{equation}\\] 4.3.3.1 Decision rule, p-value and confidence interval The critical value of the F-statistic for rejection of the null hypothesis is \\(F_{1-\\alpha,ndf,ddf_H}\\), i.e., that value such that fraction \\((1-\\alpha)\\) of the area under the distribution lies to the left of the critical value. From definition (4.32), rejection of the NH is more likely if \\(MS(T)\\) increases, meaning the treatment effect is larger; \\(MS(TR)\\) decreases meaning there is less contamination of the treatment effect by treatment-reader variability; the greater of \\(Cov2\\) or \\(Cov3\\) decreases, meaning there is less contamination of the treatment effect by between-reader and treatment-reader variability, \\(\\alpha\\) increases, meaning one is allowing a greater probability of Type I errors, \\(ndf\\) increases, meaning the more the number of treatment pairings, the greater the chance that at least one pair will reject the NH or \\(ddf_H\\) increases, as this lowers the critical value of the F-statistic. The p-value of the test is the probability, under the NH, that an equal or larger value of the F-statistic than \\(F_{ORH}\\) could be observed by chance. In other words, it is the area under the F-distribution \\(F_{ndf,ddf_H}\\) that lies above the observed value \\(F_{ORH}\\): \\[\\begin{equation} p=\\Pr(F&gt;F_{ORH} \\mid F\\sim F_{ndf,ddf_H}) \\tag{4.36} \\end{equation}\\] The \\((1-\\alpha)\\) confidence interval for \\(\\theta_{i \\bullet} - \\theta_{i&#39; \\bullet}\\) is given by (the average is over the reader index; the case-set index \\(\\{1\\}\\) is suppressed): \\[\\begin{equation} CI_{1-\\alpha,RRRC}=(\\theta_{i \\bullet} - \\theta_{i&#39; \\bullet}) \\pm t_{\\alpha/2, (ddf_H}\\sqrt{\\frac{2}{J}(MS(TR)+J\\max(Cov_2-Cov_3,0))} \\tag{4.37} \\end{equation}\\] 4.3.4 Fixed-reader random-case (FRRC) analysis Using the vertical bar notation \\(\\mid R\\) to denote that reader is regarded as a fixed effect (Roe and Metz 1997), the appropriate F -statistic for testing the null hypothesis \\(NH: \\tau_i = 0 \\; (i=1,1,2,...I)\\) is (Hillis 2007): \\[\\begin{equation} F_{ORH \\mid R}=\\frac{MS(T)}{Var-Cov_1+(J-1)\\max(Cov_2-Cov_3,0)} \\tag{4.38} \\end{equation}\\] \\(F_{ORH \\mid R}\\), a realization (i.e., observation) of a random variable, is distributed as an F-statistic with: \\[\\begin{equation} \\left.\\begin{matrix} ndf=I-1\\\\ ddf=\\infty\\\\ F_{ORH \\mid R} \\sim F_{ndf,ddf} \\end{matrix}\\right\\} \\tag{3.25} \\end{equation}\\] Alternatively, as with (4.16), \\[(I-1)F_{ORH \\mid R} \\sim t_{I-1}\\] For \\(J\\) = 1, (4.38) reduces to (4.17). The critical value of the statistic is \\(F_{1-\\alpha,I-1,\\infty}\\) which is that value such that fraction \\((1-\\alpha)\\) of the area under the distribution lies to the left of the critical value. The null hypothesis is rejected if the observed value of the F- statistic exceeds the critical value, i.e.,: \\[F_{ORH \\mid R}&gt;F_{1-\\alpha,I-1,\\infty}\\] The p-value of the test is the probability that a random sample from the distribution \\(F_{I-1,\\infty}\\) exceeds the observed value of the F statistic defined in (4.38): \\[\\begin{equation} p=\\Pr(F&gt;F_{ORH \\mid R} \\mid F \\sim F_{I-1,\\infty}) \\tag{4.39} \\end{equation}\\] The \\((1-\\alpha)\\) (symmetric) confidence interval for the difference figure of merit is given by: \\[\\begin{equation} CI_{1-\\alpha,FRRC}=(\\theta_{i \\bullet} - \\theta_{i&#39; \\bullet}) \\pm t_{\\alpha/2, \\infty}\\sqrt{\\frac{2}{J}(Var-Cov_1+(J-1)\\max(Cov_2-Cov_3,0))} \\tag{4.40} \\end{equation}\\] One can think of the numerator terms on the right hand side of (4.40) as the variance of the inter-treatment FOM difference per reader, and the division by \\(J\\) is needed as the readers, as a group, have smaller variance in inverse proportion to their numbers. The NH is rejected if any of the following equivalent conditions is met: The observed value of the F-statistic exceeds the critical value \\(F_{1-\\alpha,I-1,\\infty}\\). The p-value defined by (4.39) is less than \\(\\alpha\\). The \\((1-\\alpha)\\) confidence interval does not include zero. Notice that for J = 1, (4.40) reduces to (4.21). 4.3.5 Random-reader fixed-case (RRFC) analysis When case is treated as a fixed factor, the appropriate F-statistic for testing the null hypothesis \\(NH: \\tau_i = 0 \\; (i=1,1,2,...I)\\) is: \\[\\begin{equation} F_{ORH \\mid C}=\\frac{MS(T)}{MS(TR)} \\tag{4.41} \\end{equation}\\] \\(F_{ORH \\mid C}\\) is distributed as an F-statistic with: \\[\\begin{equation} \\left.\\begin{matrix} ndf=I-1\\\\ ddf=(I-1)(J-1)\\\\ F_{ORH \\mid C} \\sim F_{ndf,ddf} \\end{matrix}\\right\\} \\tag{3.30} \\end{equation}\\] The critical value of the statistic is \\(F_{1-\\alpha,I-1,(I-1)(J-1)}\\), which is that value such that fraction \\((1-\\alpha)\\) of the distribution lies to the left of the critical value. The null hypothesis is rejected if the observed value of the F statistic exceeds the critical value: \\[F_{ORH \\mid C}&gt;F_{1-\\alpha,I-1,(I-1)(J-1)}\\] The p-value of the test is the probability that a random sample from the distribution exceeds the observed value: \\[p=\\Pr(F&gt;F_{ORH \\mid C} \\mid F \\sim F_{1-\\alpha,I-1,(I-1)(J-1)})\\] The \\((1-\\alpha)\\) confidence interval is given by: \\[\\begin{equation} CI_{1-\\alpha,RRFC}=(\\theta_{i \\bullet} - \\theta_{i&#39; \\bullet}) \\pm t_{\\alpha/2, (I-1)(J-1)}\\sqrt{\\frac{2}{J}MS(TR)} \\tag{4.42} \\end{equation}\\] It is time to reinforce the formulae with examples. 4.3.6 Single-treatment multiple-reader analysis Suppose one has data in a single treatment \\(i\\) and multiple readers are involved. One wishes to determine if the performance of the readers as a group equals some specified value. Since only a single treatment is involved, an implicit \\(i\\) dependence in subsequent formulae is ignored. In 4.2 single-reader multiple-treatment analysis was described. It is not identical to single-treatment multiple-reader analysis. Treatment is a fixed factor while reader is a random factor. Therefore, one cannot simply use the previous analysis with reader and treatment interchanged (a graduate student tried to do just that, and he is quite smart, hence the reason for this warning; one can use the previous analysis if reader is regarded as a fixed factor, and a function in RJafroc called TBA StSignificanceTestingSingleFixedFactor() does just that). In the analysis described in this section reader is regarded as a random effect. The average performance of the readers is estimated and compared to a specified value. Hillis has described the appropriate modifications. [TBA Two approaches are described, one using the DBM pseudovalue based model and the other based on the OR model with appropriate modification. The second approach is summarized below. TBA] For single-treatment multiple-reader ORH analysis, the figure of merit model is (contrast the following equation to (4.1) noting the absence of an \\(i\\) index. If multiple modalities are present the current analysis is applicable to data in each treatment analyzed one at a time): \\[\\begin{equation} \\theta_{j\\{c\\}}=\\mu+R_j+\\epsilon_{j\\{c\\}} \\tag{4.43} \\end{equation}\\] One wishes to test the NH: \\(\\mu=\\mu_0\\) where \\(\\mu_0\\) is some pre-specified value. (since \\(C\\) = 1, in the interest of brevity one can suppress the \\(c\\) index): \\[\\begin{equation} \\mu=\\theta_{\\bullet} \\tag{4.44} \\end{equation}\\] The variance of the reader-averaged FOM can be shown (Obuchowski and Rockette 1995) to be given by (the reference is to the original OR publication, specifically Eqn. 2.3): \\[\\begin{equation} \\sigma_{\\theta_{\\bullet}}^{2}=\\frac{1}{J}(\\sigma_{R}^{2}+Var+(J-1)Cov_2) \\tag{4.45} \\end{equation}\\] 4.3.7 Connection to existing literature Rather than attempt to derive the preceding equation, it is shown how it follows from the existing literature (Obuchowski and Rockette 1995). For convenience Eqn. 2.3 in cited reference is reproduced below. \\[\\begin{equation} Var(\\theta_{i \\bullet \\bullet}) =\\frac{1}{J}(\\sigma_{b}^{2}+\\sigma_{ab}^{2}+(\\sigma_{w}^{2}/K) + \\sigma_{c}^{2}(1+J(J-1)r_2)) \\tag{4.46} \\end{equation}\\] In the OR notation, the FOM has three indices, \\(\\theta_{ijk}\\). One deletes the \\(i\\) index as one is dealing with a single treatment and one can drop the average over the \\(k\\) index, as one is dealing with a single dataset; \\(\\sigma_{b}^{2}\\) in the OR notation is what we are calling \\(\\sigma_{R}^{2}\\); for single treatment the treatment-reader interaction term \\(\\sigma_{ab}^{2}\\) is absent; and for single “replication” the term \\(\\sigma_{w}^{2}/K\\) (in OR notation \\(K\\) is the number of replications) is absent, or, more accurately, the within-reader variance \\(\\sigma_{w}^{2}\\) is absorbed into the case sampling variance \\(\\sigma_{c}^{2}\\) as the two are inseparable); the term \\(\\sigma_{\\epsilon}^{2}\\) is what we are calling \\(Var\\); and \\(\\sigma_{c}^{2}r_2\\) in OR paper is what we are calling \\(Cov_2\\). An alternative first principles derivation, due to Mr. Xuetong Zhai, is given in TBA Online Appendix 10.E. One needs to replace \\(\\sigma_{R}^{2}\\) in (4.47) with an expected value. Again, rather than attempt to derive the following equation, it is shown how it follows from the existing literature (Hillis 2014). We start with Table I ibid: this is a table of expected means squares for the OR model, analogous to TBA Table 9.1 in Chapter 09, for the DBM model. For a single treatment (in the notation of the cited reference, \\(t\\) = 1 and the treatment-reader variance component goes away and the term \\(\\sigma_{\\epsilon}^{2}\\) is what we are calling \\(Var\\)), it follows that: \\[E(MS(R))=\\sigma_{R}^{2}+Var=Cov_2\\] Substituting this equation in (4.47) yields, \\[\\begin{equation} \\sigma_{\\theta_{\\bullet}}^{2}=\\frac{1}{J}(E(MS(R))+JCov_2) \\tag{4.47} \\end{equation}\\] An estimate of \\(MS(R)\\) is given by (from here on it is understood that \\(MSR\\) is an estimate defined by: \\[\\begin{equation} MS(R)=\\frac{1}{J-1}\\sum_{j=1}^{J}(\\theta_j - \\theta_{\\bullet})^2 \\tag{4.48} \\end{equation}\\] Replacing the expected mean-square value with the estimate and avoiding negative covariance, which could lead to a negative variance estimate, one has: \\[\\begin{equation} \\sigma_{\\theta_{\\bullet}}^{2}=\\frac{1}{J}(MS(R)+J\\max(Cov_2,0)) \\tag{4.49} \\end{equation}\\] The observed value of the t-statistic for testing the NH is \\(t_{1T}\\) (the supbscript means that this statistic applies to single treatment analysis): \\[\\begin{equation} t_{1T}=\\frac{\\mu-\\mu_0}{\\sigma_{\\theta_{\\bullet}}}=(\\theta_{\\bullet}-\\mu_0)\\sqrt{\\frac{J}{(MS(R)+J\\max(Cov_2,0)}} \\tag{4.50} \\end{equation}\\] This is distributed as a t-statistic with \\(df_{H}^{I=1}\\) degrees of freedom: \\[\\begin{equation} t_{1T} \\sim t_{df_{H}^{1T}} \\tag{4.51} \\end{equation}\\] In the above equation, Hillis single-treatment degree of freedom \\(t_{df_{H}^{1T}}\\) is defined by (Hillis 2014): \\[\\begin{equation} df_{H}^{1T}=(J-1)\\left [\\frac{MS(R)+J \\max(Cov_2,0)}{MS(R)} \\right ]^2 \\tag{4.52} \\end{equation}\\] The p-value of the test is the probability that the a random sample from the specified t-distribution exceeds the magnitude of the observed value: \\[\\begin{equation} p=\\Pr(t&gt;\\left | t \\right |\\mid t \\sim t_{df_{H}^{1T}}) \\tag{4.53} \\end{equation}\\] Therefore, a \\(100 \\times (1-\\alpha)\\) percent confidence interval for \\(\\theta_{\\bullet}-\\mu_0\\) is: \\[\\begin{equation} \\theta_{\\bullet}-\\mu_0 \\pm t_{\\alpha/2,df_{H}^{1T}} \\sqrt{ \\frac{MS(R)+\\max(Cov_2,0)}{J}} \\tag{4.54} \\end{equation}\\] The single treatment method is implemented in mainSingleTreatment.R. The relevant code is listed in Online Appendix 10.F. Source the code to get the following output. 4.4 Discussion/Summary This chapter described the Obuchowski-Rockette method as modified by Hillis. As noted earlier, it has the same number of parameters as the DBMH method described in the preceding chapter, but the model (4.22) appears simpler as some terms are “hidden” in the structure of the error term. In this chapter the NH condition was considered. Extension to the alternative hypothesis, i.e., estimating statistical power, is deferred to online appendices to Chapter 11. The extension is a little simpler with the DBMH model, as it is a standard ANOVA model. For example the expressions for the DBMH non-centrality parameter was readily defined in Chapter 09, e.g., §9.7.4. Hillis has derived expressions allowing transformation between quantities in the two methods, and this is the approach adopted in this book and implemented in the cited online appendix. Online Appendix 10.A describes R implementation of the DeLong method for estimating the covariance matrix for empirical AUC. Since the main difficulty understanding the original OR method is conceptualizing the covariance matrix, the author has explained this at an elementary level, using a case-set index which is implicit in the original OR paper4. This was the reason for the gentle introduction analyzing performance of a single reader in multiple treatments. The jackknife, bootstrap and the DeLong methods, all implemented in Online Appendix 10.B, should reinforce understanding of the covariance matrix. The DBM and ORH methods are compared for this special case in Online Appendix 10.C. A minimal implementation of the ORH method for MRMC data is given in Online Appendix 10.D, which is a literal implementation of the relevant formulae. The special case of multiple readers in a single treatment is coded in Online Appendix 10.F. This will be used in Chapter 22 where standalone CAD performance is compared to a group of radiologists interpreting the same cases. The original publication (Dorfman, Berbaum, and Metz 1992) and a subsequent one (Obuchowski and Rockette 1995) were major advances. Hillis’ work showing their equivalence unified the two apparently disparate analyses, and this was a major advance. The Hillis papers, while difficult reads, are ones the author goes to repeatedly. This concludes two methods used to analyze ROC MRMC datasets. A third method, restricted to the empirical AUC, is also available (Clarkson, Kupinski, and Barrett 2006; Kupinski, Clarkson, and Barrett 2006; Gallas 2006; Gallas et al. 2009). As noted earlier, the author prefers methods that are applicable to other estimates of AUC, not just the empirical area, and to other data collection paradigms. The next chapter takes on the subject of sample size estimation using either DBMH or the ORH method. 4.5 References REFERENCES "],
["references-3.html", "REFERENCES", " REFERENCES Bunch, PC, JF Hamilton, GK Sanderson, and AH Simmons. 1978. “Free Response Approach to Measurement and Characterization of Radiographic Observer Performance.” Journal Article. In American Journal of Roentgenology, 130:382–82. AMER ROENTGEN RAY SOC 1891 PRESTON WHITE DR, RESTON, VA 22091. Chakraborty, Dev P. 2017. Observer Performance Methods for Diagnostic Imaging - Foundations, Modeling, and Applications with R-Based Examples. Book. Boca Raton, FL: CRC Press. Chakraborty, D. P., E. S. Breatnach, M. V. Yester, B. Soto, G. T. Barnes, and R. G. Fraser. 1986. “Digital and Conventional Chest Imaging: A Modified Roc Study of Observer Performance Using Simulated Nodules.” Journal Article. Radiology 158: 35–39. Clarkson, Eric, Matthew A. Kupinski, and Harrison H. Barrett. 2006. “A Probabilistic Model for the Mrmc Method, Part 1: Theoretical Development.” Journal Article. Academic Radiology 13 (11): 1410–21. https://doi.org/10.1016/j.acra.2006.07.016. DeLong, E. R., D. M. DeLong, and D. L. Clarke-Pearson. 1988. “Comparing the Areas Under Two or More Correlated Receiver Operating Characteristic Curves: A Nonparametric Approach.” Journal Article. Biometrics 44: 837–45. Dorfman, D. D., K. S. Berbaum, and C. E. Metz. 1992. “ROC Characteristic Rating Analysis: Generalization to the Population of Readers and Patients with the Jackknife Method.” Journal Article. Invest. Radiol. 27 (9): 723–31. Dorfman, Donald D., Kevin S. Berbaum, and Russell V. Lenth. 1995. “Multireader, Multicase Receiver Operating Characteristic Methodology: A Bootstrap Analysis.” Journal Article. Academic Radiology 2 (7): 626–33. http://www.sciencedirect.com/science/article/B75BK-4HNF68N-H/2/4b9d3ad6f0e11d2622af39e99b73eadf. Gallas, Brandon D. 2006. “One-Shot Estimate of Mrmc Variance: AUC.” Journal Article. Academic Radiology 13 (3): 353–62. http://www.sciencedirect.com/science/article/B75BK-4J915RW-C/2/11387ee9975e90e6ae25bef247c817de. Gallas, Brandon D., Andriy Bandos, Frank W. Samuelson, and Robert F. Wagner. 2009. “A Framework for Random-Effects Roc Analysis: Biases with the Bootstrap and Other Variance Estimators.” Journal Article. Communications in Statistics - Theory and Methods 38 (15): 2586–2603. http://www.informaworld.com/10.1080/03610920802610084. Gallas, Brandon D., Gene a Pennello, and Kyle J. Myers. 2007. “Multireader Multicase Variance Analysis for Binary Data.” Journal Article. Journal of the Optical Society of America. A, Optics, Image Science, and Vision 24 (12): 70–80. http://www.ncbi.nlm.nih.gov/pubmed/18059916. Hajian-Tilaki, K. O., James A. Hanley, L. Joseph, and J. P. Collet. 1997. “Extension of Receiver Operating Characteristic Analysis to Data Concerning Multiple Signal Detection Tasks.” Journal Article. Acad Radiol 4: 222–29. Hillis, S. L., N. A. Obuchowski, K. M. Schartz, and K. S. Berbaum. 2005. “A Comparison of the Dorfman-Berbaum-Metz and Obuchowski-Rockette Methods for Receiver Operating Characteristic (Roc) Data.” Journal Article. Statistics in Medicine 24 (10): 1579–1607. Hillis, Stephen L. 2007. “A Comparison of Denominator Degrees of Freedom Methods for Multiple Observer Roc Studies.” Journal Article. Statistics in Medicine 26: 596–619. Hillis, Stephen L. 2014. “A Marginal‐mean Anova Approach for Analyzing Multireader Multicase Radiological Imaging Data.” Journal Article. Statistics in Medicine 33 (2): 330–60. Hillis, Stephen L., and K. S. Berbaum. 2004. “Power Estimation for the Dorfman-Berbaum-Metz Method.” Journal Article. Acad. Radiol. 11 (11): 1260–73. Hillis, Stephen L., K. S. Berbaum, and C. E. Metz. 2008. “Recent Developments in the Dorfman-Berbaum-Metz Procedure for Multireader Roc Study Analysis.” Journal Article. Acad Radiol 15 (5): 647–61. Ishwaran, Hemant, and Constantine A. Gatsonis. 2000. “A General Class of Hierarchical Ordinal Regression Models with Applications to Correlated Roc Analysis.” Journal Article. The Canadian Journal of Statistics 28 (4): 731–50. Kupinski, Matthew A., Eric Clarkson, and Harrison H. Barrett. 2006. “A Probabilistic Model for the Mrmc Method, Part 2: Validation and Applications.” Journal Article. Academic Radiology 13 (11): 1422–30. https://doi.org/10.1016/j.acra.2006.07.015. Larsen, Richard J., and Morris L. Marx. 2001. An Introduction to Mathematical Statistics and Its Applications. Book. 3rd ed. Upper Saddle River, NJ: Prentice-Hall Inc. Metz, C. E., Benjamin A. Herman, and C. E. Roe. 1998. “Statistical Comparison of Two Roc-Curve Estimates Obtained from Partially-Paired Datasets.” Journal Article. Med Decis Making 18 (1): 110–21. https://doi.org/doi: 10.1177/0272989X9801800118. Niklason, L. T., N. M. Hickey, Dev P. Chakraborty, E. A. Sabbagh, M. V. Yester, R. G. Fraser, and G. T. Barnes. 1986. “Simulated Pulmonary Nodules: Detection with Dual-Energy Digital Versus Conventional Radiography.” Journal Article. Radiology 160: 589–93. Obuchowski, Nancy. 2009. “Reducing the Number of Reader Interpretations in Mrmc Studies.” Journal Article. Acad Radiol 16: 209–17. Obuchowski, Nancy A. 1998. “Sample Size Calculations in Studies of Test Accuracy.” Journal Article. Statistical Methods in Medical Research 7 (4): 371–92. https://doi.org/10.1177/096228029800700405. ———. 2000. “Sample Size Tables for Receiver Operating Characteristic Studies.” Journal Article. Am. J. Roentgenol. 175 (3): 603–8. http://www.ajronline.org/cgi/content/abstract/175/3/603. Obuchowski, N. A., and H. E. Rockette. 1995. “Hypothesis Testing of the Diagnostic Accuracy for Multiple Diagnostic Tests: An Anova Approach with Dependent Observations.” Journal Article. Communications in Statistics: Simulation and Computation 24: 285–308. Roe, C. A., and C. E. Metz. 1997. “Variance-Component Modeling in the Analysis of Receiver Operating Characteristic Index Estimates.” Journal Article. Acad. Radiol. 4 (8): 587–600. Satterthwaite, F. E. 1941. “Synthesis of Variance.” Journal Article. Psychometrika 6 (5): 309–16. ———. 1946. “An Approximate Distribution of Estimates of Variance Components.” Journal Article. Biometrics Bulletin 2 (6): 110–14. Swets, John A., and Ronald M. Pickett. 1982. Evaluation of Diagnostic Systems: Methods from Signal Detection Theory. Book. First. Series in Cognition and Perception. New York: Academic Press. Toledano, A. Y. 2003. “Three Methods for Analyzing Correlated Roc Curves: A Comparison in Real Data Sets.” Journal Article. Statistics in Medicine 22 (18): 2919–33. Toledano, A. Y., and C. Gatsonis. 1996. “Ordinal Regression Methodology for Roc Curves Derived from Correlated Data.” Journal Article. Stat Med 15 (16): 1807–26. Van Dyke, C. W., R. D. White, N. A. Obuchowski, M. A. Geisinger, R. J. Lorig, and M. A. Meziane. 1993. “Cine Mri in the Diagnosis of Thoracic Aortic Dissection.” Journal Article. 79th RSNA Meetings. "]
]
