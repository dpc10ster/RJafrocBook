---
title: "RemovedFromsourcesvariability"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Comparison of methods of estimating variability
This section compares the four methods described in this chapter for estimating standard deviation of AUC: MLE (if applicable), DeLong (if applicable), bootstrap, jackknife and population sampling with a calibrated simulator. The modifier "if applicable" is needed as the choice of FOM determines whether the MLE or the DeLong method are applicable. For example, MLE is only possible with the binormal model fitted AUC as figure of merit, while the DeLong method is only possible with the empirical AUC as figure of merit.

```{r, echo=FALSE}
source(here("R/CH07-Variability/GenerateCaseSamples.R"))
source(here("R/CH07-Variability/VarPopSampling.R"))
source(here("R/CH07-Variability/VarBootstrap.R"))
source(here("R/CH07-Variability/VarJack.R"))
source(here("R/CH07-Variability/Wilcoxon.R"))
source(here("R/CH07-Variability/VarDeLong.R"))
```


```{r, cache=TRUE}
K <- c(60, 50)
seed <- 1
P <- 2000
B <- 2000
P1 <- 20

ds <- Df2RJafrocDataset( # convert to dataset
  RocTable[1,], # NL ratings table 
  RocTable[2,], # LL ratings table
  InputIsCountsTable = TRUE)
fit <- FitBinormalRoc(ds)
VBM <- (fit$StdAUC)^2 # binormal model AUC variance
  
set.seed( seed )
VDL <- array(dim = P1)
for (p in 1 : P1) VDL[p] <- VarDeLong(K, mu, sigma, zetas)

set.seed( seed )
VBS<- array(dim = P1)
for (p in 1 : P1) VBS[p] <- VarBootstrap(K, mu, sigma, zetas, B)

set.seed( seed )
VJK<- array(dim = P1)
for (p in 1 : P1) VJK[p] <- VarJack(K, mu, sigma, zetas)

set.seed( seed )
VPS<- array(dim = P1)
for (p in 1 : P1) VPS[p] <- VarPopSampling(K, mu, sigma, zetas, P)

cat("Std binormal model = ", mean(sqrt(VBM)),"\n")
cat("Mean Std DeLong  = ", mean(sqrt(VDL)),"\n")
cat("Mean Std Boot Sampling = ", mean(sqrt(VBS)),"\n")
cat("Mean Std Jack Sampling = ", mean(sqrt(VJK)),"\n")
cat("Mean Std Pop Sampling = ", mean(sqrt(VPS)),"\n")
```


```{r, echo = FALSE, cache=TRUE, eval = FALSE}
K <- c(600, 500)
seed <- 1
cat("seed = ", seed, 
    "\nK1 = ", K[1], 
    "\nK2 = ", K[2], 
    "\nmu = ", mu, 
    "\nsigma = ", sigma, "\n")

P <- 2000
B <- 2000
P1 <- 20

set.seed( seed )
VPS<- array(dim = P1)
for (p in 1 : P1) VPS[p] <- VarPopSampling(K, mu, sigma, zetas, P)

set.seed( seed )
VBS<- array(dim = P1)
for (p in 1 : P1) VBS[p] <- VarBootstrap(K, mu, sigma, zetas, B)

set.seed( seed )
VJK<- array(dim = P1)
for (p in 1 : P1) VJK[p] <- VarJack(K, mu, sigma, zetas)

set.seed( seed )
VDL <- array(dim = P1)
for (p in 1 : P1) VDL[p] <- VarDeLong(K, mu, sigma, zetas)

cat("Mean Std Pop Sampling = ", mean(sqrt(VPS)),"\n")
cat("Mean Std Boot Sampling = ", mean(sqrt(VBS)),"\n")
cat("Mean Std Jack Sampling = ", mean(sqrt(VJK)),"\n")
cat("Mean Std DeLong  = ", mean(sqrt(VDL)),"\n")
```

```{r sourcesVariabilityTableCompareA, echo=FALSE}
df <- array(dim = c(5,4))
df[1,] <- c("ExampleTable", "Rocfit", "MLE", "0.03790")
df[2,] <- c("ExampleTable", "Rocfit", "DeLong", NA)
df[3,] <- c("ExampleTable", "Rocfit", "Bootstrap", "0.0363")
df[4,] <- c("ExampleTable", "Rocfit", "Jackknife", "0.0335")
df[5,] <- c("ExampleTable", "Rocfit", "PopulationSampling", "0.0358")
df <- as.data.frame(df)
colnames(df) <- c("Dataset", "AUC est. method", "Var. est. method", "std. dev. AUC")
knitr::kable(df, caption = "Estimates of standard deviation of parametric AUC for data in example table.", escape = FALSE)
```


```{r sourcesVariabilityTableCompareB, echo=FALSE}
df <- array(dim = c(5,4))
df[1,] <- c("ExampleTable", "Wilcoxon", "MLE", NA)
df[2,] <- c("ExampleTable", "Wilcoxon", "DeLong", "0.03331")
df[3,] <- c("ExampleTable", "Wilcoxon", "Bootstrap", "0.03635")
df[4,] <- c("ExampleTable", "Wilcoxon", "Jackknife", "0.03348")
df[5,] <- c("ExampleTable", "Wilcoxon", "PopulationSampling", "0.03575")
df <- as.data.frame(df)
colnames(df) <- c("Dataset", "AUC est. method", "Var. est. method", "std. dev. AUC")
knitr::kable(df, caption = "Different estimates of standard deviation of AUC.", escape = FALSE)
```


```{r sourcesVariabilityTableCompareC, echo=FALSE}
df <- array(dim = c(5,4))
df[1,] <- c("Cal. Simulator 60 cases", "Wilcoxon", "MLE", "NA")
df[2,] <- c("Cal. Simulator 60 cases", "Wilcoxon", "DeLong", "0.0333")
df[3,] <- c("Cal. Simulator 60 cases", "Wilcoxon", "Bootstrap", "0.0366")
df[4,] <- c("Cal. Simulator 60 cases", "Wilcoxon", "Jackknife", "0.0335")
df[5,] <- c("Cal. Simulator 60 cases", "Wilcoxon", "PopulationSampling", "0.0359")
df <- as.data.frame(df)
colnames(df) <- c("Dataset", "AUC est. method", "Var. est. method", "std. dev. AUC")
knitr::kable(df, caption = "Different estimates of standard deviation of AUC.", escape = FALSE)
```


```{r sourcesVariabilityTableCompareD, echo=FALSE}
df <- array(dim = c(5,4))
df[1,] <- c("Cal. Simulator 600 cases", "Wilcoxon", "MLE", "NA")
df[2,] <- c("Cal. Simulator 600 cases", "Wilcoxon", "DeLong", "0.0113")
df[3,] <- c("Cal. Simulator 600 cases", "Wilcoxon", "Bootstrap", "0.0110")
df[4,] <- c("Cal. Simulator 600 cases", "Wilcoxon", "Jackknife", "0.0113")
df[5,] <- c("Cal. Simulator 600 cases", "Wilcoxon", "PopulationSampling", "0.0113")
df <- as.data.frame(df)
colnames(df) <- c("Dataset", "AUC est. method", "Var. est. method", "std. dev. AUC")
knitr::kable(df, caption = "Different estimates of standard deviation of AUC.", escape = FALSE)
```


## Dependence of AUC on reader expertise {#sourcesVariabilityreaderExpertise}
Suppose one conducts an ROC study with J readers where typically J is about 5 but can be as low as 3 and as high as 20 (the wide variability reflects, in my opinion, lack of understanding of the factors affecting the optimal choice of J and the related issue of statistical power). Each reader interprets the same case sample, i.e., the same set of cases, but because they have different expertise levels and for other reasons (see below), the observed ROC counts tables will not be identical. The variance of the observed values is an empirical estimate of between-reader variance (including the inseparable within-reader component). Here is an example, in file MainBetweenReaderSd.R. This file loads the Van Dyke dataset, consisting of two modalities and five readers described in Online Chapter 24. Source the code file to get:
7.8.1: Code Output 
> source('~/book2/02 A ROC analysis/A7 Sources of variability in AUC/software/mainBetweenReaderSd.R')
between-reader variance in modality 1 = 0.003082629 
between-reader variance in modality 2 = 0.001304602 
avg. between-reader variance in both modalities = 0.002193615

Notice that the between-reader (including, as always, within-reader) variance appears to be modality dependent. Determining if the difference is significant requires more analysis. For now one simply averages the two estimates.

How can one handle between-reader variability in the notation? Each reader’s interpretation can be analyzed by MLE to get the corresponding AUC value. The notation for the observed AUC values is:

 	.	(7.11)

How does one conceptualize reader variability? As stated before, it is due to differences in expertise levels, but there is more to it. Since the single reader is characterized by parameters (R is the number of ratings bins; it is assumed that all readers employ the same number of bins, although they may employ it in different ways, i.e., the values of the thresholds may be different). While the non-diseased distribution for each reader could have mean different from 0 and variance different from unity, one can always translate it to zero and scale it to assure that the non-diseased distribution is the unit normal distribution. However, one cannot be assured that the separation and the width of the diseased distribution, and the thresholds, will not depend on the reader. Therefore, the most general way of thinking of reader variability is to put a j subscript on each of the model parameters, yielding . Now the first two of these define the population ROC curve for reader j, and the corresponding AUC value is (this equation was derived in Chapter 06, Eqn. (6.92.24)):

 	.	(7.12)

All else being equal, readers with larger   will perform better because they are better able to separate the non-diseased and diseased cases in z-space than their fellow readers. It is difficult and possibly misleading to try to estimate the differences directly from the observed ROC counts tables, but in general better readers will yield counts more skewed towards the low end of the rating scale on non-diseased cases and more skewed towards the high end of the rating scale for diseased cases. The ideal reader would rate all diseased cases one value (e.g. 5) and all non-diseased cases a smaller fixed value (e.g., 1, 2, 3, or 4), resulting in unit AUC, i.e., perfect performance. According to Eqn. (7.12), a reader with smaller   will also perform better. As noted before, typically the   parameter is greater than unity. The reasons for this general finding will be discussed later, but accept my word, for now, that the best the reader can is to reduce this parameter to unity. See Summary of Chapter 06 for reasons for the observation that generally the variance of the diseased distribution is larger than one – it has to do with the inhomogeneity of the distribution of diseased cases and the possibility that a mixture distribution is involved. As regards thresholds, while the population based performance for a particular reader does not depend on thresholds, the thresholds determine the ROC counts table, so differences in usage of the thresholds will translate to differences in estimates of , but this is expected to be a smaller effect compared to the dependence on  . To summarize, variability of readers can be attributed to variability in the binormal model parameters and, to a lesser extent, to variability in adopted thresholds.

## Dependence of AUC on modality {#sourcesVariabilityModalityEffect}
Suppose one conducts an ROC study with j (j =1,2,…J) readers but there are I  (i=1,2,…I) modalities. This is frequently referred to as the multiple reader multiple case (MRMC) paradigm. Each reader interprets the same case sample, i.e., the same set of cases, in two or more modalities. Here is an example, in file MainModalityEffect.R. This file loads the Van Dyke dataset, consisting of two modalities and five readers described in Online Chapter 24. Source the code file to get:
7.9.1: Code Output
> source('~/book2/02 A ROC analysis/A7 Sources of variability in AUC/software/mainModalityEffect.R')
reader-average FOM in modality 1 = 0.897037 reader-average FOM in modality 2 = 0.9408374 , effect size, i.e., fom modality 1 minus modality 2 = -0.04380032

Notice that the second modality has a higher FOM. Determining if the difference is significant requires more analysis as described in Chapter 09. The difference between the reader-averaged FOMs is referred to as the observed effect size.

How does on handle modality dependence of the FOM in the notation? If K is the total number of cases, the total number of interpretations involved is IJK, each of which results in a rating. MLE analysis yields IJ values for AUC, one for each modality-reader combination. The appropriate notation is

 	.	(7.13)

The most general way of thinking of reader and modality variability is to put ij subscripts on each of the model parameters, yielding . For a particular combination of modality and reader, the population ROC curve as fitted by the binormal model, yields the area under the ROC curve:

 	.	(7.14)

Given an MRMC dataset, using MLE one can estimate the parameters  for each modality-reader combination, and this could be used to design a simulator that is calibrated to the specific clinical dataset, which in turn can be used to illustrate the ideas and to test any proposed method of analyzing the data. However, the problem is more complex; the procedure needs to also account for the correlations arising from the large number of pairings inherent is such a dataset (e.g., reader 1 in modality 1 vs. reader 2 in modality 2, since both interpret a common dataset). Designing a MRMC calibrated simulator was until recently, an unsolved problem, which necessitated recent work9 by me and Mr. Xuetong Zhai. Chapter 23 describes recent progress towards this end. 

## Effect of binning {#sourcesVariabilityBinning}
There are actually two effects. (1) The empirical AUC will tend to be smaller than the true AUC. If there are few operating points, and they are clustered together, the difference may be large, Fig. 7.1 (A). 

7.10.1: Code listing

```{r}
```

This figure was generated by a binormal model simulator, with thresholds chosen to exaggerate the effect, line 4 - 7. The true or population AUC is 0.8664, while the empirical AUC is 0.8030, line 13. However, since interest is in differences in AUCs, e.g., between two modalities, and the underestimates may tend to cancel, this may not be a serious issue. However, an effect that may be problematical is that the operating points for a given reader may not span the same FPF ranges in the two modalities, in which case the empirical AUCs will be different, as depicted in Fig. 7.1 (A - B). The AUC in modality (B), where the operating points span the entire range, is 0.8580, line 21, which is closer to the population value. Since the usage of the bins is not under the researcher's control, this effect cannot be ruled out. Fitted AUCs are expected to be less sensitive, but not immune, to this effect. Fig. 7.1 (C) is a contaminated binormal model (CBM) fitted curve, line 14, to the same data as in (A), fitted AUC = 0.892, while Fig. 7.1 (D) is a CBM fitted curve, line 22, to the same data as in (B), fitted AUC = 0.867. The difference in AUCs between (A) and (B) is  0.055, while that between (C) and (D) is 0.024. The consequences of these effects on the validity of analyses using the empirical AUC have not been studied. [The parameters of the model were a = 1.33 and b = 0.667, which yields the quoted value of the population AUC. The population value is that predicted by the parameters; it has zero sampling variability. The fitted curves are those predicted by the CBM, discussed in Chapter 20.]

(2) The second effect is varying numbers of thresholds or bins between the readers. One could be a radiologist, capable of maintaining at most about 6 bins, and the other an algorithmic observer, such as CAD, capable of maintaining more bins. Moreover, if the radiologist is an expert, the data points will tend to cluster near the initial near vertical part of the ROC (see Chapter 17 for explanation). This is illustrated using code in file mainBinVariability.R. Sourcing this code yields Fig. 7.1 (E – F) and the AUC values shown in these plots.

7.10.2: Code listing

```{r}
```

In Fig. 7.1(E) and Fig. 7.1(F) the effect is dramatic. The expert radiologist trapezoidal AUC is 0.7418, while that for CAD is 0.8632; the latter is close to the population value. It is left as an exercise for the reader to demonstrate that using CBM one can avoid the severe underestimate of performance that occurs in plot (E).


 
(A) AUC = 0.8030	 
(B) AUC = 0.8580
 
(C) AUC = 0.892	 
(D) AUC = 0.867
 
(E) AUC =  0.7418	 
(F) AUC = 0.8632
Fig. 7.1 (A-D):  Plots (A - B) depict empirical plots for two simulated datasets for the same model, i.e., same continuous ratings, using different thresholds. In (A) the thresholds are clustered at low FPF values, while in (B) they are more evenly spaced. Empirical AUCs for the plots are 0.803 for (A) and 0.858 for (B). The clustering in (A) leads to a low estimate of AUC. Plots (C) and (D) are fitted curves corresponding to the same data as in (A) and (B), respectively. For each plot, the population AUC is 0.866. The fitted curves are less sensitive, but not immune, to the data clustering variations. With a large number of evenly spaced points, the empirical AUC is close to that of the fitted curve. This effect is demonstrated in plots (E) and (F). The plots were generated by mainEmpVsFit.R and  mainBinVariability.R.

## Empirical vs. fitted AUCs {#sourcesVariabilityEmpiricalVsFitted}
There is a preference with some researchers to using the empirical AUC as a figure of merit. Its usage enables analyses10-14 variously referred to as the "probabilistic", mechanistic" or "first-principles" approach and the "one-shot" approach15 to multiple reader multiple case analysis. The author is aware of some statisticians who distrust parametric modeling and the associate normality assumptions (I trust that the demonstrations in §6.2.2 may assuage the concerns). In addition, empirical AUC frees the researcher from problems with binormal model based fitting, e.g., handling degenerate datasets (these problems go away with two of the fitting methods described in later chapters). The fact that the empirical AUC can always be calculated, even, for example, with a single operating point, can make the analyst blissfully unaware of anomalous data structures. In contrast, the binormal curve-fitting method in Chapter 06 will complain when the ratings bins are not well populated, e.g., by failing to converge. This at least alerts the analyst that conditions are not optimal, and prompt data visualization and consideration of alternate fitting methods. 

If empirical AUC is defined by a large number of operating points, such as with continuous ratings obtained with algorithmic observers, then empirical AUC will be nearly equal to the true AUC, to within sampling error. However, with human observers one rarely gets more than about 6 distinct ratings. The researcher has no control over the internal sensory thresholds used by the radiologist to bin the data, and these could depend on the modality. As demonstrated in the previous section, the empirical AUC is sensitive to the choice of thresholds, especially when the number of thresholds is small, as is usually the case with radiologists, and when the operating points are clustered on the initial near vertical section of the plot, as is also the case with experts. 

 






 
 
(A)	 
(B)	 
(C)
 
(D)	 
(E)	
Fig. 7.2 (A-E): This figure shows a small sample of the 236 viewable plots in the cited online document. In this figure, each panel corresponds to a different reader (the j-index in the labels). The modality is the same (the i-index) and the dataset is labeled D1. The three curves correspond to different advanced method of fitting ROC data. The interest in this chapter is on the positions of the operating points. Reader (C) traverses more of the FPF range than does reader (E). Empirical AUC may result in a greater error for reader (E) than for reader (C). An explanation of the three fits is deferred to Chapter 18




