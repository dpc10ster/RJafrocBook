# Three proper ROC fits {#rsm-3-fits}


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(kableExtra)
library(seqinr)
library(RJafroc)
library(ggplot2)
library(gridExtra)
library(binom)
library(here)
library(foreach)
library(doRNG)
library(doParallel)

```


## How much finished {#rsm-3-fits-how-much-finished}
40%


## Introduction {#rsm-3-fits-intro}
There are three methods for fitting proper curves to ROC datasets. These are the RSM described in Chapter \@ref(rsm-fitting), and the PROPROC (proper ROC) and CBM (contaminated binormal model) described in TBA Chapter 20. This chapter compares these methods for a number of datasets. ^[Comparing the RSM to the binormal model would be inappropriate, as the latter does not predict proper ROCs.]


## Datasets {#rsm-3-fits-14-datasets}

```{r}
?`RJafroc-package`
```


The datasets are embedded in ther `RJafroc` package. They can be viewed in the help file of the package, a partial screen-shot of which is shown next.


```{r rsm-3-fits-datasets, echo=FALSE,fig.cap="Partial screen shot of `RJafroc` help file showing the datasets included with the current distribution (v2.0.1).",fig.show='hold',fig.align='center'}
knitr::include_graphics("images/compare-3-fits/datasets.png")
``` 


The datasets are identified in the code by `datasetdd` (where `dd` is an integer in the range `01` to `14`) as follows:

* `dataset01` "TONY" FROC dataset [@RN2125]

```{r, echo=FALSE}
str(dataset01$ratings)
```


* `dataset01` "VAN-DYKE" Van Dyke ROC dataset [@RN1993]

```{r, echo=FALSE}
str(dataset02$ratings)
```


* `dataset03` "FRANKEN" Franken ROC dataset [@RN1995]

```{r, echo=FALSE}
str(dataset03$ratings)
```


* `dataset04` "FEDERICA" Federica Zanca FROC dataset [@RN1882]

```{r, echo=FALSE}
str(dataset04$ratings)
```


* `dataset05` "THOMPSON" John Thompson FROC dataset [@RN2368]

```{r, echo=FALSE}
str(dataset05$ratings)
```



* `dataset06` "MAGNUS" Magnus Bath FROC dataset [@RN1929]

```{r, echo=FALSE}
str(dataset06$ratings)
```


* `dataset07` "LUCY-WARREN" Lucy Warren FROC dataset [@RN2507]

```{r, echo=FALSE}
str(dataset07$ratings)
```


* `dataset08` "PENEDO" Monica Penedo FROC dataset [@RN1520]

```{r, echo=FALSE}
str(dataset08$ratings)
```


* `dataset09` "NICO-CAD-ROC" Nico Karssemeijer ROC dataset [@hupse2013standalone]

```{r, echo=FALSE}
str(dataset09$ratings)
```


* `dataset10` "RUSCHIN" Mark Ruschin ROC dataset [@RN1646]

```{r, echo=FALSE}
str(dataset10$ratings)
```


* `dataset11` "DOBBINS-1" Dobbins I FROC dataset [@Dobbins2016MultiInstitutional]

```{r, echo=FALSE}
str(dataset11$ratings)
```


* `dataset12`  "DOBBINS-2" Dobbins II ROC dataset [@Dobbins2016MultiInstitutional]

```{r, echo=FALSE}
str(dataset12$ratings)
```



* `dataset13` "DOBBINS-3" Dobbins III FROC dataset [@Dobbins2016MultiInstitutional]

```{r, echo=FALSE}
str(dataset13$ratings)
```


* `dataset14` "FEDERICA-REAL-ROC" Federica Zanca *real* ROC dataset [@RN2318]

```{r, echo=FALSE}
str(dataset14$ratings)
```



## Application to one dataset {#rsm-3-fits-one-dataset}

Both RSM and CBM fitting methods are implemented in `RJafroc`. `PROPROC` is implemented in Windows software ^[OR DBM-MRMC 2.5 (Sept. 04, 2014; this version is no longer distributed but is available upon request.] available [here](https://perception.lab.uiowa.edu/OR-DBM-MRMC-252), last accessed 1/4/21.


The RSM has three parameters (excluding thresholds): $\mu$, $\lambda'$, and $\nu'$. CBM has two parameters $\mu_{CBM}$ and $\alpha$. `PROPROC` has two parameters $c$ and $d_a$. CBM and PROPROC are detailed in TBA Chapter 20. 


### Location of PROPROC files {#rsm-3-fits-one-dataset-proproc}

For each dataset PROPROC parameters were obtained by running the software with PROPROC selected as the curve-fitting method. The results are saved to files that end with `proprocnormareapooled.csv` ^[In accordance with R-package policies white-spaces in the original `PROPROC` output file names have been removed.] contained in "R/compare-3-fits/MRMCRuns/CCC/", where `CCC` denotes the name of the dataset (for example, for the Van Dyke dataset, `CCC` = "VD"). Examples are shown in the next two screen-shots.


```{r rsm-3-fits-mrmc-runs, echo=FALSE,fig.cap="Screen shot (1 of 2) of `R/compare-3-fits/MRMCRuns` showing the results of PROPROC analysis on 14 datasets.",fig.show='hold',fig.align='center'}
knitr::include_graphics("images/compare-3-fits/MRMCRuns.png")
``` 


```{r rsm-3-fits-mrmc-runs-vd, echo=FALSE,fig.cap="Screen shot (2 of 2) of `R/compare-3-fits/MRMCRuns/VD` showing the results of PROPROC analysis for the Van Dyke dataset.",fig.show='hold',fig.align='center'}
knitr::include_graphics("images/compare-3-fits/MRMCRuns-VD.png")
``` 

The contents of `R/compare-3-fits/MRMCRuns/VD/VDproprocnormareapooled.csv` are shown next, see Fig. \@ref(fig:rsm-3-fits-proproc-output-van-dyke). ^[The `VD.lrc` file in this directory is the Van Dyke data formatted for input to OR DBM-MRMC 2.5.] The PROPROC parameters $c$ and $d_a$  are in the last two columns. The column names are `T` = treatment; `R` = reader; `return-code` = undocumented value, `area` = PROPROC AUC; `numCAT` = number of ROC bins; `adjPMean` = undocumented value; `c` =  $c$ and `d_a` =  $d_a$, are the PROPROC parameters defined in [@RN1499].


```{r rsm-3-fits-proproc-output-van-dyke, echo=FALSE,out.width="50%",out.height="20%",fig.cap="PROPROC output for the Van Dyke ROC data set.",fig.show='hold',fig.align='center'}
knitr::include_graphics("images/compare-3-fits/vanDyke.png")
``` 



```{r rsm-3-fits-code, echo=FALSE}
source(here("R/compare-3-fits/Compare3ProperRocFits.R"))
```


### Dataset indexing {#rsm-3-fits-one-dataset-indexing}

The datasets are indexed by an integer 1 through 14. The following shows the correspondence of `index` to dataset name.

```{r}
index <- seq(1:14)
fileNames <-  c("TONY", "VD", "FR", 
            "FED", "JT", "MAG", 
            "OPT", "PEN", "NICO",
            "RUS", "DOB1", "DOB2", 
            "DOB3", "FZR")

cat("File name for index = ", index[2], " is ", fileNames[index[2]], "\n")
```

### All results for Van Dyke dataset {#rsm-3-fits-one-dataset-all-results}

The following screen shot shows the pre-analyzed files created by the function `Compare3ProperRocFits()` described below. Each file is named `allResultsCCC`, where `CCC` is the name of the dataset.

```{r rsm-3-fits-all-results-rsm6, echo=FALSE,fig.cap="Screen shot of `R/compare-3-fits/RSM6` showing the results files created by  `Compare3ProperRocFits()` .",fig.show='hold',fig.align='center'}
knitr::include_graphics("images/compare-3-fits/RSM6.png")
``` 


In the following example only the `VD` dataset is analyzed. The flag `reAnalyze` is set to `FALSE` causing pre-analyzed results (to be found in directory `R/compare-3-fits/RSM6`) to be read; if `TRUE` the analysis is redone, leading to possibly slightly different results (the maximum likelihood algorithm has inherent randomness). The `list` variable `ret` contains the results, in `allResults`, and the composite plots, in `allPlots`. These are saved to lists `plotArr` and `resultsArr`.

The following code uses the function `Compare3ProperRocFits()` to compute the 3 fits. In this code `startIndx` is the  first `index` to analyze and `endIndx` is the last. To analyze all datasets one would put `startIndx <-  1` and `endIndx <-  14`. 


```{r rsm-3-fits-code-f2, echo=TRUE, cache=TRUE}
startIndx <-  2
endIndx <- 2
ret <- Compare3ProperRocFits(startIndx = startIndx, 
                             endIndx = endIndx, 
                             reAnalyze = FALSE)

resultsArr <- plotArr <- array(list(), 
                               dim = c(endIndx - startIndx + 1))

for (f in 1:(endIndx-startIndx+1)) {
  plotArr[[f]] <- ret$allPlots[[f]]
  resultsArr[[f]] <- ret$allResults[[f]]
}
```


### Displaying one plot {#rsm-3-fits-one-plot}

The `plotArr` list contains the plots. For the current example, the plots are contained in `plotArr[[1]]`, where the list index `[[1]]` is because there is only one dataset being analyzed (the Van Dyke dataset). ^[With two datasets, the first dataset plots would be in `plotArr[[1]]` and the second in `plotArr[[2]]`.] 

It contains $I \times J = 2 \times 5 = 10$ plots. To display the plot for the VD dataset for treatment 1 and reader 2, use `plotArr[[1]][[1,2]]` as shown below. 

```{r}

plotArr[[1]][[1,2]]

```

The plot is labeled **D2, i = 1, j = 2**, meaning the second dataset, the first treatment and the second reader. It contains 3 curves:

* The RSM fitted curve is in black. It is the only one with a dotted line connecting the uppermost continuously accessible operating point to (1,1).
* The PROPROC fitted curve is in red. 
* The CBM fitted curve is in blue. 

Three operating points from the binned data are shown as well as exact 95% confidence intervals for the lowest and uppermost operating points. 


### Displaying RSM parameter values {#rsm-3-fits-rsm-one-dataset}

The parameter values corresponding to RSM plot are accessed as shown next.


```{r}
cat("RSM mu = ", resultsArr[[1]][[2]]$retRsm$mu,
"\nRSM lambdaP = ", resultsArr[[1]][[2]]$retRsm$lambdaP,
"\nRSM nuP = ", resultsArr[[1]][[2]]$retRsm$nuP,
"\nRSM zeta_1 = ", as.numeric(resultsArr[[1]][[2]]$retRsm$zetas[1]),
"\nRSM AUC = ", resultsArr[[1]][[2]]$retRsm$AUC,
"\nRSM sigma_AUC = ", as.numeric(resultsArr[[1]][[2]]$retRsm$StdAUC),
"\nRSM NLLini = ", resultsArr[[1]][[2]]$retRsm$NLLIni,
"\nRSM NLLfin = ", resultsArr[[1]][[2]]$retRsm$NLLFin)
```

The first four values are the fitted values for the RSM parameters $\mu$, $\lambda'$, $\nu'$ and $\zeta_1$. The next value is the AUC under the fitted RSM curve followed by its standard error. The last two values are the initial and final values of negative log-likelihood ^[The initial value is calculated using initial estimates of parameters and the final value is that resulting from the log-likelihood maximization procedure].  


### Displaying CBM parameter values {#rsm-3-fits-cbm-one-dataset}


```{r}
cat("CBM mu = ", resultsArr[[1]][[2]]$retCbm$mu,
"\nCBM alpha = ", resultsArr[[1]][[2]]$retCbm$alpha,
"\nCBM zeta_1 = ", as.numeric(resultsArr[[1]][[2]]$retCbm$zetas[1]),
"\nCBM AUC = ", resultsArr[[1]][[2]]$retCbm$AUC,
"\nCBM sigma_AUC = ", as.numeric(resultsArr[[1]][[2]]$retCbm$StdAUC),
"\nCBM NLLini = ", resultsArr[[1]][[2]]$retCbm$NLLIni,
"\nCBM NLLfin = ", resultsArr[[1]][[2]]$retCbm$NLLFin)
```

The first three values are the fitted values for the CBM parameters $\mu$, $\alpha$, and $\zeta_1$. The next value is the AUC under the fitted CBM curve followed by its standard error. The last two values are the initial and final values of negative log-likelihood.  


### Displaying PROPROC parameter values {#rsm-3-fits-proproc-one-dataset}


```{r}
cat("PROPROC c = ", resultsArr[[1]][[2]]$c1,
"\nPROPROC d_a = ", resultsArr[[1]][[2]]$da,
"\nPROPROC AUC = ", resultsArr[[1]][[2]]$aucProp)
```

These values are identical to those listed for treatment 1 and reader 2 in Fig. \@ref(fig:rsm-3-fits-proproc-output-van-dyke). ^[Other values for PROPROC (e.g., standard error of AUC) are not available to me.]


## All plots for Van Dyke dataset {#rsm-3-fits-all-plots-van-dyke}

Shown next are the ten plots for the Van Dyke dataset. Each plot contains 3 curves, RSM (black), CBM (blue) and PROPROC (red). The plots are arranged in pairs, with the plot on the left corrsponding to treatment 1 and that on the right corresponding to treatment 2. 


```{r rsm-3-fits-f2, fig.cap="Each panel shows RSM (black), CBM (blue) and PROPROC (red) curves fitted to the same ROC dataset. Operating points are shown as filled circles (confidence intervals are only shown for the lowest and uppermost points). In each plot the labels at the top identify the dataset (f), the treatment (i) and the reader (j) indices.", fig.show='hold', echo=FALSE}
  f <- 1
  I <- dim(ret$allPlots[[f]])[1]
  J <- dim(ret$allPlots[[f]])[2]
  for (i in 1:I) {
    for (ip in i:I) {
      if (i == ip) next
      if (i > ip) next
      for (j in 1:J) {
        p1 <- plotArr[[f]][[i,j]]
        p2 <- plotArr[[f]][[ip,j]]
        gA <- ggplotGrob(p1)
        gB <- ggplotGrob(p2)
        #grid::grid.newpage()
        #grid::grid.draw(cbind(gA, gB))
        # see R/learn/_grid.arrange.Rmd
        grid.arrange(gA,gB, ncol=2, widths = c(1,1), 
             heights=unit(c(4), c("in")))
      }
    }
  }
```


## Overview of findings {#rsm-3-fits-overview}

With 14 datasets, comprising 43 modalities, 80 readers, 2012 cases, the total number of individual modality-reader combinations is 236: in other words, there are 236 datasets to each of which the three algorithms was applied. It is easy to be overwhelmed by the numbers and this section summarizes the most important conclusion: *all three fitting methods are consistent with a single method-independent AUC*.

If one accepts the proposition that the AUCs of the three methods are identical, then the following relations should hold: 


\begin{equation}
\left. 
\begin{aligned}
AUC_{PRO} =& m_{PR} AUC_{PRO}  \\
AUC_{CBM} =& m_{CR} AUC_{PRO}  
\end{aligned}
\right \}
(\#eq:rsm-3-fits-slopes-equation1)
\end{equation}

For example, a plot of PROPROC vs. RSM AUCs should be linear with zero intercept and slope $m_{PR}$ (PR = PROPROC vs. RSM; CR= CBM vs. RSM). The reason for the zero intercept is because if one of the AUCs indicates zero performance the other AUC must also be zero. Likewise, chance level performance (AUC = 0.5) must be common to all method of estimating AUC. Finally, perfect performance must be common to all methods. All of these conditions dictate a zero-intercept fitting model. 

An analysis was conducted to determine the average slopes (i.e., over all datasets) in Eqn. \@ref(eq:rsm-3-fits-slopes-equation1) and a bootstrap analysis was conducted to determine the corresponding confidence intervals. 


Plots of PROPROC-AUC vs. RSM-AUC and CBM-AUC vs. RSM-AUC, where each plot has the constrained linear fit superposed on the data points, are shown below. Each point corresponds to a distinct modality-reader combination. The average slopes and $R^2$ values ($R^2$ is the fraction of variance explained by the constrained straight line fit) are listed in Table \@ref(tab:rsm-3-fits-slopes-table1). 


```{r rsm-3-fits-inter-corr-scatter, echo=FALSE}
source(here("R/compare-3-fits/loadDataFile.R"))
source(here("R/compare-3-fits/slopesConvVsRsm.R"))
source(here("R/compare-3-fits/slopesConvVsRsmCI.R"))
```

```{r rsm-3-fits-inter-corr-scatter2, cache=TRUE, echo=FALSE}
ret <- slopesConvVsRsm(fileNames)
retCI <- slopesConvVsRsmCI(fileNames)
x <- cbind(ret$m_pro_rsm, ret$m_cbm_rsm)
x <- rbind(x, apply(x,2, mean))
x  <- round(x, digits = 4)
x <- rbind(x, 
           c("(1.005, 1.011)", # for seed = 1
             NA,
             "(0.992, 0.997)", 
             NA))
row.names(x) <- c(as.character(1:14), "AVG", "CI")
colnames(x) <- c("mProRsm", "R2ProRsm", "mCbmRsm", "R2CbmRsm")
```


```{r rsm-3-fits-plots-2, fig.cap="Dataset `D2` (Van Dyke): Left plot is PROPROC-AUC vs. RSM-AUC with the superposed constrained linear fit. The number of data points is `nPts` = 10, equal to `IJ`. Right plot is CBM-AUC vs. RSM-AUC. The slopes and R2 values are listed in the following Table.", fig.show='hold', echo=FALSE}
grid.arrange(ret$p1[[2]], ret$p2[[2]], ncol = 2)
```

```{r rsm-3-fits-plots-3, fig.cap="Similar to previous plot, for dataset `D3` (Franken), `nPts` = 20.", fig.show='hold', echo=FALSE}
grid.arrange(ret$p1[[3]], ret$p2[[4]], ncol = 3)
```


```{r rsm-3-fits-plots-7, fig.cap="Similar to previous plot, for dataset `D7` (Lucy Warren), `nPts` = 35.", fig.show='hold', echo=FALSE}
grid.arrange(ret$p1[[7]], ret$p2[[7]], ncol = 2)
```



```{r rsm-3-fits-slopes-table1, echo=FALSE}
kbl(x, caption = "Summary of slopes and correlations for the two constrained fits: PROPROC AUC vs. RSM AUC and CBM AUC vs. RSM AUC; see below. The average of each slope equals unity to within 0.6 percent.", booktabs = TRUE, escape = FALSE) %>% kable_styling(latex_options = c("basic", "scale_down", "HOLD_position"), row_label_position = "c") 
```

Table \@ref(tab:rsm-3-fits-slopes-table1): The first column, labeled $mProRsm$, shows results of fitting straight lines, constrained to go through the origin, to fitted PROPROC AUC vs. RSM AUC results, for each of the 14 datasets, as labeled. The second column, labeled $R2ProRsm$, lists the square of the correlation coefficient for each fit. The third and fourth columns list the corresponding values for the CBM AUC vs. RSM AUC fits. The second last row lists the averages (AVG) and the last row lists the 95 percent confidence intervals (CI) for the average slopes.


## Discussion / Summary {#rsm-3-fits-discussion-summary}

Over the years, there have been several attempts at fitting FROC data. Prior to the RSM-based ROC curve approach described in this chapter, all methods were aimed at fitting FROC curves, in the mistaken belief that this approach was using all the data. The earliest was the author's FROCFIT software36. This was followed by Swensson's approach37, subsequently shown to be equivalent to the author's earlier work, as far as predicting the FROC curve was concerned11. In the meantime, CAD developers, who relied heavily on the FROC curve to evaluate their algorithms, developed an empirical approach that was subsequently put on a formal basis in the IDCA method12. 

This chapter describes an approach to fitting ROC curves, instead of FROC curves, using the RSM. Fits were described for 14 datasets, comprising 236 distinct treatment-reader combinations. All fits and parameter values are viewable in the online "Supplemental Material" directory corresponding to this chapter. Validity of fit was assessed by the chisquare goodness of fit p-value; unfortunately using adjacent bin combining this could not be calculated in most instances; ongoing research at other ways of validating the fits is underway. PROPROC and CBM were fitted to the same datasets, yielding further validation and insights. One of the insights was the finding that the AUCS were almost identical, with PROPROC yielding the highest value, followed by CBM and closely by the RSM. The PROPROC-AUC / CBM-AUC, vs. RSM-AUC straight-line fits, constrained to go through the origin, had slopes 1.0255 (1.021, 1.030) and 1.0097 (1.006, 1.013), respectively. The R2 values were generally in excess of 0.999, indicative of excellent fits.

On the face of it, fitting the ROC curve seems to be ignoring much of the data. As an example, the ROC rating on a non-diseased case is the rating of the highest-rated mark on that image, or negative infinity if the case has no marks. If the case has several NL marks, only the highest rated one is used. In fact the highest rated mark contains information about the other marks on the case, namely they were all rated lower. There is a statistical term for this, namely sufficiency38. As an example, the highest of a number of samples from a uniform distribution is a sufficient statistic, i.e., it contains all the information contained in the observed samples. While not quite the same for normally distributed values, neglect of the NLs rated lower is not as bad as might seem at first. A similar argument applies to LLs and NLs on diseased cases. The advantage of fitting to the ROC is that the coupling of NLs and LLs on diseased cases breaks the degeneracy problem described in §18.2.3.

The reader may wonder why the author chose not to fit the wAFROC. After all, it is the recommended figure of merit for FROC studies. While the methods described in this chapter are readily adapted to the wAFROC, they are more susceptible to degeneracy issues. The reason is that the y-axis is defined by LL-events, in other words by the   parameters, while the x-axis is defined by the highest rated NL on non-diseased cases, in other words by the   parameter. The consequent decoupling of parameters leads to degeneracy of the type described in §18.2.3. This is avoided in ROC fitting because the y-axis is defined by LLs and NLs, in other words all parameters of the RSM are involved. The situation with the wAFROC is not quite as severe as with fitting FROC curves but it does have a problem with degeneracy. There are some ideas on how to improve the fits, perhaps by simultaneously fitting ROC and wAFROC-operating points, which amounts to putting constraints on the parameters, but for now this remains an open research subject. Empirical wAFROC, which is the current method implemented in RJafroc, is expected to have the same issues with variability of thresholds between treatments as the empirical ROC-AUC, as discussed in §5.9. So the fitting problem has to be solved. There is no need to fit the FROC, as it should never be used as a basis of a figure of merit for human observer studies; this is distinct from the severe degeneracy issues encountered with fitting it for human observers.

The application to a large number (236) of real datasets revealed that PROPROC has serious issues. These were apparently not revealed by the millions of simulations used to validate it39. To quote the cited reference, "The new algorithm never failed to converge and produced good fits for all of the several million datasets on which it was tested". This is a good illustration of why simulations studies are not a good alternative to the method described in §18.5.1.3.  In the author's experience, this is a common misconception in this field, and is discussed further in the following chapter. Fig. 18.5, panels (J), (K) and (L) show that PROPROC, and to a lesser extent CBM, can, under some circumstances, severely overestimate performance. Recommendations regarding usage of PROPROC and CBM are deferred to Chapter 20. 

The current ROC-based effort led to some interesting findings. The near equality of the AUCs predicted by the three proper ROC fitting methods, summarized in Table 18.4, has been noted, which is explained by the fact that proper ROC fitting methods represent different approaches to realizing an ideal observer, and the ideal observer must be unique, §18.6. 

This chapter explores what is termed inter-correlations, between RSM and CBM parameters. Since they have similar physical meanings, the RSM and CBM separation parameters were found to be significantly correlated,   = 0.86 (0.76, 0.89), as were the RSM and CBM parameters corresponding to the fraction of lesions that was actually visible,  = 0.77 (0.68, 0.82). This type of correspondence between two different models can be interpreted as evidence of mutually reinforcing validity of each of the models.

The CBM method comes closest to the RSM in terms of yielding meaningful measures, but the fact that it allows the ROC curve to go continuously to (1,1) implies that it is not completely accounting for search performance, §17.8. There are two components to search performance: finding lesions and avoiding non-lesions. The CBM model accounts for finding lesions, but it does not account for avoiding suspicious regions that are non-diseased, an important characteristic of expert radiologists.

An important finding is the inverse correlation between search performance and lesion-classification performance, which suggest there could be tradeoffs in attempts to optimize them. As a simplistic illustration, a low-resolution gestalt-view of the image1, such as seen by the peripheral viewing mechanism, is expected to make it easier to rapidly spot deviations from the expected normal template described in Chapter 15. However, the observer may not be able to switch effectively between this and the high-resolution viewing mode necessary to correctly classify found suspicious region. 

The main scientific conclusion of this chapter is that search-performance is the primary bottleneck in limiting observer performance. It is unfortunate that search is ignored in the ROC paradigm, usage of which is decreasing, albeit at an agonizingly slow rate. Evidence presented in this chapter should convince researchers to reconsider the focus of their investigations, most of which is currently directed at improving classification performance, which has been shown not to be the bottleneck. Another conclusion is that the three method of fitting ROC data yield almost identical AUCs. Relative to the RSM the PROPROC estimates are about 2.6% larger while CBM estimates are about 1% larger. This was a serendipitous finding that makes sense, in retrospect, but to the best of the author's knowledge is not known in the research community. PROPROC and to a lesser extent CBM are prone to severely overestimating performance in situations where the operating points are limited to a steep ascending section at the low end of false positive fraction scale. This parallels an earlier comment regarding the FROC, namely measurements derived from the steep part of the curve are unreliable, §17.10.1.


## Appendix 1


## References {#rsm-3-fits-references}


