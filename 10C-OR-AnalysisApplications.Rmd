---
output:
  pdf_document: default
  html_document: default
---
# Applications using the Obuchowski Rockette method {#ORApplications} 

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(kableExtra)
library(ggplot2)
library(RJafroc)
library(here)
```

## Introduction {#ORApplications-introduction}  

This chapter illustrates Obuchowski-Rockette analysis with several examples. The first example is a full-blown "hand-calculation" for `dataset02`, showing explicit implementations of formulae presented in the previous chapter. The second example shows application of the `RJafroc` package function `StSignificanceTesting()` to the same dataset: this function encapsulates all formulae and accomplishes all analyses with one function call. The third example shows application of the `StSignificanceTesting() function to an ROC dataset derived from the Federica Zanca dataset [@RN1882], which has five modalities and four readers. This illustrates multiple treatment pairings (in contrast, `dataset02` has only one treatment pairing). The fourth example shows application of `StSignificanceTesting()` to `dataset04`, which is an **FROC** dataset (in contrast to the previous examples, which employed **ROC** datasets). It illustrates the key difference involved in FROC analysis, namely the choice of figure of merit. The final example again uses `dataset04`, i.e., FROC data, *but this time we use DBM analysis*. Since DBM analysis is pseudovalue based, and the figure of merit is not the empirical AUC under the ROC, one may expect to see differences from the previously presented OR analysis on the same dataset.

Each analysis involves the following steps: 

* Calculate the figure of merit; 
* Calculate the variance-covariance matrix and mean-squares;
* Calculate the NH statistic, p-value and confidence interval(s).
* For each analysis, three sub-analyses are shown: 
    + random-reader random-case (RRRC),
    + fixed-reader random-case (FRRC), and
    + random-reader fixed-case (RRFC).

## Hand calculation using dataset02 {#ORApplications-dataset02-hand}

Dataset `dataset02 is well-know in the literature [@RN1993] as it has been widely used to illustrate advances in ROC methodology. The following code extract the numbers of modalities, readers and cases for `dataset02` and defines strings `modalityID`, `readerID` and `diffTRName` that will be needed later on.

```{r}
I <- length(dataset02$ratings$NL[,1,1,1])
J <- length(dataset02$ratings$NL[1,,1,1])
K <- length(dataset02$ratings$NL[1,1,,1])
modalityID <- dataset02$descriptions$modalityID
readerID <- dataset02$descriptions$readerID
diffTRName <- array(dim = choose(I, 2))
ii <- 1
for (i in 1:I) {
  if (i == I) 
    break
  for (ip in (i + 1):I) {
    diffTRName[ii] <- 
      paste0("trt", modalityID[i], 
             sep = "-", "trt", modalityID[ip])
    ii <- ii + 1
  }
}
```

The dataset consists of I = `r I` treatments,  J = `r J` readers and  K = `r K` cases.

### Random-Reader Random-Case (RRRC) analysis hand calculation {#ORApplications-RRRC-dataset02-hand}
* The first step is to calculate the figures of merit. 
* The following code uses `UtilFigureOfMerit()` to this end. Note that `FOM = "Wilcoxon"` has to be explicitly specified.

```{r}
foms <- UtilFigureOfMerit(dataset02, FOM = "Wilcoxon")
print(foms)
```

* For example, for the first treatment, `"trt0"`, the second reader `"rdr1"` figure of merit is `r foms["trt0", "rdr1"]`.
* The next step is to calculate the variance-covariance matrix and the mean-squares.
* The function `UtilORVarComponentsFactorial()` returns these quantities, which are saved to `vc`. 
* The `Factorial` in the function name is because this code applies to the factorial design. A different function is used for a split-plot design.

```{r}
vc <- UtilORVarComponentsFactorial(
  dataset02, FOM = "Wilcoxon", covEstMethod = "jackknife")
print(vc)
```

* The next step is the calculate the NH testing statistic. 
* The relevant equation is Eqn. \@ref(eq:F-ORH-RRRC). 
* `vc` contains the values needed in this equation, as follows:
+ MS(T) is in `vc$TRanova["T", "MS"]`, whose value is `r vc$TRanova["T", "MS"]`. 
+ MS(TR) is in `vc$TRanova["TR", "MS"]`, whose value is `r vc$TRanova["TR", "MS"]`. 
+ `Cov2` is in `vc$VarCom["Cov2", "Estimates"]`, whose value is `r vc$VarCom["Cov2", "Estimates"]`. 
+ `Cov3` is in `vc$VarCom["Cov3", "Estimates"]`, whose value is `r vc$VarCom["Cov3", "Estimates"]`. 

Applying Eqn. \@ref(eq:F-ORH-RRRC) one gets (`den` is the denominator on the right hand side of the referenced equation) and F_ORH_RRRC is the value of the F-statistic:

```{r}
den <- vc$TRanova["TR", "MS"] + 
  J* max(vc$VarCom["Cov2", "Estimates"] - 
           vc$VarCom["Cov3", "Estimates"],0)
F_ORH_RRRC <- vc$TRanova["T", "MS"]/den
print(F_ORH_RRRC)
```

* The F-statistic has numerator degrees of freedom $\text{ndf} = I - 1$ and denominator degrees of freedom, `ddf`, to be calculated next.
* From the previous chapter, `ddf` is calculated using Eqn. \@ref(eq:ddfH-RRRC)). The numerator of `ddf` is identical to `den^2`, where `den` was calculated in the preceding code block. The implementation follows:

```{r}
ddf <- den^2*(I-1)*(J-1)/(vc$TRanova["TR", "MS"])^2
print(ddf)
```

* The next step is calculation of the p-value for rejecting the NH
* The relevant equation is Eqn. \@ref(eq:pValueOR-RRRC) whose implementation follows: 

```{r}
p <- 1 - pf(F_ORH_RRRC, I - 1, ddf)
print(p)
```

* The difference is not significant at $\alpha$ = 0.05. 
* The next step is calculation of confidence intervals.
* Since `I` = 2, their is only one paired difference in reader-averaged FOMs, namely, the first treatment minus the second.

```{r}
trtMeans <- rowMeans(foms)
trtMeanDiffs <- trtMeans[1] - trtMeans[2]
names(trtMeanDiffs) <- "trt0-trt1"
print(trtMeans)
print(trtMeanDiffs)
```

* `trtMeans`contains the reader-averaged figures of merit for each treatment.
* `trtMeanDiffs`contains the reader-averaged difference figure of merit.
* From the previous chapter, the $(1-\alpha)$ confidence interval for $\theta_{1 \bullet} - \theta_{2 \bullet}$ is given by Eqn. \@ref(eq:CI-DiffFomRRRC), in which equation the expression inside the square-root symbol is `2/J*den`. 
* $\alpha$, the significance level of the test, is set to 0.05. 
* The implementation follows:

```{r}
alpha <- 0.05
stdErr <- sqrt(2/J*den)
t_crit <- abs(qt(alpha/2, ddf))
CI_RRRC <- c(trtMeanDiffs - t_crit*stdErr, 
             trtMeanDiffs + t_crit*stdErr)
names(CI_RRRC) <- c("Lower", "Upper")
print(CI_RRRC)
```

The confidence interval includes zero, which confirms the F-statistic finding that the reader-averaged FOM difference between treatments is not significant. 

Calculated next is the confidence interval for the reader-averaged FOM for each treatment. The relevant equations are Eqn. \@ref(eq:CI-RRRC-df-IndvlTrt) and Eqn. \@ref(eq:CI-RRRC-IndvlTrt). The implementation follows:

```{r}
df_i <- array(dim = I)
den_i <- array(dim = I)
stdErr_i <- array(dim = I)
ci <- array(dim = c(I, 2))
CI_RRRC_IndvlTrt <- data.frame()
for (i in 1:I) {
  den_i[i] <- vc$IndividualTrt[i, "msREachTrt"] + 
    J * max(vc$IndividualTrt[i, "cov2EachTrt"], 0)
  df_i[i] <- 
    (den_i[i])^2/(vc$IndividualTrt[i, "msREachTrt"])^2 * (J - 1)
  stdErr_i[i] <- sqrt(den_i[i]/J)
  ci[i,] <- 
    c(trtMeans[i] + qt(alpha/2, df_i[i]) * stdErr_i[i], 
      trtMeans[i] + qt(1-alpha/2, df_i[i]) * stdErr_i[i])
  rowName <- paste0("trt", modalityID[i])
  CI_RRRC_IndvlTrt <- rbind(
    CI_RRRC_IndvlTrt, 
    data.frame(Estimate = trtMeans[i], 
               StdErr = stdErr_i[i],
               DFi = df_i[i],
               CILower = ci[i,1],
               CIUpper = ci[i,2],
               Cov2i = vc$IndividualTrt[i,"cov2EachTrt"],
               row.names = rowName,
               stringsAsFactors = FALSE))
}
print(CI_RRRC_IndvlTrt)
```


### Fixed-Reader Random-Case (FRRC) analysis {#ORApplications-FRRC-dataset02-hand}
* The chi-square statistic is calculated using Eqn. \@ref(eq:DefFStatFRRC-OR) and Eqn. \@ref(eq:ChisqStatFRRC-OR). 
* The needed quantities are in `vc`. 
* For example, MS(T) is in vc$TRanova["T", "MS"], see above. Likewise for `Cov2` and `Cov3`.
* The remaining needed quantities are:
+ `Var` is in `vc$VarCom["Var", "Estimates"]`, whose value is `r vc$VarCom["Var", "Estimates"]`. 
+ `Cov1` is in `vc$VarCom["Cov1", "Estimates"]`, whose value is `r vc$VarCom["Cov1", "Estimates"]`. 
* The degree of freedom is $I-1$.
* The implementation follows:

```{r}
den_FRRC <- vc$VarCom["Var","Estimates"] - 
  vc$VarCom["Cov1","Estimates"] + 
  (J - 1) * max(vc$VarCom["Cov2","Estimates"] - 
                  vc$VarCom["Cov3","Estimates"] ,0)
chisqVal <- (I-1)*vc$TRanova["T","MS"]/den_FRRC
p <- 1 - pchisq(chisqVal, I - 1)
FTests <- data.frame(MS = c(vc$TRanova["T", "MS"], den_FRRC),
                     Chisq = c(chisqVal,NA),
                     DF = c(I - 1, NA),
                     p = c(p,NA),
                     row.names = c("Treatment", "Error"),
                     stringsAsFactors = FALSE)
print(FTests)
```

* Since p < 0.05, one has a significant finding. 
* Freezing reader variability shows a significant difference between the treatments. 
* The downside is that the conclusion applies only to the readers used in the study.
* The next step is to calculate the confidence interval for the reader-averaged FOM difference.
* The relevant equation is Eqn. \@ref(eq:CIDiffFomFRRC-OR), whose implementation follows.

```{r}
stdErr <- sqrt(2 * den_FRRC/J)
zStat <- vector()
PrGTz <- vector()
CI <- array(dim = c(choose(I,2),2))
for (i in 1:choose(I,2)) {
  zStat[i] <- trtMeanDiffs[i]/stdErr
  PrGTz[i] <- 2 * pnorm(abs(zStat[i]), lower.tail = FALSE)
  CI[i, ] <- c(trtMeanDiffs[i] + qnorm(alpha/2) * stdErr, 
               trtMeanDiffs[i] + qnorm(1-alpha/2) * stdErr)
}
ciDiffTrtFRRC <- data.frame(Estimate = trtMeanDiffs, 
                            StdErr = rep(stdErr, choose(I, 2)),
                            z = zStat, 
                            PrGTz = PrGTz, 
                            CILower = CI[,1],
                            CIUpper = CI[,2], 
                            row.names = diffTRName,
                            stringsAsFactors = FALSE)
print(ciDiffTrtFRRC)
```

* Consistent with the chi-square statistic significant finding, one finds that the treatment difference confidence interval does not include zero.
* The next step is to calculate the confidence interval for the reader-averaged figures of merit for each treatment.
* The relevant formula is in Eqn. \@ref(eq:CIIndTrtFomFRRC-OR), whose implementation follows:

```{r}
stdErr <- vector()
df <- vector()
CI <- array(dim = c(I,2))
ciAvgRdrEachTrt <- data.frame()
for (i in 1:I) {
  df[i] <- K - 1
  stdErr[i] <- 
    sqrt((vc$IndividualTrt[i,"varEachTrt"] + 
            (J-1)*max(vc$IndividualTrt[i,"cov2EachTrt"],0))/J)
  CI[i, ] <- c(trtMeans[i] + qnorm(alpha/2) * stdErr[i],
               trtMeans[i] + qnorm(1-alpha/2) * stdErr[i])
  rowName <- paste0("trt", modalityID[i])
  ciAvgRdrEachTrt <- 
    rbind(ciAvgRdrEachTrt, 
          data.frame(Estimate = trtMeans[i], 
                     StdErr = stdErr[i],
                     DF = df[i],
                     CILower = CI[i,1],
                     CIUpper = CI[i,2],
                     row.names = rowName,
                     stringsAsFactors = FALSE))
}
print(ciAvgRdrEachTrt)
```
* Finally, one calculates confidence intervals for the FOM differences for individual readers. 
* The relevant formula is in Eqn. \@ref(eq:CIIndRdrDiffFomFRRC-OR), whose implementation follows:

```{r}
trtMeanDiffs1 <- array(dim = c(J, choose(I, 2)))
Reader <- array(dim = c(J, choose(I, 2)))
stdErr <- array(dim = c(J, choose(I, 2)))
zStat <- array(dim = c(J, choose(I, 2)))
trDiffNames <- array(dim = c(J, choose(I, 2)))
PrGTz <- array(dim = c(J, choose(I, 2)))
CIReader <- array(dim = c(J, choose(I, 2),2))
ciDiffTrtEachRdr <- data.frame()
for (j in 1:J) {
  Reader[j,] <- rep(readerID[j], choose(I, 2))
  stdErr[j,] <- 
    sqrt(
      2 * 
        (vc$IndividualRdr[j,"varEachRdr"] - 
           vc$IndividualRdr[j,"cov1EachRdr"]))
  pair <- 1
  for (i in 1:I) {
    if (i == I) break
    for (ip in (i + 1):I) {
      trtMeanDiffs1[j, pair] <- foms[i, j] - foms[ip, j]
      trDiffNames[j,pair] <- diffTRName[pair]
      zStat[j,pair] <- trtMeanDiffs1[j,pair]/stdErr[j,pair]
      PrGTz[j,pair] <- 
        2 * pnorm(abs(zStat[j,pair]), lower.tail = FALSE)
      CIReader[j, pair,] <- 
        c(trtMeanDiffs1[j,pair] + 
            qnorm(alpha/2) * stdErr[j,pair], 
          trtMeanDiffs1[j,pair] + 
            qnorm(1-alpha/2) * stdErr[j,pair])
      rowName <- 
        paste0("rdr", Reader[j,pair], "::", trDiffNames[j, pair])
      ciDiffTrtEachRdr <- rbind(
        ciDiffTrtEachRdr, 
        data.frame(Estimate = trtMeanDiffs1[j, pair], 
                   StdErr = stdErr[j,pair], 
                   z = zStat[j, pair], 
                   PrGTz = PrGTz[j, pair], 
                   CILower = CIReader[j, pair,1],
                   CIUpper = CIReader[j, pair,2],
                   row.names = rowName,
                   stringsAsFactors = FALSE))
      pair <- pair + 1
    }
  }
}
print(ciDiffTrtEachRdr)
```

The notation in the first column shows the reader and the treatment pairing. For example, `rdr1::trt0-trt1` means the FOM difference for reader `rdr1`. Only the fifth reader, i.e., `rdr4`, shows a significant difference between the treatments: the p-value is `r ciDiffTrtEachRdr["rdr4::trt0-trt1", "PrGTz"]` and the confidence interval also does not include zero. The large FOM difference for this reader, `r ciDiffTrtEachRdr["rdr4::trt0-trt1", "Estimate"]`, was enough to result in a significant finding for FRRC analysis. The FOM differences for the other readers are about a factor of `r ciDiffTrtEachRdr["rdr4::trt0-trt1", "Estimate"]/ciDiffTrtEachRdr["rdr1::trt0-trt1", "Estimate"]` or more smaller than that for this reader.

### Random-Reader Fixed-Case (RRFC) analysis {#ORApplications-RRFC-dataset02-hand}
The F-statistic is shown in Eqn. \@ref(eq:DefFStatRRFC). This time `ndf` = $I-1$ and `ddf` = $(I-1) \times (J-1)$, the values proposed in the Obuchowski-Rockette paper. The implementation follows:

```{r}
den <- vc$TRanova["TR","MS"]
f <- vc$TRanova["T","MS"]/den
ddf <- ((I - 1) * (J - 1))
p <- 1 - pf(f, I - 1, ddf)
FTests_RRFC <- 
  data.frame(DF = c(I-1,(I-1)*(J-1)), 
             MS = c(vc$TRanova["T","MS"],vc$TRanova["TR","MS"]), 
             F = c(f,NA),  p = c(p,NA), 
             row.names = c("T","TR"), 
             stringsAsFactors = FALSE)
print(FTests_RRFC)
```

Freezing case variability also results in a significant finding, but the conclusion is only applicable to the specific case set used in the study. Next one calculates confidence intervals for the reader-averaged FOM differences, the relevant formula is in Eqn. \@ref(eq:CIDiffFomRRFC), whose implementation follows.

```{r}
stdErr <- sqrt(2 * den/J)
tStat <- vector()
PrGTt <- vector()
CI <- array(dim = c(choose(I,2), 2))
for (i in 1:choose(I,2)) {
  tStat[i] <- trtMeanDiffs[i]/stdErr
  PrGTt[i] <- 2 * 
    pt(abs(tStat[i]), ddf, lower.tail = FALSE)
  CI[i, ] <- c(trtMeanDiffs[i] + qt(alpha/2, ddf) * stdErr, 
               trtMeanDiffs[i] + qt(1-alpha/2, ddf) * stdErr)
}
ciDiffTrt_RRFC <- 
  data.frame(Estimate = trtMeanDiffs, 
             StdErr = rep(stdErr, choose(I, 2)), 
             DF = rep(ddf, choose(I, 2)), 
             t = tStat, 
             PrGTt = PrGTt, 
             CILower = CI[,1],
             CIUpper = CI[,2],
             row.names = diffTRName, 
             stringsAsFactors = FALSE)

print(ciDiffTrt_RRFC)
```
* As expected because the overall F-test showed significance, the confidence interval does not include zero (the p-value is identical to that found by the F-test). 
* This completes the hand calculations.

## Using RJafroc: dataset02 {#ORApplications-dataset02-RJafroc}
### Random-Reader Random-Case (RRRC) analysis {#ORApplications-RRRC-dataset02-RJafroc}
* This is accomplished using the significance testing function `StSignificanceTesting()`. 
* Since `analysisOption` is not explicitly specified in the following code, the function `StSignificanceTesting` performs all three analyses: `RRRC`, `FRRC` and `RRFC`.
* The significance level of the test, also an argument, `alpha`, defaults to 0.05. 
* The code below applies this function and saves the returned object to `st1`. 
* The first member of this object, a  `list` object named `FOMs`, is then displayed. 
* `FOMs` contains three data frames: 
+ `FOMS$foms`, the figures of merit for each treatment and reader, 
+ `FOMS$trtMeans`, the figures of merit for each treatment averaged over readers, and 
+ `FOMS$trtMeanDiffs`, the inter-treatment difference figures of merit averaged over readers. The difference is always the first treatment minus the second, etc., in this example, `trt0` minus `trt1`.

```{r}
st1 <- StSignificanceTesting(dataset02, FOM = "Wilcoxon", method = "OR")
print(st1$FOMs)
```

* Displayed next are the variance components and mean-squares. 
* These are contained in the `ANOVA` `list` object. 
* `ANOVA$TRanova` contains the treatment-reader ANOVA table, i.e. the sum of squares, the degrees of freedom and the mean-squares, listed for the treatment, reader and treatment-reader factors, i.e., `T`, `R` and `TR`.
* `ANOVA$VarCom` contains the OR variance components and the correlations.
* `ANOVA$IndividualTrt` contains the quantities necessary for individual treatment analyses.
* `ANOVA$IndividualRdr` contains the quantities necessary for individual reader analyses.

```{r}
print(st1$ANOVA)
```

* Displayed next are the results of RRRC analysis, contained in the `RRRC` `list` object.
* `RRRC$FTests` contains the results of the F-tests.
* `RRRC$ciDiffTrt` contains the results of the confidence intervals for the inter-treatment difference FOMs, averaged over readers.
* `RRRC$ciAvgRdrEachTrt` contains the results of the confidence intervals for the treatments, averaged over readers.

```{r}
print(st1$RRRC$FTests)
```

* TBA

```{r}
print(st1$RRRC$ciDiffTrt[,-c(2:5)])
```

* TBA

```{r}
print(st1$RRRC$ciAvgRdrEachTrt[,-c(2,3,6)])
```

* TBA

### Fixed-Reader Random-Case (FRRC) analysis {#ORApplications-FRRC-dataset02-RJafroc}

* TBA

```{r}
print(st1$FRRC$FTests)
```

* TBA

```{r}
print(st1$FRRC$ciDiffTrt[,-c(2:4)])
```

* TBA

```{r}
print(st1$FRRC$ciAvgRdrEachTrt[,-c(2,3,5)])
```

### Random-Reader Fixed-Case (RRFC) analysis {#ORApplications-RRFC-dataset02-RJafroc}

```{r}
print(st1$RRFC$FTests)
```

* TBA

```{r}
print(st1$RRFC$ciDiffTrt[,-c(2,3,4,5)])
```

## Using RJafroc: dataset04 {#ORApplications-dataset04-RJafroc}
* The second example uses the Federica Zanca dataset [@RN1882], i.e., `dataset04`, which has five modalities and four readers. 
* This illustrates the situation when multiple treatment pairings are involved. In contrast, the previous example had only one treatment pairing.
* Since this is an FROC dataset, in order to keep this comparable with the previous example, one converts it to an inferred-ROC dataset.
* The function `DfFroc2Roc(dataset04)` converts, using the highest-rating, the FROC dataset to an inferred-ROC dataset.
* The results are contained in the returned `list` object `st2`.

```{r}
ds <- DfFroc2Roc(dataset04) # convert to ROC
I <- length(ds$ratings$NL[,1,1,1])
J <- length(ds$ratings$NL[1,,1,1])
cat("I = ", I, ", J = ", J, "\n")
st2 <- StSignificanceTesting(ds, FOM = "Wilcoxon", method = "OR")
print(st2$FOMs)
print(st2$ANOVA$TRanova)
print(st2$ANOVA$VarCom)
```

### Random-Reader Random-Case (RRRC) analysis {#ORApplications-RRRC-dataset04}

```{r}
print(st2$RRRC$FTests)
```

* TBA

```{r}
print(st2$RRRC$ciDiffTrt[,-c(2:5)])
```

* TBA

```{r}
print(st2$RRRC$ciAvgRdrEachTrt[,-c(2,3,6)])
```

* TBA

### Fixed-Reader Random-Case (FRRC) analysis {#ORApplications-FRRC-dataset04}

```{r}
print(st2$FRRC$FTests)
```

* TBA

```{r}
print(st2$FRRC$ciDiffTrt[,-c(2:4)])
```

* TBA

```{r}
print(st2$FRRC$ciAvgRdrEachTrt[,-c(2,3,5)])
```

* TBA

```{r}
print(st2$FRRC$FTests)
```

* TBA

### Random-Reader Fixed-Case (RRFC) analysis {#ORApplications-RRFC-dataset04}

```{r}
print(st2$RRFC$FTests)
```

* TBA

```{r}
print(st2$RRFC$ciDiffTrt[,-c(2,3,4,5)])
```



## Using RJafroc: dataset04, FROC analysis {#ORApplications-dataset04-FROC-RJafroc}
* The third example uses `dataset04`, but this time we use the FROC data, i.e, we do not convert it to inferred-ROC. 
* Since this is an FROC dataset, one needs to use an FROC figure of merit. 
* In this example the weighted AFROC figure of merit `FOM = "wAFROC"` is specified. This is the recommended figure of merit when both normal and abnormal cases are present in the dataset.
* If the dataset does not contain normal cases, then the weighted AFROC1 figure of merit `FOM = "wAFROC1"` should be specified. 

```{r}
ds1 <- dataset04 # do NOT convert to ROC
# comment/uncomment following code to disable/enable unequal weights
# K2 <- length(ds1$ratings$LL[1,1,,1])
# weights <- array(dim = c(K2, max(ds1$lesions$perCase)))
# perCase <- ds1$lesions$perCase
# for (k2 in 1:K2) {
#   sum <- 0
#   for (el in 1:perCase[k2]) {
#     weights[k2,el] <- 1/el
#     sum <- sum + 1/el
#   }
#   weights[k2,1:perCase[k2]] <- weights[k2,1:perCase[k2]] / sum
# }
# ds1$lesions$weights <- weights
ds <- ds1
FOM <- "wAFROC" # also try wAFROC1, MaxLLF and MaxNLF
st3 <- StSignificanceTesting(ds, FOM = FOM, method = "OR")
print(st3$FOMs)
print(st3$ANOVA$TRanova)
print(st3$ANOVA$VarCom)
```

### Random-Reader Random-Case (RRRC) analysis {#ORApplications-RRRC-dataset04-FROC}

```{r}
print(st3$RRRC$FTests)
```

* TBA

```{r}
print(st3$RRRC$ciDiffTrt[,-c(2:5)])
```

* TBA

```{r}
print(st3$RRRC$ciAvgRdrEachTrt[,-c(2,3,6)])
```

* TBA

### Fixed-Reader Random-Case (FRRC) analysis {#ORApplications-FRRC-dataset04-FROC}

```{r}
print(st3$FRRC$FTests)
```

* TBA

```{r}
print(st3$FRRC$ciDiffTrt[,-c(2:4)])
```

* TBA

```{r}
print(st3$FRRC$ciAvgRdrEachTrt[,-c(2,3,5)])
```

* TBA

```{r}
print(st3$FRRC$FTests)
```

* TBA

### Random-Reader Fixed-Case (RRFC) analysis {#ORApplications-RRFC-dataset04-FROC}

```{r}
print(st3$RRFC$FTests)
```

* TBA

```{r}
print(st3$RRFC$ciDiffTrt[,-c(2,3,4,5)])
```


## Using RJafroc for dataset04, FROC analysis, DBM method {#ORApplications-dataset04-FROC-DBM-RJafroc}
* The fourth example again uses `dataset04`, i.e., FROC data, *but this time we use DBM analysis*.
* The key difference below is in the call to `StSignificanceTesting()` function, where we set `method = "DBM"`.
* Since DBM analysis is pseudovalue based, and the figure of merit is not the empirical AUC under the ROC, one expects to see differences from the previously presented OR analysis, contained in `st3`.

```{r}
st4 <- StSignificanceTesting(ds, FOM = FOM, method = "DBM") # Note: using DBM analysis
print(st4$FOMs)
print(st4$ANOVA$TRCanova)
print(st4$ANOVA$VarCom)
```

### Random-Reader Random-Case (RRRC) analysis {#ORApplications-RRRC-dataset04-FROC-DBM}

```{r}
print(st4$RRRC$FTests)
```

* TBA

```{r}
print(st4$RRRC$ciDiffTrt[,-c(2:5)])
```

* TBA

```{r}
print(st4$RRRC$ciAvgRdrEachTrt[,-c(2,3,6)])
```

* TBA

### Fixed-Reader Random-Case (FRRC) analysis {#ORApplications-FRRC-dataset04-FROC-DBM}

```{r}
print(st4$FRRC$FTests)
```

* TBA

```{r}
print(st4$FRRC$ciDiffTrt[,-c(2:4)])
```

* TBA

```{r}
print(st4$FRRC$ciAvgRdrEachTrt[,-c(2,3,5)])
```

* TBA

### Random-Reader Fixed-Case (RRFC) analysis {#ORApplications-RRFC-dataset04-FROC-DBM}

```{r}
print(st4$RRFC$FTests)
```

* TBA

```{r}
print(st4$RRFC$ciDiffTrt[,-c(2,3,4,5)])
```



## Discussion/Summary/5

## Tentative, need to think over {#ToMullOver1-tentative}
A comparison was run between results of OR and DBM for the FROC dataset. Except for `FRRC`, where differences are expected (because `ddf` in the former is $\infty$, while that in the later is $(I-1)\times(J-1))$, the results for the p-values were identical. This was true for the following FOMs: `wAFROC`, with equal and unequal weights, and `MaxLLF`. The confidence intervals (again, excluding `FRRC`) were identical for `FOM` = `wAFROC`. Slight differences were observed for `FOM` = `MaxLLF`.  

## References {#ORApplications-references}

