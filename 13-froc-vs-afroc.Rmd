# FROC vs. wAFROC {#froc-vs-afroc}

```
output:
  rmarkdown::pdf_document:
    fig_caption: yes        
    includes:  
      in_header: R/learn/my_header.tex

```

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval.after = "fig.cap"
)

library(RJafroc)
library(ggplot2)
library(kableExtra)
library(gridExtra)
library(abind)
library(here)
library(dplyr)
```

## Introduction {#froc-vs-wafroc-intro}

-   TBA This chapter needs a major rewrite; 10/6/20
-   One panel is being repeated
-   Don't need columns 1 and 3 in table

The FROC curve was introduced in [@bunch1977free] and ever since it has been widely used for evaluating performance in the free-response paradigm, particularly in 1 algorithm development. Typically 1 researchers report "sensitivity was observed to be xx at yy false positives per image." Occasionally, using less ambiguous terminology, they report an observed operating point on the FROC, as in "LLF was observed to be xx at NLF = yy". The lessons learned from ROC analysis, see Section \@ref(binary-task-beam-study), that a scalar FOM is preferable to sensitivity-specificity pairs, has apparently been forgotten.

This chapter recommends adoption of the wAFROC as the preferred operating characteristic in assessing performance in the free-response paradigm, and details simulation-based studies supporting this recommendation.

## FROC vs. wAFROC

Recall, from Section \@ref(froc-paradigm-preview-rsm), that the RSM is defined by parameters $\mu, \lambda, \nu$, and a threshold parameter $\zeta_1$ which determines if latent localizations are actually marked. This section examines RSM-predicted empirical FROC, wAFROC and ROC panels for two simulated observers denoted RAD-1 and RAD-2. The former could be an algorithmic observer while the latter could be a radiologist. For typical threshold $\zeta_1$ parameters, three types of simulations are considered: RAD-2 has moderately better performance than RAD-1, RAD-2 has much better performance than RAD-1 and RAD-2 has slightly better performance than RAD-1. For each type of simulation pairs of FROC, wAFROC and ROC curves are shown, one for each observer. Finally the simulations and panels are repeated for hypothetical RAD-1 and RAD-2 observers who report all suspicious regions, i.e., $\zeta_1 = -\infty$ for each observer. Both RAD-1 and RAD-2 observers share the same $\lambda, \nu$ parameters, and the only difference between them is in the $\mu$ and $\zeta_1$ parameters.   

```{r doOneFigure, echo=FALSE}
do_one_figure <- function(
  seed, Lmax, mu1, 
  mu2, lambda, nu, zeta1_1, zeta1_2, K1, K2) {
  set.seed(seed)
  Lk2 <- floor(runif(K2, 1, Lmax + 1))
  
  retABC <- CadVsRadPlots (
    mu1, 
    mu2, 
    lambda, 
    nu, 
    zeta1_1 = zeta1_1, 
    zeta1_2 = zeta1_2, 
    K1, 
    K2, 
    Lk2, 
    seed)
  
  froc_plot_A <- retABC$froc$Plot + labs(tag = "A")
  wafroc_plot_B <- retABC$wafroc$Plot + labs(tag = "B")
  roc_plot_C <- retABC$roc$Plot + labs(tag = "C")
  wafroc_1_B <- retABC$wafroc1
  wafroc_2_B <- retABC$wafroc2
  roc_1_C <- retABC$roc1
  roc_2_C <- retABC$roc2
  
  retDEF <- CadVsRadPlots (
    mu1, 
    mu2, 
    lambda, 
    nu, 
    zeta1_1 = -Inf, 
    zeta1_2 = -Inf, 
    K1, 
    K2, 
    Lk2, 
    seed)
  
  froc_plot_D <- retDEF$froc$Plot + labs(tag = "D")
  wafroc_plot_E <- retDEF$wafroc$Plot + labs(tag = "E")
  roc_plot_F <- retDEF$roc$Plot + labs(tag = "F")
  wafroc_1_E <- retDEF$wafroc1
  wafroc_2_E <- retDEF$wafroc2
  roc_1_F <- retDEF$roc1
  roc_2_F <- retDEF$roc2
  
  return(list(
    froc_plot_A = froc_plot_A,
    wafroc_plot_B = wafroc_plot_B,
    roc_plot_C = roc_plot_C,
    froc_plot_D = froc_plot_D,
    wafroc_plot_E = wafroc_plot_E,
    roc_plot_F = roc_plot_F,
    wafroc_1_B = wafroc_1_B,
    wafroc_2_B = wafroc_2_B,
    roc_1_C = roc_1_C,
    roc_2_C = roc_2_C,
    wafroc_1_E = wafroc_1_E,
    wafroc_2_E = wafroc_2_E,
    roc_1_F = roc_1_F,
    roc_2_F = roc_2_F
  ))
}
```


### Moderate difference in performance

```{r fig1, cache = FALSE, attr.source = ".numberLines"}
source(here("R/CH13-CadVsRadPlots/CadVsRadPlots.R"))

nu <- 1
lambda <- 1
K1 <- 500
K2 <- 700
mu1 <- 1.0
mu2 <- 1.5
zeta1_1 <- -1
zeta1_2 <- 1.5
Lmax <- 2
seed <- 1

ret <- do_one_figure (
  seed, Lmax, mu1, 
  mu2, lambda, nu, zeta1_1, zeta1_2, K1, K2)

froc_plot_1A <- ret$froc_plot_A
wafroc_plot_1B <- ret$wafroc_plot_B
roc_plot_1C <- ret$roc_plot_C
froc_plot_1D <- ret$froc_plot_D
wafroc_plot_1E <- ret$wafroc_plot_E
roc_plot_1F <- ret$roc_plot_F
wafroc_1_1B <- ret$wafroc_1_B
wafroc_2_1B <- ret$wafroc_2_B
roc_1_1C <- ret$roc_1_C
roc_2_1C <- ret$roc_2_C
wafroc_1_1E <- ret$wafroc_1_E
wafroc_2_1E <- ret$wafroc_2_E
roc_1_1F <- ret$roc_1_F
roc_2_1F <- ret$roc_2_F
```

The $\lambda$ and $\nu$ parameters are defined at lines 3 and 4 of the preceding code: $\lambda = \nu = 1$. The number of simulated cases is defined, lines 5-6, by $K_1 = 500$ and $K_2 = 700$. The simulated RAD-1 observer $\mu$ parameter is defined at line 7 by $\mu_{1} = 1$ and that of the simulated RAD-2 observer is defined at line 8 by $\mu_{2} = 1.5$. Based on these choices one expect RAD-2 to be moderately better than RAD-1. The corresponding threshold parameters are (lines 9 -10) $\zeta_{1} = -1$ for RAD-1 and $\zeta_{1} = 1.5$ for RAD-2. The maximum number of lesions per case is defined at line 11 by `Lmax` = 2. The actual number of lesions per case is determined determined by random sampling within the helper function `do_one_figure()` called at lines 14-16. This function returns a large list `ret`, whose contents are as follows:

* `ret$froc_plot_A`: a pair of FROC panels for the thresholds specified above, a red panel labeled "R: 1" corresponding to RAD-1 and a blue panel labeled "R: 2" corresponding to RAD-2. These are shown in panel A.
* `ret$wafroc_plot_B`: a pair of wAFROC panels, similarly labeled. These are shown in panel B.
* `ret$roc_plot_C`: a pair of ROC panels, similarly labeled. These are shown in panel C.
* `ret$froc_plot_D`: a pair of FROC panels for the both thresholds at $-\infty$. These are shown in panel D.
* `ret$froc_plot_E`: a pair of wAFROC panels for the both thresholds at $-\infty$. These are shown in panel E.
* `ret$froc_plot_F`: a pair of ROC panels for the both thresholds at $-\infty$. These are shown in panel F.
* `ret$wafroc_1_B`: the wAFROC AUC for RAD-1, i.e., the area under the curve labeled "R: 1" in panel B.
* `ret$wafroc_2_B`: the wAFROC AUC for RAD-2, i.e., the area under the curve labeled "R: 2" in panel B.
* `ret$roc_1_C`: the ROC AUC for RAD-1, i.e., the area under the curve labeled "R: 1" in panel C.
* `ret$roc_2_C`: the ROC AUC for RAD-2, i.e., the area under the curve labeled "R: 2" in panel C.
* `ret$wafroc_1_E`: the wAFROC AUC for RAD-1, i.e., the area under the curve labeled "R: 1" in panel E.
* `ret$wafroc_2_E`: the wAFROC AUC for RAD-2, i.e., the area under the curve labeled "R: 2" in panel E.
* `ret$roc_1_F`: the ROC AUC for RAD-1, i.e., the area under the curve labeled "R: 1" in panel F.
* `ret$roc_2_F`: the ROC AUC for RAD-2, i.e., the area under the curve labeled "R: 2" in panel F.


```{r extractEndPts1, echo=FALSE}
# extract coordinates of end-point
# the 1 refers to fig. 1
nlf_1_1A <- max(froc_plot_1A$data$genAbscissa[froc_plot_1A$data$Reader == "R: 1"]) 
llf_1_1A <- max(froc_plot_1A$data$genOrdinate[froc_plot_1A$data$Reader == "R: 1"]) 
nlf_2_1A <- max(froc_plot_1A$data$genAbscissa[froc_plot_1A$data$Reader == "R: 2"]) 
llf_2_1A <- max(froc_plot_1A$data$genOrdinate[froc_plot_1A$data$Reader == "R: 2"]) 

nlf_1_1D <- max(froc_plot_1D$data$genAbscissa[froc_plot_1D$data$Reader == "R: 1"]) 
llf_1_1D <- max(froc_plot_1D$data$genOrdinate[froc_plot_1D$data$Reader == "R: 1"]) 
nlf_2_1D <- max(froc_plot_1D$data$genAbscissa[froc_plot_1D$data$Reader == "R: 2"]) 
llf_2_1D <- max(froc_plot_1D$data$genOrdinate[froc_plot_1D$data$Reader == "R: 2"]) 
```



```{r froc-vs-afroc-plot1, fig.cap="Plots A and D: FROC curves for the RAD-1 and RAD-2 observers; B and E are corresponding wAFROC curves and C and F are corresponding ROC curves. All curves in this plot are for $\\lambda = \\nu = 1$. All RAD_1 curves are for $\\mu = 1$ and all RAD_2 curves are for $\\mu = 1.5$. For panels A, B and C, $\\zeta_1 = -1$ for RAD-1 and $\\zeta_1 = 1.5$ for RAD-2. For panels D, E and F, $\\zeta_1 = -\\infty$ for RAD-1 and RAD-2.", fig.show='hold', echo=FALSE}
grid.arrange(froc_plot_1A,wafroc_plot_1B,roc_plot_1C,froc_plot_1D,wafroc_plot_1E,roc_plot_1F,nrow=2,ncol=3)
```


The coordinates of the end-point of the RAD-1 FROC in panel A are (`r nlf_1_1A`, `r llf_1_1A`). Those of the RAD-2 FROC curve in A are (`r nlf_2_1A`, `r llf_2_1A`). The FROC for the RAD-1 observer extends to much larger NLF values while that for the RAD-2 observer is relatively short and steep. One suspects the RAD-2 observer is performing better than RAD-1: he is better at finding lesions and producing fewer NLs, both of which are desirable characteristics, but he is adopting a too-strict reporting criterion. If he could be induced to relax the threshold and report more NLs, his LLF would exceed that of the RAD-1 observer while still maintaining a lower NLF. However, as this involves a subjective extrapolation, it is not possible to objectively quantify this from the FROC curves. The basic issue is the lack of a common NLF range for the two panels. If a common NLF range is "forced", for example defined as the common NLF range 0 to `r max(froc_plot_1A$data$genAbscissa[froc_plot_1A$data$Reader == "R: 2"])`, where both curves contribute, it would ignore most NLs from the RAD-1 observer.

Algorithm developers typically quote LLF at a specified NLF. According to the two panels in A, the RAD-2 observer is better if the NLF value is chosen to less than `r max(froc_plot_1A$data$genAbscissa[froc_plot_1A$data$Reader == "R: 2"])` (this is the maximum NLF value for the RAD-2 curve in A) but there is no basis for comparison for larger values of NLF (because the RAD-2 observer does not provide any data beyond the observed end-point). A similar problem was encountered in ROC analysis when comparing a pair of sensitivity-specificity values, where, given differing choices of thresholds, ambiguous results can be obtained, see Section \@ref(binary-task-beam-study). Indeed, this was the rationale for using AUC under the ROC curve as an unambiguous measure of performance.

Plot B shows wAFROC curves for the same datasets whose FROC curves are shown in panel A. **The wAFROC is contained within the unit square, a highly desirable characteristic, which solves the lack of a common NLF range problem with the FROC.** The wAFROC AUC under the RAD-2 observer is visibly greater than that for the RAD-1 observer, even though -- due to his higher threshold -- his AUC estimate is actually biased downward (because the RAD-2 observer is adopting a high threshold, his $\text{LLF}_{\text{max}}$ is smaller than it would have been with a lower threshold, and consequently the area under the large straight line segment from the uppermost non-trivial operating point to (1,1) is smaller). AUCs under the two wAFROC panels in B are `r wafroc_1_1B` for RAD-1 and `r wafroc_2_1B` for RAD-2.


Plot C shows ROC curves. Since the curves cross, it is not clear which has the larger AUC. AUCs under the two curves in C are `r roc_1_1C` for RAD-1 and `r roc_2_1C` for RAD-2, which are close, but here is an example where the ordering given by the wAFROC is opposite to that given by the ROC. 

Plots D, E and F correspond to A, B and C with this important difference: the two threshold parameters are set to $-\infty$. The coordinates of the end-point of the RAD-1 FROC in panel D are (`r nlf_1_1D`, `r llf_1_1D`). Those of the RAD-2 FROC in panel D are (`r nlf_2_1D`, `r llf_2_1D`). The RAD-2 observer has higher LLF at lower NLF, and there can be no doubt that he is better. Panels E and F confirm that RAD-2 is actually the better observer *over the entire FPF range*. AUCs under the two wAFROC curves in E are `r wafroc_1_1E` for RAD-1 and `r wafroc_2_1E` for RAD-2. AUCs under the two ROC curves in F are `r roc_1_1F` for RAD-1 and `r roc_2_1F` for RAD-2. These confirm the visual impressions of panels in panels E and F. Notice that each ROC AUC is larger than the corresponding wAFROC AUC. This is because the probability of a lesion localization (case is declared positive *and* a lesion is correctly localized) is smaller than the probability of a true positive (case is declared positive). In other words, the ROC is everywhere above the wAFROC.


### Large difference in performance

```{r fig2, cache = FALSE, echo=FALSE}
source(here("R/CH13-CadVsRadPlots/CadVsRadPlots.R"))

nu <- 1
lambda <- 1
K1 <- 500
K2 <- 700
mu1 <- 1
mu2 <- 2
zeta1_1 <- -1
zeta1_2 <- 2
Lmax <- 2
seed <- 1

ret <- do_one_figure (
  seed, Lmax, mu1, 
  mu2, lambda, nu, zeta1_1, zeta1_2, K1, K2)

froc_plot_2A <- ret$froc_plot_A
wafroc_plot_2B <- ret$wafroc_plot_B
roc_plot_2C <- ret$roc_plot_C
froc_plot_2D <- ret$froc_plot_D
wafroc_plot_2E <- ret$wafroc_plot_E
roc_plot_2F <- ret$roc_plot_F
wafroc_1_2B <- ret$wafroc_1_B
wafroc_2_2B <- ret$wafroc_2_B
roc_1_2C <- ret$roc_1_C
roc_2_2C <- ret$roc_2_C
wafroc_1_2E <- ret$wafroc_1_E
wafroc_2_2E <- ret$wafroc_2_E
roc_1_2F <- ret$roc_1_F
roc_2_2F <- ret$roc_2_F
```


```{r extractEndPts2, echo=FALSE}
# extract coordinates of end-point
# nlf_1_2A = nlf_rdr1_fig2panelA
nlf_1_2A <- max(froc_plot_2A$data$genAbscissa[froc_plot_2A$data$Reader == "R: 1"]) 
llf_1_2A <- max(froc_plot_2A$data$genOrdinate[froc_plot_2A$data$Reader == "R: 1"]) 
nlf_2_2A <- max(froc_plot_2A$data$genAbscissa[froc_plot_2A$data$Reader == "R: 2"]) 
llf_2_2A <- max(froc_plot_2A$data$genOrdinate[froc_plot_2A$data$Reader == "R: 2"]) 

nlf_1_2D <- max(froc_plot_2D$data$genAbscissa[froc_plot_2D$data$Reader == "R: 1"]) 
llf_1_2D <- max(froc_plot_2D$data$genOrdinate[froc_plot_2D$data$Reader == "R: 1"]) 
nlf_2_2D <- max(froc_plot_2D$data$genAbscissa[froc_plot_2D$data$Reader == "R: 2"]) 
llf_2_2D <- max(froc_plot_2D$data$genOrdinate[froc_plot_2D$data$Reader == "R: 2"]) 

x <- wafroc_plot_2B$data$genAbscissa[wafroc_plot_2B$data$Reader == "R: 2"];fpfRad2B <- x[length(x)-1]
x <- wafroc_plot_2E$data$genAbscissa[wafroc_plot_2E$data$Reader == "R: 2"];fpfRad2E <- x[length(x)-1]
llf_2_2B <- max(froc_plot_2A$data$genOrdinate[froc_plot_2A$data$Reader == "R: 2"]) # llf is identical to that of A
llf_2_2E <- max(froc_plot_2D$data$genOrdinate[froc_plot_2D$data$Reader == "R: 2"]) # llf is identical to that of D
```

```{r froc-vs-afroc-plot2, fig.cap="Similar to preceding figure but with the following changes. All RAD_2 curves are for $\\mu = 2$ and for panels A, B and C $\\zeta_1 = 2$ for RAD-2.", fig.show='hold', echo=FALSE}
grid.arrange(froc_plot_2A,wafroc_plot_2B,roc_plot_2C,froc_plot_2D,wafroc_plot_2E,roc_plot_2F,nrow=2,ncol=3)
```


In Fig. \@ref(fig:froc-vs-afroc-plot2) panel A, the RAD-1 parameters are the same as in Fig. \@ref(fig:froc-vs-afroc-plot1), but the RAD-2 parameters are $\mu_{2} = 2$ and $\zeta_1 = +2$. Doubling the separation parameter over that of RAD-1 ($\mu_{1} = 1$) has a huge effect on performance. The end-point coordinates of the FROC for RAD-1 are (`r nlf_1_2A`, `r llf_1_2A`). The end-point coordinates of the FROC for RAD-2 are (`r nlf_2_2A`, `r llf_2_2A`). The common NLF region defined by NLF = 0 to NLF = `r nlf_2_2A` *would exclude almost all of the marks made by RAD-1*. The wAFROC panels in panel B show the markedly greater performance of RAD-2 over RAD-1 (the AUCs are `r wafroc_1_2B` for RAD-1 and `r wafroc_2_2B` for RAD-2). The inter-reader difference is larger (compared to Fig. \@ref(fig:froc-vs-afroc-plot1) panel B), despite the greater downward bias working against the RAD-2 observer. Panel C shows ROC panels for the two observers. Although the curves cross, it is evident that RAD-2 has the greater AUC. The AUCs are `r roc_1_2C` for RAD-1 and `r roc_2_2C` for RAD-2.

Plots D, E and F correspond to A, B and C with the difference that the two threshold parameters are set to $-\infty$. The coordinates of the end-point of the RAD-1 FROC in panel D are (`r nlf_1_2D`, `r llf_1_2D`). Those of the RAD-2 FROC in panel D are (`r nlf_2_2D`, `r llf_2_2D`). The RAD-2 observer has higher LLF at lower NLF, and there can be no doubt that he is better. Panels E and F confirm that RAD-2 is actually the better observer *over the entire FPF range*. AUCs under the two wAFROC curves in E are `r wafroc_1_2E` for RAD-1 and `r wafroc_2_2E` for RAD-2. AUCs under the two ROC curves in F are `r roc_1_2F` for RAD-1 and `r roc_2_2F` for RAD-2. These confirm the visual impressions of panels in panels E and F. Notice that each ROC AUC is larger than the corresponding wAFROC AUC. 

### Small difference in performance and identical thresholds
```{r fig3, cache = FALSE, echo=FALSE}
source(here("R/CH13-CadVsRadPlots/CadVsRadPlots.R"))

nu <- 1
lambda <- 1
K1 <- 500
K2 <- 700
mu1 <- 1.0
mu2 <- 1.1
zeta1_1 <- -1
zeta1_2 <- -1
Lmax <- 2
seed <- 1

ret <- do_one_figure (
  seed, Lmax, mu1, 
  mu2, lambda, nu, zeta1_1, zeta1_2, K1, K2)

froc_plot_3A <- ret$froc_plot_A
wafroc_plot_3B <- ret$wafroc_plot_B
roc_plot_3C <- ret$roc_plot_C
froc_plot_3D <- ret$froc_plot_D
wafroc_plot_3E <- ret$wafroc_plot_E
roc_plot_3F <- ret$roc_plot_F
wafroc_1_3B <- ret$wafroc_1_B
wafroc_2_3B <- ret$wafroc_2_B
roc_1_3C <- ret$roc_1_C
roc_2_3C <- ret$roc_2_C
wafroc_1_3E <- ret$wafroc_1_E
wafroc_2_3E <- ret$wafroc_2_E
roc_1_3F <- ret$roc_1_F
roc_2_3F <- ret$roc_2_F
```



```{r extractEndPts3, echo=FALSE}
# extract coordinates of end-point
nlf_1_3A <- max(froc_plot_3A$data$genAbscissa[froc_plot_3A$data$Reader == "R: 1"]) 
llf_1_3A <- max(froc_plot_3A$data$genOrdinate[froc_plot_3A$data$Reader == "R: 1"]) 
nlf_2_3A <- max(froc_plot_3A$data$genAbscissa[froc_plot_3A$data$Reader == "R: 2"]) 
llf_2_3A <- max(froc_plot_3A$data$genOrdinate[froc_plot_3A$data$Reader == "R: 2"]) 

nlf_1_3D <- max(froc_plot_3D$data$genAbscissa[froc_plot_3D$data$Reader == "R: 1"]) 
llf_1_3D <- max(froc_plot_3D$data$genOrdinate[froc_plot_3D$data$Reader == "R: 1"]) 
nlf_2_3D <- max(froc_plot_3D$data$genAbscissa[froc_plot_3D$data$Reader == "R: 2"]) 
llf_2_3D <- max(froc_plot_3D$data$genOrdinate[froc_plot_3D$data$Reader == "R: 2"]) 
```


```{r froc-vs-afroc-plot3, fig.cap="Similar to preceding figure but with the following changes. All RAD_2 curves are for $\\mu = 1.1$ and for panels A, B and C, $\\zeta_1 = -1$ for RAD-2.", fig.show='hold', echo=FALSE}
grid.arrange(froc_plot_3A,wafroc_plot_3B,roc_plot_3C,froc_plot_3D,wafroc_plot_3E,roc_plot_3F,nrow=2,ncol=3)
```


The final example, Fig. \@ref(fig:froc-vs-afroc-plot3) shows that *when there is a small difference in performance*, there is less ambiguity in using the FROC as a basis for measuring performance. The RAD-1 parameters are the same as in Fig. \@ref(fig:froc-vs-afroc-plot1) but the RAD-2 parameters are $\mu = 1.1$ and $\zeta_1= -1$. In other words, the $\mu$ parameter is 10% larger and the thresholds are identical. This time there is much more common NLF range overlap in panel A and one is counting most of the marks for the RAD-1 reader. The end-point coordinates of the FROC for RAD-1 are (`r nlf_1_3A`, `r llf_1_3A`). The end-point coordinates of the FROC for RAD-2 are (`r nlf_2_3A`, `r llf_2_3A`). The common NLF region defined by NLF = 0 to NLF = `r nlf_2_3A` includes almost all of the marks made by RAD-1. The wAFROC panels in panel B show the slight greater performance of RAD-2 over RAD-1 (the AUCs are `r wafroc_1_3B` for RAD-1 and `r wafroc_2_3B` for RAD-2). Panel C shows ROC panels for the two observers. Although the curves cross, it is evident that RAD-2 has the greater AUC. The AUCs are `r roc_1_2C` for RAD-1 and `r roc_2_2C` for RAD-2.

Plots D, E and F correspond to A, B and C with the difference that the two threshold parameters are set to $-\infty$. The coordinates of the end-point of the RAD-1 FROC in panel D are (`r nlf_1_3D`, `r llf_1_3D`). Those of the RAD-2 FROC in panel D are (`r nlf_2_3D`, `r llf_2_3D`). Panels E and F confirm that RAD-2 is actually the better observer over the entire FPF range. AUCs under the two wAFROC curves in E are `r wafroc_1_3E` for RAD-1 and `r wafroc_2_3E` for RAD-2. AUCs under the two ROC curves in F are `r roc_1_3F` for RAD-1 and `r roc_2_3F` for RAD-2. These confirm the visual impressions of panels in panels E and F. Notice that each ROC AUC is larger than the corresponding wAFROC AUC. 

STOP convert this to a table
## Effect size comparison {#froc-vs-afroc-effect-sizes}

In all three figures the wAFROC effect size (the difference in AUCs) is larger than the corresponding ROC effect size. 

* For Fig. \@ref(fig:froc-vs-afroc-plot1) panels B and C:
   + The wAFROC effect size is `r (wafroc_2_1B - wafroc_1_1B)`, 
   + The ROC effect size is `r (roc_2_1C - roc_1_1C)`. 
* For Fig. \@ref(fig:froc-vs-afroc-plot2) panels B and C: 
   + The wAFROC effect size is `r (wafroc_2_2B - wafroc_1_2B)`, 
   + The ROC effect size is `r (roc_2_2C - roc_1_2C)`. 
* For Fig. \@ref(fig:froc-vs-afroc-plot3) panels B and C: 
   + The wAFROC effect size is `r (wafroc_2_3B - wafroc_1_3B)`, 
   + The ROC effect size is `r (roc_2_3C - roc_1_3C)`. 

Since effect size enters as the *square* in sample size formulas, it is no wonder that wAFROC yields greater statistical power than ROC. The small difference example is more typical of modality comparison studies where the modalities being compared are only slightly different. In this case the wAFROC effect size is about twice the corresponding ROC value - see chapter on FROC sample size TBA.


## Summary of simulations

In order to get a better overview, the following tables summarize the numerical values from the plots in this chapter. Table \@ref(tab:froc-vs-afroc-summary-table-rdr1) refers to the RAD-1 observer, and Table \@ref(tab:froc-vs-afroc-summary-table-rdr2) refers to the RAD-2 observer.

### Summary of RAD-1 simulations

```{r rdr1, echo=FALSE}
dig <- 4

#cell1 <- paste0("(", as.character(format(nlf_1_1A, digits = dig)), ", ", as.character(format(llf_1_1A, digits = dig)), ")")
#cell2 <- paste0("(", as.character(format(nlf_1_1D, digits = dig)), ", ", as.character(format(llf_1_1D, digits = dig)), ")")
cell1 <- as.character(format(wafroc_1_1B, digits = dig))
cell2 <- as.character(format(wafroc_1_1E, digits = dig))
cell3 <- as.character(format(roc_1_1C, digits = dig))
cell4 <- as.character(format(roc_1_1F, digits = dig))
```


```{r froc-vs-afroc-summary-table-rdr1, echo=FALSE}
tableCells = array(dim = c(1,4))

tableCells[1, 1]  <- cell1
tableCells[1, 2]  <- cell2
tableCells[1, 3]  <- cell3
tableCells[1, 4]  <- cell4

df <- as.data.frame(tableCells)
colnames(df) <- c("wAFROC-B", "wAFROC-E", "ROC-C", "ROC-F")
knitr::kable(df, caption = "Summary of RAD-1 simulations: A refers to panel A, B refers to panel B, etc.", escape = FALSE)
```


-   The first column is labeled "wAFROC-B", meaning the RAD-1 wAFROC AUC in panel B, which are identical for the three figures (one may visually confirm that the red curves in panels A, B ad C in the three figures are identical; likewise for the red curves in panels D, E and F).
-   The second column is labeled "wAFROC-E", meaning the RAD-1 wAFROC AUC in panel E, which are identical for the three figures.
-   The third column is labeled "ROC-C", meaning the RAD-1 ROC AUC in panel C, which are identical for the three figures.
-   The fourth column is labeled "ROC-F", meaning the RAD-1 ROC AUC in panel F, which are identical for the three figures.


### Summary of RAD-2 simulations
```{r rdr2, echo=FALSE}

cell_11 <- "1"
#cell_12 <- paste0("(", as.character(format(nlf_2_1A, digits = dig)), ", ", as.character(format(llf_2_1A, digits = dig)), ")")
#cell_13 <- paste0("(", as.character(format(nlf_2_1D, digits = dig)), ", ", as.character(format(llf_2_1D, digits = dig)), ")")
cell_12 <- as.character(format(wafroc_2_1B, digits = dig))
cell_13 <- as.character(format(wafroc_2_1E, digits = dig))
cell_14 <- as.character(format(roc_2_1C, digits = dig))
cell_15 <- as.character(format(roc_2_1F, digits = dig))

cell_21 <- "2"
#cell_22 <- paste0("(", as.character(format(nlf_2_2A, digits = dig)), ", ", as.character(format(llf_2_2A, digits = dig)), ")")
#cell_23 <- paste0("(", as.character(format(nlf_2_2D, digits = dig)), ", ", as.character(format(llf_2_2D, digits = dig)), ")")
cell_22 <- as.character(format(wafroc_2_2B, digits = dig))
cell_23 <- as.character(format(wafroc_2_2E, digits = dig))
cell_24 <- as.character(format(roc_2_2C, digits = dig))
cell_25 <- as.character(format(roc_2_2F, digits = dig))

cell_31 <- "3"
#cell_32 <- paste0("(", as.character(format(nlf_2_3A, digits = dig)), ", ", as.character(format(llf_2_3A, digits = dig)), ")")
#cell_33 <- paste0("(", as.character(format(nlf_2_3D, digits = dig)), ", ", as.character(format(llf_2_3D, digits = dig)), ")")
cell_32 <- as.character(format(wafroc_2_3B, digits = dig))
cell_33 <- as.character(format(wafroc_2_3E, digits = dig))
cell_34 <- as.character(format(roc_2_3C, digits = dig))
cell_35 <- as.character(format(roc_2_3F, digits = dig))
```


```{r froc-vs-afroc-summary-table-rdr2, echo=FALSE}
tableCells = array(dim = c(3,5))

tableCells[1,]  <- c(cell_11, cell_12, cell_13, cell_14, cell_15)#, cell_16, cell_17)
tableCells[2,]  <- c(cell_21, cell_22, cell_23, cell_24, cell_25)#, cell_26, cell_27)
tableCells[3,]  <- c(cell_31, cell_32, cell_33, cell_34, cell_35)#, cell_36, cell_37)
                     
df <- as.data.frame(tableCells)
rownames(df) <- c("1","2","3")
colnames(df) <- c("Fig", "wAFROC-B", "wAFROC-E", "ROC-C", "ROC-F")
knitr::kable(df, caption = "Summary of RAD-2 simulations: Fig refers to the figure number in this chapter, A refers to panel A, B refers to panel B, etc.", escape = FALSE)
```

-   The first column refers to the figure number, for example, "1" refers to Fig. \@ref(fig:froc-vs-afroc-plot1), "2" refers to Fig. \@ref(fig:froc-vs-afroc-plot2), and "3" refers to Fig. \@ref(fig:froc-vs-afroc-plot3).
-   The second column is labeled "wAFROC-B", meaning the RAD-2 wAFROC AUC corresponding to the blue curve in panel B.
-   The third column is labeled "wAFROC-E", meaning the RAD-2 wAFROC AUC corresponding to the blue curve in panel E.
-   The fourth column is labeled "ROC-C", meaning the RAD-2 ROC AUC corresponding to the blue curve in panel C.
-   The fifth column is labeled "ROC-F", meaning the RAD-2 ROC AUC corresponding to the blue curve in panel F.



## Comments {#froc-vs-wafroc-comments}

-   For the same figure label the RAD-1 panels are identical in the three figures. This is the reason why Table \@ref(tab:froc-vs-afroc-summary-table-rdr1) has only one row. A *fixed* RAD-1 dataset is being compared to *varying* RAD-2 datasets.
-   The first RAD-2 dataset, Fig. \@ref(fig:froc-vs-afroc-plot1) A, B or C, might be considered representative of an average radiologist, the second one, Fig. \@ref(fig:froc-vs-afroc-plot2) A, B or C, is a super-expert and the third one, Fig. \@ref(fig:froc-vs-afroc-plot3) A, B or C, is only nominally better than RAD-1.
-   Plots D, E and F are for hypothetical RAD-1 and RAD-2 observers that report *all* suspicious regions. The differences between A and D are minimal for the RAD-1 observer, but marked for the RAD-2 observer. Likewise for the differences between B and E.


## Discussion {#froc-vs-wafroc-Discussion}

## References {#froc-vs-wafroc-references}
