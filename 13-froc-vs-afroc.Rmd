# FROC vs. wAFROC {#froc-vs-afroc}

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(RJafroc)
library(ggplot2)
library(kableExtra)
library(gridExtra)
library(abind)
library(here)
```

## Introduction {#froc-vs-wafroc-intro}

-   TBA This chapter needs a major rewrite; 10/6/20
-   One plot is being repeated
-   Need to add comparisons to ROC
-   Don't need columns 1 and 3 in table

The FROC curve was introduced in [@bunch1977free] and ever since it has been widely used for evaluating performance in the free-response paradigm, particularly in CAD algorithm development. Typically CAD researchers report "sensitivity was observed to be xx at yy false positives per image." Occasionally, using less ambiguous terminology, they report an observed operating point on the FROC, as in "LLF was observed to be xx at NLF = yy". The lessons learned from ROC analysis, see Section \@ref(binary-task-beam-study), that a scalar FOM is preferable to sensitivity-specificity pairs, has apparently been forgotten.

This chapter recommends adoption of the wAFROC as the preferred operating characteristic in assessing performance in the free-response paradigm, and details simulation-based studies supporting this recommendation.

## FROC vs. wAFROC

This section examines RSM-predicted FROC and wAFROC plots for a simulated CAD (algorithmic) and RAD (radiologist) observers. Recall, from Section \@ref(froc-paradigm-preview-rsm), that the RSM is defined by 3 parameters $\mu, \lambda, \nu$ and the lowest reporting threshold parameter $\zeta_1$ which determines if latent localizations are actually marked.

```{r, cache = TRUE, attr.source = ".numberLines"}
source(here("R/CH13-GenerateCadVsRadPlots/GenerateCadVsRadPlots.R"))

nu <- 1
lambda <- 1
K1 <- 500
K2 <- 700
muCad <- 1.0
muRad <- 1.5
zeta1Cad <- -1
zeta1Rad <- 1.5
Lmax <- 2
seed <- 1
set.seed(seed)
Lk2 <- floor(runif(K2, 1, Lmax + 1))

ret1AB <- GenerateCadVsRadPlots (
  muCad, 
  muRad, 
  zeta1Cad, 
  zeta1Rad, 
  K1, 
  K2, 
  Lk2, 
  seed)

froc_plot_1A <- ret1AB$froc$Plot + labs(tag = "A")
wafroc_plot_1B <- ret1AB$wafroc$Plot + labs(tag = "B")
wafroc_cad_1B <- ret1AB$fomCad
wafroc_rad_1B <- ret1AB$fomRad

zeta1Cad <- -Inf
zeta1Rad <- -Inf

ret1CD <- GenerateCadVsRadPlots (
  muCad, 
  muRad, 
  zeta1Cad, 
  zeta1Rad, 
  K1, 
  K2, 
  Lk2, 
  seed)

froc_plot_1C <- ret1CD$froc$Plot + labs(tag = "C")
wafroc_plot_1D <- ret1CD$wafroc$Plot + labs(tag = "D")
wafroc_cad_1D <- ret1CD$fomCad
wafroc_rad_1D <- ret1CD$fomRad
```

Both CAD and RAD observers share the same $\lambda, \nu$. These are defined at lines 3 and 4 of the preceding code: $\lambda = \nu = 1$. The number of simulated cases is defined, lines 5-6, by $K_1 = 500$ and $K_2 = 700$. The simulated CAD observer $\mu$ parameter is defined at line 7 by by $\mu_{CAD} = 1$ and that of the simulated RAD observer is defined at line 8 by $\mu_{RAD} = 1.5$. The corresponding threshold parameters are (lines 9 -10) $\zeta_{1} = -1$ for CAD and $\zeta_{1} = 1.5$ for RAD. The maximum number of lesions per case is defined at line 11 by `Lmax` = 2. The actual number of lesions per case `Lk2` is determined at line 14 (`Lk2` is a $K_2$ length array consisting of random integers 1 or 2).

Line 16 calls the helper function `GenerateCadVsRadPlots()` (the file containing this function is sourced at line 1), which calculates the FROC and wAFROC plots and other statistics. The FROC is extracted at line 26 and labeled A, while the wAFROC is extracted at line 27 and labeled B. The following code extracts the coordinates of the end-points of the respective curves.

```{r, echo=TRUE}
# extract coordinates of end-point
nlfCad1A <- max(froc_plot_1A$data$genAbscissa[froc_plot_1A$data$Reader == "R: CAD"]) 
llfCad1A <- max(froc_plot_1A$data$genOrdinate[froc_plot_1A$data$Reader == "R: CAD"]) 
nlfRad1A <- max(froc_plot_1A$data$genAbscissa[froc_plot_1A$data$Reader == "R: RAD"]) 
llfRad1A <- max(froc_plot_1A$data$genOrdinate[froc_plot_1A$data$Reader == "R: RAD"]) 

nlfCad1C <- max(froc_plot_1C$data$genAbscissa[froc_plot_1C$data$Reader == "R: CAD"]) 
llfCad1C <- max(froc_plot_1C$data$genOrdinate[froc_plot_1C$data$Reader == "R: CAD"]) 
nlfRad1C <- max(froc_plot_1C$data$genAbscissa[froc_plot_1C$data$Reader == "R: RAD"]) 
llfRad1C <- max(froc_plot_1C$data$genOrdinate[froc_plot_1C$data$Reader == "R: RAD"]) 
```

```{r froc-vs-afroc-plot1, fig.cap="Plots A and B are for CAD $\\zeta_1 = -1$ and RAD $\\zeta_1 = 1.5$ and plots C and D are plots for CAD $\\zeta_1 = -\\infty$ and RAD $\\zeta_1 = -\\infty$. Plots A and C: FROC curves for the CAD and RAD observers. B and D: corresponding wAFROC curves.", fig.show='hold', echo=FALSE}
grid.arrange(froc_plot_1A,wafroc_plot_1B,froc_plot_1C,wafroc_plot_1D,nrow=2,ncol=2)
```

The coordinates of the end-point of the CAD FROC in plot A are (`r nlfCad1A`, `r llfCad1A`). Those of the RAD FROC plot in A are (`r nlfRad1A`, `r llfRad1A`). The FROC for the CAD observer extends to much larger NLF values while that for the RAD observer is relatively short and steeper, as in Fig. \@ref(fig:froc-vs-afroc-plot1), plot A. One suspects the RAD observer is performing better than CAD. He is better at finding lesions and producing fewer NLs, both of which are desirable characteristics. One suspects that *if* he could be induced to relax the threshold and report more NLs, his LLF would exceed that of the CAD observer while still maintaining a lower $\text{NLF}_{\text{max}}$. However, lacking the ability to induce the radiologist to relax his threshold, it is not possible to quantify this suspicion from the observed FROC curves. [^froc-vs-afroc-1]

[^froc-vs-afroc-1]: The basic issue is the lack of a common NLF range for the two plots. If a common NLF range is "forced", for example defined as the common NLF range 0 to `r max(froc_plot_1A$data$genAbscissa[froc_plot_1A$data$Reader == "R: RAD"])` where both curves contribute, it would ignore most NLs from the CAD observer.

CAD algorithm developers typically quote LLF at a specified NLF. According to the two plots in A, the RAD observer is better if the NLF value is chosen to less than `r max(froc_plot_1A$data$genAbscissa[froc_plot_1A$data$Reader == "R: RAD"])` (this is the maximum NLF value for the RAD plot in A) but there is no basis for comparison for larger values of NLF (because the RAD observer does not provide any data beyond the observed end-point). A similar problem was encountered in ROC analysis when comparing a pair of sensitivity-specificity values, where, given differing choices of thresholds, ambiguous results can be obtained, see Section \@ref(binary-task-beam-study). Indeed, this was the rationale for using AUC under the ROC curve as an unambiguous measure of performance.

wAFROC curves, for the same datasets, whose FROC curves are shown in plot A, are shown in plot B. **Like the ROC, the wAFROC is contained within the unit square, a highly desirable characteristic, which solves the lack of a common NLF range problem with the FROC.** The wAFROC AUC under the RAD observer is visibly greater than that for the CAD observer, even though -- due to his higher threshold -- his AUC estimate is actually biased downward against him [^froc-vs-afroc-2]. AUCs under the two wAFROC plots in B are `r wafroc_cad_1B` for CAD and `r wafroc_rad_1B` for RAD, consistent with the visual impression of RAD \> CAD.

[^froc-vs-afroc-2]: Because the RAD observer is adopting a high threshold $\zeta_1 = 1.5$, his $\text{LLF}_{\text{max}}$ is smaller than it would have been with a lower threshold, and consequently the area under the large straight line segment from the uppermost non-trivial operating point to (1,1) is smaller than would have been the case with a lower threshold.

Since plots A and B are based on different choices of lowest reporting threshold, it is pertinent to ask what happens for identical thresholds. Lines 31-32 set the two threshold parameters to $-\infty$ and line 34 calls the function `GenerateCadVsRadPlots()` with these new values. The FROC is extracted at line 44 and labeled C, while the wAFROC is extracted at line 45 and labeled D.

The coordinates of the end-point of the CAD FROC in plot C are (`r nlfCad1C`, `r llfCad1C`). Those of the RAD FROC plot in C are (`r nlfRad1C`, `r llfRad1C`). The RAD observer has higher LLF at lower NLF, and there is no doubt that he is better. Plot C confirms that RAD is actually the better observer *over his entire NLF range*.

Plot D shows the corresponding wAFROC curves. The AUCs are `r wafroc_cad_1D` for CAD and `r wafroc_rad_1D` for RAD, confirming that the RAD observer is indeed better. Moreover, this comparison, based on comparing two scalars, is unambiguous. [^froc-vs-afroc-3]

[^froc-vs-afroc-3]: The differences from the previous values (corresponding to plot B) namely `r wafroc_cad_1B` for CAD and `r wafroc_rad_1B` for RAD, is much larger -- and visually striking -- for the RAD observer than for the CAD observer. This is because the CAD observer was already adopting a low threshold $\zeta_1 = -1$ in plot B, so lowering it to $-\infty$ in plot D has a smaller effect.

### Large difference in performance

It is interesting to compare the two readers when there is large difference in performance. In Fig. \@ref(fig:froc-vs-afroc-plot2) (A), which exaggerates the difference between CAD and RAD, the CAD parameters are the same as in Fig. \@ref(fig:froc-vs-afroc-plot1), but the RAD parameters are $\mu_{RAD} = 2$ and $\zeta_1 = +2$. Doubling the separation parameter over that of CAD, $\mu_{CAD} = 1$, has a huge effect on performance.

```{r, cache = TRUE, echo=FALSE}
source(here("R/CH13-GenerateCadVsRadPlots/GenerateCadVsRadPlots.R"))

nu <- 1
lambda <- 1
K1 <- 500
K2 <- 700
muCad <- 1.0
muRad <- 2
zeta1Cad <- -1
zeta1Rad <- 2
Lmax <- 2
seed <- 1
set.seed(seed)
Lk2 <- floor(runif(K2, 1, Lmax + 1))

ret2AB <- GenerateCadVsRadPlots (muCad, muRad, zeta1Cad, zeta1Rad, K1, K2, Lk2, seed)

froc_plot_2A <- ret2AB$froc$Plot + labs(tag = "A")
wafroc_plot_2B <- ret2AB$wafroc$Plot + labs(tag = "B")
wafroc_cad_2B <- ret2AB$fomCad
wafroc_rad_2B <- ret2AB$fomRad

zeta1Cad <- -Inf
zeta1Rad <- -Inf

ret2CD <- GenerateCadVsRadPlots (muCad, muRad, zeta1Cad, zeta1Rad, K1, K2, Lk2, seed)

froc_plot_2C <- ret2CD$froc$Plot + labs(tag = "C")
wafroc_plot_2D <- ret2CD$wafroc$Plot + labs(tag = "D")
wafroc_cad_2D <- ret2CD$fomCad
wafroc_rad_2D <- ret2CD$fomRad
```

```{r, echo=FALSE}
# extract coordinates of end-point
nlfCad2A <- max(froc_plot_2A$data$genAbscissa[froc_plot_2A$data$Reader == "R: CAD"]) 
llfCad2A <- max(froc_plot_2A$data$genOrdinate[froc_plot_2A$data$Reader == "R: CAD"]) 
nlfRad2A <- max(froc_plot_2A$data$genAbscissa[froc_plot_2A$data$Reader == "R: RAD"]) 
llfRad2A <- max(froc_plot_2A$data$genOrdinate[froc_plot_2A$data$Reader == "R: RAD"]) 

nlfCad2C <- max(froc_plot_2C$data$genAbscissa[froc_plot_2C$data$Reader == "R: CAD"]) 
llfCad2C <- max(froc_plot_2C$data$genOrdinate[froc_plot_2C$data$Reader == "R: CAD"]) 
nlfRad2C <- max(froc_plot_2C$data$genAbscissa[froc_plot_2C$data$Reader == "R: RAD"]) 
llfRad2C <- max(froc_plot_2C$data$genOrdinate[froc_plot_2C$data$Reader == "R: RAD"]) 
```

The end-point coordinates of the FROC for RAD are (`r nlfRad2A`, `r llfRad2A`). The common NLF region defined by NLF = 0 to NLF = `r nlfRad2A` *would exclude almost all of the marks made by CAD*. The wAFROC plots in plot B show the markedly greater performance of RAD over CAD (the AUCs are `r wafroc_cad_2B` for CAD and `r wafroc_rad_2B` for RAD). The difference is larger (compared to Fig. \@ref(fig:froc-vs-afroc-plot1) plot B), in spite of the greater downward bias working against the RAD observer.

Plots C and D correspond to A and B, respectively, with $\zeta_1$ = $-\infty$ for both readers. It reveals the full extent of the curves, when each observer marks every suspicious region, as long as it exists and no matter how low its z-sample.

```{r froc-vs-afroc-plot2, fig.cap="Plots A and B are for CAD $\\zeta_1 = -1$ and RAD $\\zeta_1 = 2$ and plots C and D are plots for CAD $\\zeta_1 = -\\infty$ and RAD $\\zeta_1 = -\\infty$. A and C: FROC curves for the CAD and RAD observers. B and D: corresponding wAFROC curves.", fig.show='hold', echo=FALSE}
grid.arrange(froc_plot_2A,wafroc_plot_2B,froc_plot_2C,wafroc_plot_2D,nrow=2,ncol=2)
```

Fig. \@ref(fig:froc-vs-afroc-plot2) (A) FROC curves for CAD observer and the RAD observer. The CAD observer is identical to that shown in Fig. \@ref(fig:froc-vs-afroc-plot1). The RAD observer is characterized by $\mu = 2$ and $\zeta_1 = 2$. This time it is impossible to compare the two FROC curves, as the common range is very small. However, wAFROC, plot B, clearly shows the expected superiority of the RAD observer, in spite of the severe underestimate of the corresponding AUC. 

### Small difference in performance

The final example, Fig. \@ref(fig:froc-vs-afroc-plot3) shows that *when there is a small difference in performance*, there is less ambiguity in using the FROC as a basis for measuring performance. The CAD parameters are the same as in Fig. \@ref(fig:froc-vs-afroc-plot1) but the RAD parameters are $\mu = 1.1$ and $\zeta_1= -1$. This time there is much more common NLF range overlap in plot A and one is counting most of the marks for the CAD reader. The superior wAFROC-based performance of RAD is also apparent in B. Plots C and D correspond to $\zeta_1 = -\infty$, and they confirm the similar performances (apparent in plots A and B) with a slight superiority for RAD.

```{r, cache = TRUE, echo=FALSE}
source(here("R/CH13-GenerateCadVsRadPlots/GenerateCadVsRadPlots.R"))

nu <- 1
lambda <- 1
K1 <- 500
K2 <- 700
muCad <- 1.0
muRad <- 1.1
zeta1Cad <- -1
zeta1Rad <- -1
Lmax <- 2
seed <- 1
set.seed(seed)
Lk2 <- floor(runif(K2, 1, Lmax + 1))

ret3AB <- GenerateCadVsRadPlots (muCad, muRad, zeta1Cad, zeta1Rad, K1, K2, Lk2, seed)

froc_plot_3A <- ret3AB$froc$Plot + labs(tag = "A")
wafroc_plot_3B <- ret3AB$wafroc$Plot + labs(tag = "B")
wafroc_cad_3B <- ret3AB$fomCad
wafroc_rad_3B <- ret3AB$fomRad

zeta1Cad <- -Inf
zeta1Rad <- -Inf

ret3CD <- GenerateCadVsRadPlots (muCad, muRad, zeta1Cad, zeta1Rad, K1, K2, Lk2, seed)

froc_plot_3C <- ret3CD$froc$Plot + labs(tag = "C")
wafroc_plot_3D <- ret3CD$wafroc$Plot + labs(tag = "D")
wafroc_cad_3D <- ret3CD$fomCad
wafroc_rad_3D <- ret3CD$fomRad
```

```{r froc-vs-afroc-plot3, fig.cap="Plots A and B are for CAD $\\zeta_1 = -1$ and RAD $\\zeta_1 = -1$ and plots C and D are plots for CAD $\\zeta_1 = -\\infty$ and RAD $\\zeta_1 = -\\infty$. A and C: FROC curves for the CAD and RAD observers. B and D: corresponding wAFROC curves.", fig.show='hold', echo=FALSE}
grid.arrange(froc_plot_3A,wafroc_plot_3B,froc_plot_3C,wafroc_plot_3D,nrow=2,ncol=2)
```

**The bottom line is that in all three cases the wAFROC yields the right conclusion, while the FROC comparison can only be made when the two performances are similar.**



## Summary of simulations
In order to get a better overview, the following tables summarize the numerical values from the plots in this chapter. Table \@ref(tab:froc-vs-afroc-summary-table-cad) refers to CAD, and Table \@ref(tab:froc-vs-afroc-summary-table-rad) refers to the RAD observer.

```{r, echo=FALSE}
# extract coordinates of end-point
nlfCad3A <- max(froc_plot_3A$data$genAbscissa[froc_plot_3A$data$Reader == "R: CAD"]) 
llfCad3A <- max(froc_plot_3A$data$genOrdinate[froc_plot_3A$data$Reader == "R: CAD"]) 
nlfRad3A <- max(froc_plot_3A$data$genAbscissa[froc_plot_3A$data$Reader == "R: RAD"]) 
llfRad3A <- max(froc_plot_3A$data$genOrdinate[froc_plot_3A$data$Reader == "R: RAD"]) 

nlfCad3C <- max(froc_plot_3C$data$genAbscissa[froc_plot_3C$data$Reader == "R: CAD"]) 
llfCad3C <- max(froc_plot_3C$data$genOrdinate[froc_plot_3C$data$Reader == "R: CAD"]) 
nlfRad3C <- max(froc_plot_3C$data$genAbscissa[froc_plot_3C$data$Reader == "R: RAD"]) 
llfRad3C <- max(froc_plot_3C$data$genOrdinate[froc_plot_3C$data$Reader == "R: RAD"]) 
```


```{r cad, echo=FALSE}
dig <- 4

cell1 <- paste0("(", as.character(format(nlfCad1A, digits = dig)), ", ", as.character(format(llfCad1A, digits = dig)), ")")
cell2 <- paste0("(", as.character(format(nlfCad1C, digits = dig)), ", ", as.character(format(llfCad1C, digits = dig)), ")")
cell3 <- as.character(format(wafroc_cad_1B, digits = dig))
cell4 <- as.character(format(wafroc_cad_1D, digits = dig))

```


```{r froc-vs-afroc-summary-table-cad, echo=FALSE}
tableCells = array(dim = c(1,4))

tableCells[1, 1]  <- cell1
tableCells[1, 2]  <- cell2
tableCells[1, 3]  <- cell3
tableCells[1, 4]  <- cell4

df <- as.data.frame(tableCells)
colnames(df) <- c("FROC-A", "FROC-C", "wAFROC-B", "wAFROC-D")
knitr::kable(df, caption = "Summary of CAD simulations: A refers to plot A, B refers to plot B, etc.", escape = FALSE)
```


### Summary of CAD simulations
* The first column is labeled "FROC-A", meaning the CAD FROC plots labeled A, which are identical for Fig. \@ref(fig:froc-vs-afroc-plot1), Fig. \@ref(fig:froc-vs-afroc-plot2) and Fig. \@ref(fig:froc-vs-afroc-plot3). 
* The second column is labeled "FROC-C", meaning the CAD FROC plots labeled C, which are identical for the three figures. 
* The third column is labeled "wAFROC-B", meaning the CAD wAFROC plots labeled B, which are identical for the three figures.
* The last column is labeled "wAFROC-D", meaning the CAD wAFROC plots labeled D, which are identical for the three figures.


```{r, echo=FALSE}

cell_11 <- "1"
cell_12 <- paste0("(", as.character(format(nlfRad1A, digits = dig)), ", ", as.character(format(llfRad1A, digits = dig)), ")")
cell_13 <- paste0("(", as.character(format(nlfRad1C, digits = dig)), ", ", as.character(format(llfRad1C, digits = dig)), ")")
cell_14 <- as.character(format(wafroc_rad_1B, digits = dig))
cell_15 <- as.character(format(wafroc_rad_1D, digits = dig))

cell_21 <- "2"
cell_22 <- paste0("(", as.character(format(nlfRad2A, digits = dig)), ", ", as.character(format(llfRad2A, digits = dig)), ")")
cell_23 <- paste0("(", as.character(format(nlfRad2C, digits = dig)), ", ", as.character(format(llfRad2C, digits = dig)), ")")
cell_24 <- as.character(format(wafroc_rad_2B, digits = dig))
cell_25 <- as.character(format(wafroc_rad_2D, digits = dig))

cell_31 <- "3"
cell_32 <- paste0("(", as.character(format(nlfRad3A, digits = dig)), ", ", as.character(format(llfRad3A, digits = dig)), ")")
cell_33 <- paste0("(", as.character(format(nlfRad3C, digits = dig)), ", ", as.character(format(llfRad3C, digits = dig)), ")")
cell_34 <- as.character(format(wafroc_rad_3B, digits = dig))
cell_35 <- as.character(format(wafroc_rad_3D, digits = dig))
```


```{r froc-vs-afroc-summary-table-rad, echo=FALSE}
tableCells = array(dim = c(3,5))

tableCells[1,]  <- c(cell_11, cell_12, cell_13, cell_14, cell_15)
tableCells[2,]  <- c(cell_21, cell_22, cell_23, cell_24, cell_25)
tableCells[3,]  <- c(cell_31, cell_32, cell_33, cell_34, cell_35)
                     
df <- as.data.frame(tableCells)
rownames(df) <- c("1","2","3")
colnames(df) <- c("Fig", "FROC-A", "FROC-C", "wAFROC-B", "wAFROC-D")
knitr::kable(df, caption = "Summary of RAD simulations: Fig refers to the figure number in this chapter, A refers to plot A, B refers to plot B, etc.", escape = FALSE)
```


### Summary of RAD simulations

* The first column refers to the figure number, for example, "1" refers to Fig. \@ref(fig:froc-vs-afroc-plot1), "2" refers to Fig. \@ref(fig:froc-vs-afroc-plot2), and "3" refers to Fig. \@ref(fig:froc-vs-afroc-plot3).
* The second column is labeled "FROC-A", meaning the RAD FROC plot labeled A. 
* The third column is labeled "FROC-C", meaning the RAD FROC plots labeled C. 
* The fourth column is labeled "wAFROC-B", meaning the RAD wAFROC plots labeled B.
* The last column is labeled "wAFROC-D", meaning the RAD wAFROC plots labeled D.


## Comments {#froc-vs-wafroc-comments}
* For the same figure label (A, B, C or D) the CAD plots are identical in the three figures. This is the reason why Table \@ref(tab:froc-vs-afroc-summary-table-cad) has only one row. 
* A *fixed* CAD dataset is being compared to *varying* RAD datasets.
* The first RAD dataset, Fig. \@ref(fig:froc-vs-afroc-plot1) A or B, might be considered an average radiologist, the second one, Fig. \@ref(fig:froc-vs-afroc-plot2) A or B, is a super-expert and the third one, Fig. \@ref(fig:froc-vs-afroc-plot3) A or B, is only nominally better than CAD.
* Plots C and D are for hypothetical CAD and RAD readers that report all suspicious regions. The differences between A and C are minimal for the CAD observer, but marked for the RAD observer. Likewise for the differences between B and D.

## FROC gives incorrect performance ordering {#froc-vs-wafroc-froc-incorrect-ordering}
* With reference to Table \@ref(tab:froc-vs-afroc-summary-table-cad), consider the differences between FROC-A and FROC-C: both end-point NLF and LLF *increase* when $\zeta_1$ is reduced from -1 to $-\infty$. The observer in A and that in C are operating on the same underlying FROC curve: A's end-point is lower than C's end-point because A's threshold is higher. **If one believes that a simple change in reporting threshold should not affect performance, then their performances, according to FROC curve, are identical. In the ROC paradigm two points on the same underlying ROC curve represent the same intrinsic performance -- all that is happening is that they are employing different thresholds and represent different tradeoffs between sensitivity and specificity. But, extending this concept to the FROC curve leads to a wrong conclusion -- see next point.**
* The difference between CAD wAFROC-B (`r wafroc_cad_1B`) and CAD wAFROC-D (`r wafroc_cad_1D``) are in the opposite direction. This is not a sampling artifact. When $\zeta_1$ is reduced from -1 to $-\infty$, it results in predominantly more NL marks than more LL marks and performance decreases. Just think of two observers: both make the same LL marks but the second makes more NL marks - surely the second observer must be worse. **The wAFROC gives the correct ordering of the two observers, one that is missed by the FROC.**


## To be moved {#froc-vs-wafroc-to-move}
A misconception exists that using the rating of only one NL mark, as in wAFROC, must sacrifice statistical power. In fact, the chosen mark is a special one, namely the highest rated NL mark on a non-diseased case, which carries more information than a randomly chosen NL mark. If the sampling distribution of the z-sample were uniform, then the highest sample is a sufficient statistic, meaning that it carries all the information in the samples. The highest rated z-sampler from a normal distribution is not a sufficient statistic, so there is some loss of information, but not as much as would occur with a randomly picked z-sample.

(A) 

(B) 

(C) 

(D) Fig. \@ref(fig:froc-vs-afroc-plot3): (A, B) FROC/wAFROC curves for CAD and RAD observers. The CAD observer is identical to that shown in Fig. \@ref(fig:froc-vs-afroc-plot2) (A, B). The RAD observer is characterized by mu = 1.1 and $\zeta_1$ = -1. This time it is possible to compare the two FROC curves, as the common NLF range is large. Both FROC and wAFROC show the expected slight superiority of the RAD observer. AUCs under the two wAFROC plots are 0.608 for CAD and 0.634 for RAD. Plots C and D correspond to A and B, respectively, with $\zeta_1$ = $-\infty$ for both observers. Since $\zeta_1$ in A and B is already quite small, lowering it to $-\infty$ does not pick up too many marks. AUCs under the two wAFROC plots in D are 0.601 for CAD and 0.624 for RAD. 13.16.4: Other issues with the FROC Loss of statistical power is not the only issue with the FROC. Because it counts NLs on both diseased and non-diseased cases, the curve depends on disease-prevalence in the dataset. Because the numbers of LLs per case is variable, the curve gives undue importance to those diseased cases with unusually large numbers of lesions. As noted in 13.16.2, the clinical importance of a NL on a non-diseased case differs from that on a diseased case. The FROC curve ignores this distinction.

## Discussion {#froc-vs-wafroc-Discussion}

## References {#froc-vs-wafroc-references}
